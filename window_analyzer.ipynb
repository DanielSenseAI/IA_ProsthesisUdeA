{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98492106",
   "metadata": {},
   "source": [
    "### 100 ms Window and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0009df82",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preprocessed_data\\\\DB4\\\\DB4_w200_env0_f0.6[20250406-194002].parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(file_path, database, filename)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the Parquet file into a pandas DataFrame\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\escob\\Desktop\\PhD\\Code\\IA_ProsthesisUdeA\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\escob\\Desktop\\PhD\\Code\\IA_ProsthesisUdeA\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    263\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    273\u001b[0m         path_or_handle,\n\u001b[0;32m    274\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    278\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\escob\\Desktop\\PhD\\Code\\IA_ProsthesisUdeA\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    129\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\escob\\Desktop\\PhD\\Code\\IA_ProsthesisUdeA\\venv\\Lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocessed_data\\\\DB4\\\\DB4_w200_env0_f0.6[20250406-194002].parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Replace 'your_file.parquet' with the actual file name\n",
    "filename = 'DB4_w200_env0_f0.6[20250406-194002].parquet'\n",
    "file_path = 'preprocessed_data'\n",
    "database = 'DB4'\n",
    "\n",
    "file_path = os.path.join(file_path, database, filename)\n",
    "\n",
    "# Load the Parquet file into a pandas DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, verbose=True, log_to_file=False, log_file=\"run_log.txt\"):\n",
    "        self.verbose = verbose\n",
    "        self.log_to_file = log_to_file\n",
    "        self.log_file = log_file\n",
    "        if self.log_to_file:\n",
    "            with open(self.log_file, \"w\") as f:\n",
    "                f.write(f\"=== PyCaret Run Log - {datetime.datetime.now()} ===\\n\")\n",
    "\n",
    "    def log(self, msg, level=\"INFO\"):\n",
    "        formatted = f\"[{level}] {msg}\"\n",
    "        if self.verbose:\n",
    "            print(formatted)\n",
    "        if self.log_to_file:\n",
    "            with open(self.log_file, \"a\") as f:\n",
    "                f.write(formatted + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64bd57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 columns with constant values:\n",
      "  - HFD: 0\n",
      "\n",
      "Shape after removing constant columns: (3280764, 48)\n"
     ]
    }
   ],
   "source": [
    "# 1. First, let's identify columns with constant values\n",
    "def identify_constant_columns(df):\n",
    "    \"\"\"\n",
    "    Identify columns that contain only a single value.\n",
    "    Returns a list of column names with constant values.\n",
    "    \"\"\"\n",
    "    constant_columns = []\n",
    "    for col in df.columns:\n",
    "        # Check if the column has only one unique value\n",
    "        if df[col].nunique() == 1:\n",
    "            constant_value = df[col].iloc[0]\n",
    "            constant_columns.append((col, constant_value))\n",
    "    return constant_columns\n",
    "\n",
    "# 3. Apply the functions to identify constant columns and rows\n",
    "constant_columns = identify_constant_columns(df)\n",
    "print(f\"Found {len(constant_columns)} columns with constant values:\")\n",
    "for col, value in constant_columns:\n",
    "    print(f\"  - {col}: {value}\")\n",
    "\n",
    "# 4. Create a cleaned dataframe with constant columns removed\n",
    "constant_column_names = [col for col, _ in constant_columns]\n",
    "df_cleaned = df.drop(columns=constant_column_names)\n",
    "print(f\"\\nShape after removing constant columns: {df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0c8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 columns with NaN values:\n",
      "  No NaN values found in the dataset!\n",
      "\n",
      "Columns with >1% NaN values:\n",
      "  No columns with significant NaN values\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the DataFrame\n",
    "nan_counts = df.isna().sum()\n",
    "columns_with_nans = nan_counts[nan_counts > 0]\n",
    "\n",
    "print(f\"Found {len(columns_with_nans)} columns with NaN values:\")\n",
    "if len(columns_with_nans) > 0:\n",
    "    print(columns_with_nans)\n",
    "else:\n",
    "    print(\"  No NaN values found in the dataset!\")\n",
    "\n",
    "# Calculate percentage of NaN values in each column\n",
    "nan_percentage = (df.isna().sum() / len(df)) * 100\n",
    "high_nan_cols = nan_percentage[nan_percentage > 1].sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nColumns with >1% NaN values:\")\n",
    "if len(high_nan_cols) > 0:\n",
    "    for col, pct in high_nan_cols.items():\n",
    "        print(f\"  - {col}: {pct:.2f}%\")\n",
    "else:\n",
    "    print(\"  No columns with significant NaN values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177c0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for extreme outliers (z-score > 5):\n",
      "  No columns with significant outliers found\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers in each feature column\n",
    "# Exclude metadata columns\n",
    "metadata_cols = ['subject', 'filename', 'grasp', 'relabeled', 'channel', 'window_id']\n",
    "feature_cols = [col for col in df.columns if col not in metadata_cols]\n",
    "\n",
    "# Calculate z-scores for each feature\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nChecking for extreme outliers (z-score > 5):\")\n",
    "outlier_cols = {}\n",
    "\n",
    "for col in feature_cols:\n",
    "    # Skip columns that have constant values\n",
    "    if df[col].nunique() <= 1:\n",
    "        continue\n",
    "    \n",
    "    # Calculate z-scores, ignoring NaN values\n",
    "    z_scores = np.abs(stats.zscore(df[col], nan_policy='omit'))\n",
    "    \n",
    "    # Count values with z-score > 5 (extreme outliers)\n",
    "    outlier_count = np.sum(z_scores > 5)\n",
    "    outlier_percentage = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    if outlier_percentage > 1:  # Only report columns with >1% outliers\n",
    "        outlier_cols[col] = outlier_percentage\n",
    "\n",
    "if outlier_cols:\n",
    "    sorted_outliers = dict(sorted(outlier_cols.items(), key=lambda x: x[1], reverse=True))\n",
    "    for col, pct in sorted_outliers.items():\n",
    "        print(f\"  - {col}: {pct:.2f}% extreme outliers\")\n",
    "else:\n",
    "    print(\"  No columns with significant outliers found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c0eb270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features with highly skewed distributions (skewness > 3):\n",
      "  - CoV: -649.38 (skewed left)\n",
      "  - MNP_STD: 522.34 (skewed right)\n",
      "  - MNP: 175.73 (skewed right)\n",
      "  - TTP: 175.73 (skewed right)\n",
      "  - VAR: 175.69 (skewed right)\n",
      "  - WAMP: -9.03 (skewed left)\n",
      "  - WL_STD: 7.65 (skewed right)\n",
      "  - TD_STD: 7.65 (skewed right)\n",
      "  - MAVS_STD: 7.65 (skewed right)\n",
      "  - mDWT_STD: 7.30 (skewed right)\n",
      "  - SSC_STD: -6.33 (skewed left)\n",
      "  - LE: -5.55 (skewed left)\n",
      "  - KURT: 5.49 (skewed right)\n",
      "  - MAV_STD: 4.90 (skewed right)\n",
      "  - IAV_STD: 4.90 (skewed right)\n",
      "  - DASDV: 4.88 (skewed right)\n",
      "  - PKF: 4.46 (skewed right)\n",
      "  - RMS: 4.18 (skewed right)\n",
      "  - RMS_STD: 4.13 (skewed right)\n",
      "  - VAR_STD: 4.13 (skewed right)\n",
      "  - LOG: 3.88 (skewed right)\n",
      "  - MAV: 3.87 (skewed right)\n",
      "  - IAV: 3.87 (skewed right)\n",
      "  - mDWT: 3.81 (skewed right)\n",
      "  - WL: 3.52 (skewed right)\n",
      "  - TD: 3.52 (skewed right)\n",
      "  - MAVS: 3.52 (skewed right)\n",
      "  - CC: 3.52 (skewed right)\n"
     ]
    }
   ],
   "source": [
    "# Check feature distributions - focus on skewness\n",
    "print(\"\\nFeatures with highly skewed distributions (skewness > 3):\")\n",
    "skewed_features = {}\n",
    "\n",
    "for col in feature_cols:\n",
    "    if df[col].nunique() <= 1:\n",
    "        continue\n",
    "    \n",
    "    # Calculate skewness, ignoring NaN values\n",
    "    skewness = df[col].skew()\n",
    "    \n",
    "    # Only report features with high skewness (absolute value > 3)\n",
    "    if abs(skewness) > 3:\n",
    "        skewed_features[col] = skewness\n",
    "\n",
    "if skewed_features:\n",
    "    sorted_skewed = dict(sorted(skewed_features.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "    for col, skew in sorted_skewed.items():\n",
    "        direction = \"right\" if skew > 0 else \"left\"\n",
    "        print(f\"  - {col}: {skew:.2f} (skewed {direction})\")\n",
    "else:\n",
    "    print(\"  No highly skewed features found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation between features - ITERATIVE APPROACH\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Start with all feature columns (excluding metadata)\n",
    "remaining_features = [col for col in df_cleaned.columns if col not in metadata_cols]\n",
    "removed_features = []\n",
    "correlation_history = []\n",
    "corr_threshold = 0.95\n",
    "\n",
    "print(\"Starting iterative correlation removal process...\")\n",
    "print(f\"Initial feature count: {len(remaining_features)}\")\n",
    "\n",
    "iteration = 1\n",
    "while True:\n",
    "    # Calculate correlation matrix for remaining features\n",
    "    corr_matrix = df_cleaned[remaining_features].corr()\n",
    "    \n",
    "    # Find the highest correlation pair\n",
    "    highest_corr = 0\n",
    "    highest_pair = None\n",
    "    \n",
    "    for i in range(len(remaining_features)):\n",
    "        for j in range(i+1, len(remaining_features)):\n",
    "            correlation = abs(corr_matrix.iloc[i, j])\n",
    "            if correlation > highest_corr:\n",
    "                highest_corr = correlation\n",
    "                highest_pair = (remaining_features[i], remaining_features[j], correlation)\n",
    "    \n",
    "    # If highest correlation is below threshold, we're done\n",
    "    if highest_corr < corr_threshold:\n",
    "        print(f\"\\nNo more pairs with correlation >= {corr_threshold}\")\n",
    "        print(f\"Highest remaining correlation: {highest_corr:.4f}\")\n",
    "        break\n",
    "    \n",
    "    # Record this pair's correlation\n",
    "    correlation_history.append(highest_pair)\n",
    "    \n",
    "    # Decide which feature to remove based on average correlation with other features\n",
    "    feature1, feature2, corr_value = highest_pair\n",
    "    \n",
    "    # Calculate average correlation with all other features\n",
    "    avg_corr1 = corr_matrix[feature1].abs().mean()\n",
    "    avg_corr2 = corr_matrix[feature2].abs().mean()\n",
    "    \n",
    "    # Also consider NaN percentage\n",
    "    nan_pct1 = nan_percentage.get(feature1, 0)\n",
    "    nan_pct2 = nan_percentage.get(feature2, 0)\n",
    "    \n",
    "    # Remove the feature with higher average correlation or more NaNs\n",
    "    if avg_corr1 > avg_corr2 or nan_pct1 > nan_pct2:\n",
    "        to_remove = feature1\n",
    "        to_keep = feature2\n",
    "    else:\n",
    "        to_remove = feature2\n",
    "        to_keep = feature1\n",
    "    \n",
    "    # Remove the selected feature\n",
    "    remaining_features.remove(to_remove)\n",
    "    removed_features.append((to_remove, highest_corr))\n",
    "    \n",
    "    print(f\"Iteration {iteration}: Removed '{to_remove} [{to_keep}]' (corr={highest_corr:.4f})\")\n",
    "    iteration += 1\n",
    "\n",
    "# Print summary of removed features\n",
    "print(f\"\\nRemoved {len(removed_features)} features due to high correlation:\")\n",
    "for feature, corr in removed_features:\n",
    "    print(f\"  - {feature} (max corr: {corr:.4f})\")\n",
    "\n",
    "print(f\"\\nRetained {len(remaining_features)} features after correlation analysis\")\n",
    "\n",
    "# Create a heatmap of the final correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "final_corr = df_cleaned[remaining_features].corr()\n",
    "sns.heatmap(final_corr, annot=False, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('Correlation Matrix After Removing Highly Correlated Features')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top 10 highest remaining correlations\n",
    "print(\"\\nTop 10 highest remaining correlations:\")\n",
    "high_corrs = []\n",
    "for i in range(len(remaining_features)):\n",
    "    for j in range(i+1, len(remaining_features)):\n",
    "        high_corrs.append((\n",
    "            remaining_features[i],\n",
    "            remaining_features[j],\n",
    "            abs(final_corr.iloc[i, j])\n",
    "        ))\n",
    "\n",
    "high_corrs.sort(key=lambda x: x[2], reverse=True)\n",
    "for f1, f2, c in high_corrs[:10]:\n",
    "    print(f\"  - {f1} and {f2}: {c:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31dc1181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature count: 42\n",
      "Removed 11 highly correlated features (>|0.98|):\n",
      "\n",
      "  - Removed 'IAV' instead of 'MAV' (corr=1.0000) [Group: amplitude]\n",
      "  - Removed 'RMS' instead of 'MAV' (corr=0.9935) [Group: amplitude]\n",
      "  - Removed 'MAV' instead of 'LOG' (corr=0.9898) [Group: amplitude]\n",
      "  - Removed 'MAV_STD' instead of 'IAV_STD' (corr=1.0000) [Group: std]\n",
      "  - Removed 'RMS_STD' instead of 'MAV_STD' (corr=0.9916) [Group: std]\n",
      "  - Removed 'VAR_STD' instead of 'MAV_STD' (corr=0.9916) [Group: std]\n",
      "  - Removed 'WL' instead of 'TD' (corr=1.0000) [Group: change]\n",
      "  - Removed 'MAVS' instead of 'WL' (corr=1.0000) [Group: change]\n",
      "  - Removed 'WL_STD' instead of 'TD_STD' (corr=1.0000) [Group: std]\n",
      "  - Removed 'TD_STD' instead of 'MAVS_STD' (corr=1.0000) [Group: std]\n",
      "  - Removed 'SM1' instead of 'MNPF' (corr=1.0000) [Group: freq]\n",
      "\n",
      "Final retained features: 31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your feature groups\n",
    "feature_groups = {\n",
    "    'amplitude': [\n",
    "        'MAV', 'IAV', 'RMS', 'TTP', 'LOG', 'MYOP', 'WAMP', 'DASDV', 'CoV'\n",
    "    ],\n",
    "    'change': [\n",
    "        'WL', 'ZC', 'SSC', 'TD', 'MAVS'\n",
    "    ],\n",
    "    'moments': [\n",
    "        'VAR', 'SKEW', 'KURT'\n",
    "    ],\n",
    "    'std': [\n",
    "        'MAV_STD', 'IAV_STD', 'RMS_STD', 'WL_STD', 'ZC_STD',\n",
    "        'SSC_STD', 'VAR_STD', 'TD_STD', 'MAVS_STD'\n",
    "    ],\n",
    "    'freq': [\n",
    "        'MDF', 'PKF', 'MNF', 'TTP', 'SM1', 'SM2', 'SM3', 'MNPF'\n",
    "    ],\n",
    "    'wavelet': ['mDWT', 'mDWT_STD'],\n",
    "    'complexity': ['SampEn', 'CC', 'LE', 'HFD']\n",
    "}\n",
    "\n",
    "# Build a reverse index: feature -> group\n",
    "feature_to_group = {}\n",
    "for group, features in feature_groups.items():\n",
    "    for f in features:\n",
    "        feature_to_group[f] = group\n",
    "\n",
    "# Exclude metadata\n",
    "metadata_cols = ['subject', 'filename', 'grasp', 'relabeled', 'channel', 'window_id']\n",
    "feature_cols = [col for col in df_cleaned.columns if col not in metadata_cols]\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_threshold = 0.98\n",
    "corr_matrix = df_cleaned[feature_cols].corr().abs()\n",
    "\n",
    "# Track what gets removed\n",
    "to_remove = set()\n",
    "removed_pairs = []\n",
    "\n",
    "# Loop through upper triangle of correlation matrix\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        f1 = corr_matrix.columns[i]\n",
    "        f2 = corr_matrix.columns[j]\n",
    "        corr = corr_matrix.iloc[i, j]\n",
    "\n",
    "        if corr > corr_threshold:\n",
    "            group1 = feature_to_group.get(f1, 'unknown')\n",
    "            group2 = feature_to_group.get(f2, 'unknown')\n",
    "\n",
    "            # Prefer to keep features from different groups\n",
    "            if group1 != group2:\n",
    "                continue\n",
    "\n",
    "            # If from same group, remove the one with higher mean correlation to others\n",
    "            mean_corr_f1 = corr_matrix[f1].mean()\n",
    "            mean_corr_f2 = corr_matrix[f2].mean()\n",
    "\n",
    "            if mean_corr_f1 >= mean_corr_f2:\n",
    "                if f1 not in to_remove:\n",
    "                    to_remove.add(f1)\n",
    "                    removed_pairs.append((f1, f2, corr))\n",
    "            else:\n",
    "                if f2 not in to_remove:\n",
    "                    to_remove.add(f2)\n",
    "                    removed_pairs.append((f2, f1, corr))\n",
    "\n",
    "# Filter features\n",
    "final_features = [f for f in feature_cols if f not in to_remove]\n",
    "\n",
    "# --- Reporting ---\n",
    "print(f\"Initial feature count: {len(feature_cols)}\")\n",
    "print(f\"Removed {len(to_remove)} highly correlated features (>|{corr_threshold}|):\\n\")\n",
    "for f_rm, f_keep, corr in removed_pairs:\n",
    "    print(f\"  - Removed '{f_rm}' instead of '{f_keep}' (corr={corr:.4f}) [Group: {feature_to_group.get(f_rm)}]\")\n",
    "\n",
    "print(f\"\\nFinal retained features: {len(final_features)}\")\n",
    "\n",
    "# Optionally: update df\n",
    "df_selected = df_cleaned[metadata_cols + final_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "425a9c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train/Test split: 219936 train, 54984 test\n",
      "[INFO] PyCaret setup completed\n",
      "[INFO] Model comparison completed\n",
      "\n",
      "=== MODEL COMPARISON ===\n",
      "                                    Model  Accuracy     AUC  Recall      F1\n",
      "lightgbm  Light Gradient Boosting Machine    0.2456  0.6814  0.2456  0.2332\n",
      "rf               Random Forest Classifier    0.2433  0.6728  0.2433  0.2368\n",
      "et                 Extra Trees Classifier    0.2389  0.6694  0.2389  0.2328\n",
      "gbc          Gradient Boosting Classifier    0.2365  0.0000  0.2365  0.2195\n",
      "lr                    Logistic Regression    0.2137  0.0000  0.2137  0.1896\n",
      "ridge                    Ridge Classifier    0.2120  0.0000  0.2120  0.1820\n",
      "ada                  Ada Boost Classifier    0.2093  0.0000  0.2093  0.1870\n",
      "knn                K Neighbors Classifier    0.1827  0.5845  0.1827  0.1833\n",
      "qda       Quadratic Discriminant Analysis    0.1772  0.0000  0.1772  0.1393\n",
      "dt               Decision Tree Classifier    0.1734  0.5349  0.1734  0.1734\n",
      "nb                            Naive Bayes    0.1626  0.5707  0.1626  0.1101\n",
      "svm                   SVM - Linear Kernel    0.1319  0.0000  0.1319  0.1160\n",
      "dummy                    Dummy Classifier    0.1196  0.5000  0.1196  0.0255\n",
      "[INFO] Model evaluation on test set completed\n",
      "\n",
      "Test Accuracy: 0.2558\n",
      "[INFO] Feature importance extraction completed\n",
      "\n",
      "=== TOP FEATURES ===\n",
      "     Feature  Importance\n",
      "19       SM3        1692\n",
      "20      MNPF        1500\n",
      "29      SKEW        1432\n",
      "18       SM2        1396\n",
      "23        LE        1274\n",
      "21    SampEn        1229\n",
      "5   MAVS_STD        1189\n",
      "12   MNF_STD        1110\n",
      "10       CoV        1089\n",
      "28       LOG        1082\n",
      "8        SSC        1037\n",
      "4         TD        1004\n",
      "30      KURT         882\n",
      "14  mDWT_STD         869\n",
      "22        CC         867\n",
      "11       MNF         858\n",
      "15       MDF         857\n",
      "24     DASDV         839\n",
      "16       PKF         829\n",
      "9    SSC_STD         787\n",
      "\n",
      "Total Runtime: 1455.79 seconds\n"
     ]
    }
   ],
   "source": [
    "from pycaret.classification import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = Logger(verbose=True, log_to_file=False)\n",
    "\n",
    "# Measure total time\n",
    "start_time_total = time.time()\n",
    "\n",
    "# === Step 1: Filter data for selected classes ===\n",
    "filtered_labels = [55, 2, 4, 14, 10, 16, 17, 19, 32]\n",
    "df_reduced = df_selected[df_selected['relabeled'].isin(filtered_labels)].sample(frac=1, random_state=42)\n",
    "\n",
    "models_to_test = [\n",
    "    'dummy',    # Dummy classifier (baseline)\n",
    "    'lr',       # Logistic Regression\n",
    "    'dt',       # Decision Tree\n",
    "    'lightgbm', # LightGBM\n",
    "    'nb',       # Naive Bayes\n",
    "    'ridge',    # Ridge Classifier\n",
    "    'knn',      # K Nearest Neighbors\n",
    "    'ada',      # AdaBoost\n",
    "    'qda',      # Quadratic Discriminant Analysis\n",
    "    'et',       # Extra Trees Classifier\n",
    "    'rf',       # Random Forest\n",
    "    'gbc',      # Gradient Boosting Classifier\n",
    "    'svm'       # Support Vector Machine (Linear)\n",
    "]\n",
    "\n",
    "# === Step 2: Train/Test Split ===\n",
    "train_df, test_df = train_test_split(df_reduced, test_size=0.2, random_state=42)\n",
    "logger.log(f\"Train/Test split: {len(train_df)} train, {len(test_df)} test\")\n",
    "\n",
    "# === Step 3: PyCaret Setup (silent) ===\n",
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    s = setup(\n",
    "        data=train_df,\n",
    "        target='relabeled',\n",
    "        numeric_features=[col for col in train_df.columns \n",
    "                          if col not in metadata_cols and col != 'relabeled'],\n",
    "        ignore_features=['subject', 'filename', 'grasp', 'channel', 'window_id'],\n",
    "        normalize=True,\n",
    "        transformation=False,\n",
    "        feature_selection=False,\n",
    "        polynomial_features=False,\n",
    "        fold=2,\n",
    "        session_id=42,\n",
    "        verbose=False,\n",
    "        html=False,\n",
    "        n_jobs=-1,\n",
    "        use_gpu=True,\n",
    "        log_experiment=False,\n",
    "        profile=False\n",
    "    )\n",
    "logger.log(\"PyCaret setup completed\")\n",
    "\n",
    "# === Step 4: Compare Models ===\n",
    "with redirect_stdout(f):\n",
    "    best_models = compare_models(\n",
    "        include=models_to_test,\n",
    "        n_select=1,\n",
    "        fold=2,\n",
    "        sort='Accuracy',\n",
    "        verbose=False,\n",
    "        turbo=True\n",
    "    )\n",
    "logger.log(\"Model comparison completed\")\n",
    "\n",
    "# Pull comparison results\n",
    "comparison_df = pull()\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "# Check if 'Precision' exists in the DataFrame\n",
    "columns_to_display = ['Model', 'Accuracy', 'AUC', 'Recall', 'F1']\n",
    "if 'Precision' in comparison_df.columns:\n",
    "    columns_to_display.insert(4, 'Precision')  # Add 'Precision' if it exists\n",
    "print(comparison_df[columns_to_display])\n",
    "\n",
    "# === Step 5: Finalize Best Model ===\n",
    "selected_model = best_models[0] if isinstance(best_models, list) else best_models\n",
    "finalized_model = finalize_model(selected_model)\n",
    "\n",
    "# === Step 6: Evaluate on Test Set ===\n",
    "test_predictions = predict_model(finalized_model, data=test_df, verbose=False)\n",
    "logger.log(\"Model evaluation on test set completed\")\n",
    "\n",
    "# Manual Accuracy (PyCaret's column is often NaN)\n",
    "accuracy = (test_predictions['prediction_label'] == test_predictions['relabeled']).mean()\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# === Step 7: Feature Importance ===\n",
    "def get_model_feature_importance(model, X, y, top_n=20):\n",
    "    try:\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            imp = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            imp = np.abs(model.coef_[0]) if model.coef_.ndim > 1 else np.abs(model.coef_)\n",
    "        else:\n",
    "            from sklearn.inspection import permutation_importance\n",
    "            result = permutation_importance(model, X, y, n_repeats=3, random_state=42, n_jobs=-1)\n",
    "            imp = result.importances_mean\n",
    "        return pd.DataFrame({'Feature': X.columns, 'Importance': imp}).sort_values(by='Importance', ascending=False).head(top_n)\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame({'Feature': ['Error'], 'Importance': [0]})\n",
    "logger.log(\"Feature importance extraction completed\")\n",
    "\n",
    "X_test = test_df.drop(columns=['relabeled', 'subject', 'filename', 'grasp', 'channel', 'window_id'])\n",
    "y_test = test_df['relabeled']\n",
    "importance_df = get_model_feature_importance(finalized_model, X_test, y_test)\n",
    "\n",
    "print(\"\\n=== TOP FEATURES ===\")\n",
    "print(importance_df)\n",
    "\n",
    "# === Runtime Summary ===\n",
    "print(f\"\\nTotal Runtime: {(time.time() - start_time_total):.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff50dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean DataFrame with optimal features\n",
    "# 1. Remove constant columns (already done)\n",
    "# 2. Remove columns with high NaN percentage (>20%)\n",
    "high_nan_columns = list(nan_percentage[nan_percentage > 20].index)\n",
    "\n",
    "# 3. Use the iteratively determined features to remove\n",
    "correlation_features_to_remove = [feature for feature, _ in removed_features]\n",
    "\n",
    "# Combine all columns to remove\n",
    "columns_to_remove = list(set(constant_column_names + high_nan_columns + correlation_features_to_remove))\n",
    "\n",
    "# Create optimized dataframe\n",
    "df_optimized = df.drop(columns=columns_to_remove)\n",
    "\n",
    "print(f\"\\nOptimized DataFrame Summary:\")\n",
    "print(f\"  - Original feature count: {len(feature_cols)}\")\n",
    "print(f\"  - Removed constant columns: {len(constant_column_names)}\")\n",
    "print(f\"  - Removed high-NaN columns: {len(high_nan_columns)}\")\n",
    "print(f\"  - Removed correlated features: {len(correlation_features_to_remove)}\")\n",
    "print(f\"  - Final feature count: {len([c for c in df_optimized.columns if c not in metadata_cols])}\")\n",
    "print(f\"  - Final shape: {df_optimized.shape}\")\n",
    "\n",
    "# Save the cleaned and optimized dataframe\n",
    "cleaned_file_path = os.path.join('preprocessed_data', database, f\"{os.path.splitext(filename)[0]}_cleaned.parquet\")\n",
    "df_optimized.to_parquet(cleaned_file_path, index=False)\n",
    "print(f\"\\nSaved cleaned dataframe to: {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac53f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyCaret's classification module\n",
    "from pycaret.classification import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress all warnings\n",
    "\n",
    "# Start timing for the entire process\n",
    "start_time_total = time.time()\n",
    "\n",
    "# Filter specific movements\n",
    "print(\"Step 1: Filtering data for selected grasp types...\")\n",
    "start_time = time.time()\n",
    "filtered_labels = [55, 2, 4, 14, 10, 16, 17, 19, 32]\n",
    "\n",
    "# Filter to include only the specified labels\n",
    "df_reduced = df_optimized[df_optimized['relabeled'].isin(filtered_labels)]\n",
    "print(f\"✓ Done in {time.time() - start_time:.2f}s - Filtered data shape: {df_reduced.shape}\")\n",
    "\n",
    "# SAMPLING - Uncommenting this will make your analysis much faster\n",
    "start_time = time.time()\n",
    "print(\"\\nStep 2: Sampling data for faster execution...\")\n",
    "sample_frac = 1  # Using 30% of data for faster results\n",
    "df_reduced = df_reduced.sample(frac=sample_frac, random_state=42)\n",
    "print(f\"✓ Done in {time.time() - start_time:.2f}s - Using {sample_frac*100:.0f}% sample with shape: {df_reduced.shape}\")\n",
    "\n",
    "# Split data for training\n",
    "### SUBJECT SPLITTING --- TESTES GENERALIZATION\n",
    "print(\"\\nStep 3: Splitting data by subjects...\")\n",
    "start_time = time.time()\n",
    "#subjects = df_reduced['subject'].unique()\n",
    "#test_subjects = np.random.choice(subjects, size=int(len(subjects)*0.2), replace=False)\n",
    "#train_df = df_reduced[~df_reduced['subject'].isin(test_subjects)]\n",
    "#test_df = df_reduced[df_reduced['subject'].isin(test_subjects)]\n",
    "\n",
    "### Traditioonal split (80/20) can be used as well\n",
    "train_df, test_df = train_test_split(df_reduced, test_size=0.2, random_state=42)\n",
    "print(f\"✓ Done in {time.time() - start_time:.2f}s - Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize PyCaret setup with maximum speed optimization\n",
    "print(\"\\nStep 4: Setting up PyCaret (this may take a few minutes)...\")\n",
    "start_time = time.time()\n",
    "s = setup(\n",
    "    data=train_df,\n",
    "    target='relabeled',\n",
    "    numeric_features=[col for col in train_df.columns \n",
    "                     if col not in metadata_cols and col != 'relabeled'],\n",
    "    ignore_features=['subject', 'filename', 'grasp', 'channel', 'window_id'],\n",
    "    normalize=True,\n",
    "    transformation=False,\n",
    "    feature_selection=False,\n",
    "    # feature_interaction=False,  # Removed as it is not a valid argument\n",
    "    polynomial_features=False,\n",
    "    fold=2,  # Minimum folds for speed\n",
    "    session_id=42,\n",
    "    verbose=False,\n",
    "    html=False,\n",
    "    n_jobs=-1,\n",
    "    use_gpu=True,\n",
    "    log_experiment=False,  # Disable logging for speed\n",
    "    experiment_name=None,  # Disable experiment name to avoid prints\n",
    "    # Removed 'silent' argument as it is not valid\n",
    "    profile=False  # Disable data profiling\n",
    ")\n",
    "print(f\"✓ Done in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Compare all fast models\n",
    "print(\"\\nStep 5: Training and comparing models...\")\n",
    "start_time = time.time()\n",
    "models_to_test = [\n",
    "    'lr',       # Logistic Regression\n",
    "    'dt',       # Decision Tree\n",
    "    'lightgbm', # LightGBM\n",
    "    'nb',       # Naive Bayes\n",
    "    'ridge',    # Ridge Classifier\n",
    "    'knn',      # K Nearest Neighbors\n",
    "    'ada'       # AdaBoost\n",
    "\n",
    "]\n",
    "\n",
    "best_models = compare_models(\n",
    "    include=models_to_test,\n",
    "    n_select=1,  # Only select the best model to save time\n",
    "    fold=2,\n",
    "    sort='Accuracy',\n",
    "    verbose=False,\n",
    "    turbo=True\n",
    ")\n",
    "print(f\"✓ Done in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Get the results table with all models compared\n",
    "print(\"\\n=== MODEL COMPARISON TABLE ===\")\n",
    "comparison_df = pull()\n",
    "print(comparison_df)\n",
    "\n",
    "# Select the model with best performance\n",
    "print(\"\\nStep 6: Selecting best model...\")\n",
    "start_time = time.time()\n",
    "if isinstance(best_models, list):\n",
    "    selected_model = best_models[0]\n",
    "else:\n",
    "    selected_model = best_models\n",
    "print(f\"✓ Done in {time.time() - start_time:.2f}s - Selected model: {selected_model.__class__.__name__}\")\n",
    "\n",
    "# Fix feature importance with a robust approach\n",
    "print(\"\\nStep 7: Calculating feature importance (with failsafe)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define a reliable function to get feature importance for any model\n",
    "def get_model_feature_importance(model, X, y, top_n=20):\n",
    "    \"\"\"Safe function to extract feature importance using multiple methods\"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "    importance_df = None\n",
    "    \n",
    "    # Method 1: Try model's built-in feature_importances_\n",
    "    try:\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importance\n",
    "            })\n",
    "            return importance_df.sort_values('Importance', ascending=False).head(top_n)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Method 2: Try model's coef_ attribute (for linear models)\n",
    "    try:\n",
    "        if hasattr(model, 'coef_'):\n",
    "            importance = np.abs(model.coef_[0]) if model.coef_.ndim > 1 else np.abs(model.coef_)\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importance\n",
    "            })\n",
    "            return importance_df.sort_values('Importance', ascending=False).head(top_n)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Method 3: Fallback to permutation importance (works with any model)\n",
    "    try:\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        result = permutation_importance(\n",
    "            model, X, y, \n",
    "            n_repeats=3,  # Lower for speed\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': result.importances_mean\n",
    "        })\n",
    "        return importance_df.sort_values('Importance', ascending=False).head(top_n)\n",
    "    except:\n",
    "        return pd.DataFrame({'Feature': [\"Error calculating importance\"], 'Importance': [0]})\n",
    "        \n",
    "# Extract X and y from test data\n",
    "X_test = test_df.drop(['relabeled', 'subject', 'filename', 'grasp', 'channel', 'window_id'], axis=1) \n",
    "y_test = test_df['relabeled']\n",
    "\n",
    "# Get the best model and finalize it\n",
    "try:\n",
    "    finalized_model = finalize_model(selected_model)\n",
    "    model_sklearn = finalized_model\n",
    "except:\n",
    "    model_sklearn = selected_model  # Use the original model if finalize fails\n",
    "\n",
    "# Get feature importance table with our reliable function\n",
    "importance_df = get_model_feature_importance(model_sklearn, X_test, y_test)\n",
    "\n",
    "print(f\"✓ Done in {time.time() - start_time:.2f}s\")\n",
    "print(\"\\n=== FEATURE IMPORTANCE (TOP 20) ===\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"\\nStep 8: Evaluating model on test set...\")\n",
    "start_time = time.time()\n",
    "test_predictions = predict_model(selected_model, data=test_df, verbose=False)\n",
    "print(f\"✓ Done in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "print(\"\\n=== TEST SET PERFORMANCE ===\")\n",
    "if 'Accuracy' in test_predictions.columns:\n",
    "    print(f\"Accuracy: {test_predictions['Accuracy'].mean():.4f}\")\n",
    "else:\n",
    "    # Calculate accuracy manually\n",
    "    correct = sum(test_predictions['prediction_label'] == test_predictions['relabeled'])\n",
    "    accuracy = correct / len(test_predictions)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Report total runtime\n",
    "total_time = time.time() - start_time_total\n",
    "print(f\"\\nTotal execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528fc33",
   "metadata": {},
   "source": [
    "### 200 ms Window with original signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d9078b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "subject",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "grasp",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "relabeled",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "channel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "window_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "MAV",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "MAV_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "IAV",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "IAV_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "RMS",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "RMS_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "WL",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "WL_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "ZC",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "ZC_STD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VAR",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "VAR_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "TD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "TD_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "MAVS",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "MAVS_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "MNP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MNP_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "SSC",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "SSC_STD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CoV",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "MNF",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MNF_STD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mDWT",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "mDWT_STD",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "MDF",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PKF",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TTP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SM1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SM2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SM3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MNPF",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SampEn",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CC",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "LE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "HFD",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DASDV",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "MYOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WAMP",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "CARD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOG",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "SKEW",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "KURT",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "915e755d-9e49-4a98-af25-0ad3c7b43110",
       "rows": [
        [
         "0",
         "s1",
         "S1_E1_A1.mat",
         "0",
         "0",
         "Channel 1",
         "0",
         "0.07268493",
         "0.056120064",
         "29.07397",
         "0.056120064",
         "0.09182897",
         "0.0912947",
         "18.172205",
         "0.038477883",
         "72",
         "0.38456276248253896",
         "0.008334721",
         "0.0912947",
         "18.172205",
         "0.038477883",
         "0.045544375",
         "0.038477883",
         "3.334579222159665",
         "0.01396844",
         "169",
         "0.49428567174743776",
         "9.229766",
         "287.77991585656235",
         "0.9193194277447894",
         "26.970665",
         "1.7784784",
         "170.0",
         "75.0",
         "666.915844431933",
         "181.41443079416754",
         "20388.942767392295",
         "6900135.080302768",
         "181.41443079416754",
         "1.2560334010149452",
         "0.045544375",
         "-1.6278291351172431",
         "0",
         "0.05962246",
         "0.8725",
         "329",
         "0.86",
         "0.049659815",
         "0.2776951491832733",
         "3.6414504051208496"
        ],
        [
         "1",
         "s1",
         "S1_E1_A1.mat",
         "0",
         "0",
         "Channel 1",
         "1",
         "0.0685611",
         "0.059857458",
         "27.424438",
         "0.059857458",
         "0.091013946",
         "0.09067748",
         "19.511942",
         "0.037520465",
         "89",
         "0.4162966868883889",
         "0.008222405",
         "0.09067748",
         "19.511942",
         "0.037520465",
         "0.04890211",
         "0.037520465",
         "3.291453164681468",
         "0.013660782",
         "173",
         "0.49571406173238336",
         "11.59729",
         "293.9386586691443",
         "0.9181502100667345",
         "29.239788",
         "1.7800525",
         "155.0",
         "180.0",
         "658.2906329362936",
         "191.35579567895485",
         "23404.60059815331",
         "8670363.30008563",
         "191.35579567895485",
         "1.3225791008098295",
         "0.04890211",
         "-1.448815290649264",
         "0",
         "0.06163767",
         "0.8125",
         "351",
         "0.86",
         "0.042523373",
         "0.39877432584762573",
         "3.593000888824463"
        ],
        [
         "2",
         "s1",
         "S1_E1_A1.mat",
         "0",
         "0",
         "Channel 1",
         "2",
         "0.07661761",
         "0.06560446",
         "30.647045",
         "0.06560446",
         "0.10086726",
         "0.10016721",
         "21.682623",
         "0.044764202",
         "91",
         "0.41958809624917187",
         "0.01003347",
         "0.10016721",
         "21.682623",
         "0.044764202",
         "0.05434241",
         "0.044764202",
         "4.013421411449569",
         "0.018106185",
         "161",
         "0.4907994178726444",
         "8.443518",
         "302.2876761957127",
         "0.9526968183768609",
         "28.44428",
         "3.0167978",
         "180.0",
         "170.0",
         "802.6842822899138",
         "206.46829277031551",
         "20198.789171961314",
         "6977230.912360582",
         "206.46829277031551",
         "1.307365275406118",
         "0.05434241",
         "-1.2270321979283079",
         "0",
         "0.07040548",
         "0.87",
         "347",
         "0.88",
         "0.049029414",
         "0.2888721227645874",
         "4.0635480880737305"
        ],
        [
         "3",
         "s1",
         "S1_E1_A1.mat",
         "0",
         "0",
         "Channel 1",
         "3",
         "0.06762289",
         "0.061000116",
         "27.049158",
         "0.061000116",
         "0.09107069",
         "0.090551816",
         "19.083462",
         "0.039921794",
         "94",
         "0.4243663611785771",
         "0.008199631",
         "0.090551816",
         "19.083462",
         "0.039921794",
         "0.047828224",
         "0.039921794",
         "3.281449879945573",
         "0.015473839",
         "183",
         "0.4983812588721307",
         "9.3278265",
         "310.973422516914",
         "0.9530742630018255",
         "32.41613",
         "2.1232061",
         "175.0",
         "40.0",
         "656.2899759891146",
         "196.97137608184133",
         "25122.83728472423",
         "9154284.783832913",
         "196.97137608184133",
         "1.339070429657915",
         "0.047828224",
         "-1.3927402330756982",
         "0",
         "0.06229999",
         "0.8125",
         "338",
         "0.84",
         "0.041330677",
         "0.3430921137332916",
         "4.368191719055176"
        ],
        [
         "4",
         "s1",
         "S1_E1_A1.mat",
         "0",
         "0",
         "Channel 1",
         "4",
         "0.0633938",
         "0.05752819",
         "25.35752",
         "0.05752819",
         "0.08560529",
         "0.08515195",
         "18.244017",
         "0.041090827",
         "95",
         "0.4259177099999599",
         "0.007250854",
         "0.08515195",
         "18.244017",
         "0.041090827",
         "0.04572435",
         "0.041090827",
         "2.9025849268462247",
         "0.014352459",
         "184",
         "0.49857755798241055",
         "9.678132",
         "306.25112361332924",
         "0.9398386825956875",
         "32.61578",
         "2.1468825",
         "180.0",
         "140.0",
         "580.5169853692449",
         "210.53629092249957",
         "22849.931673300387",
         "8249733.291984299",
         "210.53629092249957",
         "1.3432219862477017",
         "0.04572435",
         "-1.2772314864132563",
         "0",
         "0.06147498",
         "0.8125",
         "330",
         "0.9",
         "0.03842084",
         "0.7280833721160889",
         "4.574465274810791"
        ]
       ],
       "shape": {
        "columns": 49,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>filename</th>\n",
       "      <th>grasp</th>\n",
       "      <th>relabeled</th>\n",
       "      <th>channel</th>\n",
       "      <th>window_id</th>\n",
       "      <th>MAV</th>\n",
       "      <th>MAV_STD</th>\n",
       "      <th>IAV</th>\n",
       "      <th>IAV_STD</th>\n",
       "      <th>...</th>\n",
       "      <th>CC</th>\n",
       "      <th>LE</th>\n",
       "      <th>HFD</th>\n",
       "      <th>DASDV</th>\n",
       "      <th>MYOP</th>\n",
       "      <th>WAMP</th>\n",
       "      <th>CARD</th>\n",
       "      <th>LOG</th>\n",
       "      <th>SKEW</th>\n",
       "      <th>KURT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>S1_E1_A1.mat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Channel 1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072685</td>\n",
       "      <td>0.056120</td>\n",
       "      <td>29.073971</td>\n",
       "      <td>0.056120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045544</td>\n",
       "      <td>-1.627829</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059622</td>\n",
       "      <td>0.8725</td>\n",
       "      <td>329</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.049660</td>\n",
       "      <td>0.277695</td>\n",
       "      <td>3.641450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s1</td>\n",
       "      <td>S1_E1_A1.mat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Channel 1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.068561</td>\n",
       "      <td>0.059857</td>\n",
       "      <td>27.424438</td>\n",
       "      <td>0.059857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048902</td>\n",
       "      <td>-1.448815</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061638</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>351</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.042523</td>\n",
       "      <td>0.398774</td>\n",
       "      <td>3.593001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s1</td>\n",
       "      <td>S1_E1_A1.mat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Channel 1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.076618</td>\n",
       "      <td>0.065604</td>\n",
       "      <td>30.647045</td>\n",
       "      <td>0.065604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054342</td>\n",
       "      <td>-1.227032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070405</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>347</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.049029</td>\n",
       "      <td>0.288872</td>\n",
       "      <td>4.063548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s1</td>\n",
       "      <td>S1_E1_A1.mat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Channel 1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.067623</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>27.049158</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047828</td>\n",
       "      <td>-1.392740</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>338</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.041331</td>\n",
       "      <td>0.343092</td>\n",
       "      <td>4.368192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s1</td>\n",
       "      <td>S1_E1_A1.mat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Channel 1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063394</td>\n",
       "      <td>0.057528</td>\n",
       "      <td>25.357519</td>\n",
       "      <td>0.057528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045724</td>\n",
       "      <td>-1.277231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>330</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.038421</td>\n",
       "      <td>0.728083</td>\n",
       "      <td>4.574465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject      filename  grasp  relabeled    channel  window_id       MAV  \\\n",
       "0      s1  S1_E1_A1.mat      0          0  Channel 1          0  0.072685   \n",
       "1      s1  S1_E1_A1.mat      0          0  Channel 1          1  0.068561   \n",
       "2      s1  S1_E1_A1.mat      0          0  Channel 1          2  0.076618   \n",
       "3      s1  S1_E1_A1.mat      0          0  Channel 1          3  0.067623   \n",
       "4      s1  S1_E1_A1.mat      0          0  Channel 1          4  0.063394   \n",
       "\n",
       "    MAV_STD        IAV   IAV_STD  ...        CC        LE  HFD     DASDV  \\\n",
       "0  0.056120  29.073971  0.056120  ...  0.045544 -1.627829    0  0.059622   \n",
       "1  0.059857  27.424438  0.059857  ...  0.048902 -1.448815    0  0.061638   \n",
       "2  0.065604  30.647045  0.065604  ...  0.054342 -1.227032    0  0.070405   \n",
       "3  0.061000  27.049158  0.061000  ...  0.047828 -1.392740    0  0.062300   \n",
       "4  0.057528  25.357519  0.057528  ...  0.045724 -1.277231    0  0.061475   \n",
       "\n",
       "     MYOP  WAMP  CARD       LOG      SKEW      KURT  \n",
       "0  0.8725   329  0.86  0.049660  0.277695  3.641450  \n",
       "1  0.8125   351  0.86  0.042523  0.398774  3.593001  \n",
       "2  0.8700   347  0.88  0.049029  0.288872  4.063548  \n",
       "3  0.8125   338  0.84  0.041331  0.343092  4.368192  \n",
       "4  0.8125   330  0.90  0.038421  0.728083  4.574465  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Replace 'your_file.parquet' with the actual file name\n",
    "filename = 'DB4_w400_env0_f0.6[20250406-201248].parquet'\n",
    "file_path = 'preprocessed_data'\n",
    "database = 'DB4'\n",
    "\n",
    "file_path = os.path.join(file_path, database, filename)\n",
    "\n",
    "# Load the Parquet file into a pandas DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df764e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 columns with constant values:\n",
      "  - HFD: 0\n",
      "\n",
      "Shape after removing constant columns: (1638756, 48)\n"
     ]
    }
   ],
   "source": [
    "# 1. First, let's identify columns with constant values\n",
    "def identify_constant_columns(df):\n",
    "    \"\"\"\n",
    "    Identify columns that contain only a single value.\n",
    "    Returns a list of column names with constant values.\n",
    "    \"\"\"\n",
    "    constant_columns = []\n",
    "    for col in df.columns:\n",
    "        # Check if the column has only one unique value\n",
    "        if df[col].nunique() == 1:\n",
    "            constant_value = df[col].iloc[0]\n",
    "            constant_columns.append((col, constant_value))\n",
    "    return constant_columns\n",
    "\n",
    "# 3. Apply the functions to identify constant columns and rows\n",
    "constant_columns = identify_constant_columns(df)\n",
    "print(f\"Found {len(constant_columns)} columns with constant values:\")\n",
    "for col, value in constant_columns:\n",
    "    print(f\"  - {col}: {value}\")\n",
    "\n",
    "# 4. Create a cleaned dataframe with constant columns removed\n",
    "constant_column_names = [col for col, _ in constant_columns]\n",
    "df_cleaned = df.drop(columns=constant_column_names)\n",
    "print(f\"\\nShape after removing constant columns: {df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1529a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features with highly skewed distributions (skewness > 3):\n",
      "  - CoV: 1158.67 (skewed right)\n",
      "  - MNP_STD: 368.17 (skewed right)\n",
      "  - TTP: 84.32 (skewed right)\n",
      "  - MNP: 84.32 (skewed right)\n",
      "  - VAR: 84.31 (skewed right)\n",
      "  - WAMP: -9.63 (skewed left)\n",
      "  - KURT: 8.68 (skewed right)\n",
      "  - SSC_STD: -6.79 (skewed left)\n",
      "  - mDWT_STD: 6.48 (skewed right)\n",
      "  - WL_STD: 6.33 (skewed right)\n",
      "  - TD_STD: 6.33 (skewed right)\n",
      "  - MAVS_STD: 6.33 (skewed right)\n",
      "  - LE: -5.43 (skewed left)\n",
      "  - MAV_STD: 4.46 (skewed right)\n",
      "  - IAV_STD: 4.46 (skewed right)\n",
      "  - DASDV: 4.36 (skewed right)\n",
      "  - RMS: 3.85 (skewed right)\n",
      "  - RMS_STD: 3.83 (skewed right)\n",
      "  - VAR_STD: 3.83 (skewed right)\n",
      "  - LOG: 3.66 (skewed right)\n",
      "  - MAV: 3.60 (skewed right)\n",
      "  - IAV: 3.60 (skewed right)\n",
      "  - mDWT: 3.55 (skewed right)\n",
      "  - PKF: 3.50 (skewed right)\n",
      "  - WL: 3.39 (skewed right)\n",
      "  - TD: 3.39 (skewed right)\n",
      "  - MAVS: 3.39 (skewed right)\n",
      "  - CC: 3.39 (skewed right)\n"
     ]
    }
   ],
   "source": [
    "# Check feature distributions - focus on skewness\n",
    "print(\"\\nFeatures with highly skewed distributions (skewness > 3):\")\n",
    "skewed_features = {}\n",
    "\n",
    "for col in feature_cols:\n",
    "    if df[col].nunique() <= 1:\n",
    "        continue\n",
    "    \n",
    "    # Calculate skewness, ignoring NaN values\n",
    "    skewness = df[col].skew()\n",
    "    \n",
    "    # Only report features with high skewness (absolute value > 3)\n",
    "    if abs(skewness) > 3:\n",
    "        skewed_features[col] = skewness\n",
    "\n",
    "if skewed_features:\n",
    "    sorted_skewed = dict(sorted(skewed_features.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "    for col, skew in sorted_skewed.items():\n",
    "        direction = \"right\" if skew > 0 else \"left\"\n",
    "        print(f\"  - {col}: {skew:.2f} (skewed {direction})\")\n",
    "else:\n",
    "    print(\"  No highly skewed features found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c4c7b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial feature count: 42\n",
      "Removed 11 highly correlated features (>|0.98|):\n",
      "\n",
      "  - Removed 'MAV' instead of 'IAV' (corr=1.0000) [Group: amplitude]\n",
      "  - Removed 'RMS' instead of 'MAV' (corr=0.9928) [Group: amplitude]\n",
      "  - Removed 'MAV_STD' instead of 'IAV_STD' (corr=1.0000) [Group: std]\n",
      "  - Removed 'RMS_STD' instead of 'MAV_STD' (corr=0.9924) [Group: std]\n",
      "  - Removed 'VAR_STD' instead of 'MAV_STD' (corr=0.9924) [Group: std]\n",
      "  - Removed 'IAV' instead of 'LOG' (corr=0.9914) [Group: amplitude]\n",
      "  - Removed 'WL' instead of 'TD' (corr=1.0000) [Group: change]\n",
      "  - Removed 'WL_STD' instead of 'TD_STD' (corr=1.0000) [Group: std]\n",
      "  - Removed 'TD' instead of 'MAVS' (corr=1.0000) [Group: change]\n",
      "  - Removed 'TD_STD' instead of 'MAVS_STD' (corr=1.0000) [Group: std]\n",
      "  - Removed 'SM1' instead of 'MNPF' (corr=1.0000) [Group: freq]\n",
      "\n",
      "Final retained features: 31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your feature groups\n",
    "feature_groups = {\n",
    "    'amplitude': [\n",
    "        'MAV', 'IAV', 'RMS', 'TTP', 'LOG', 'MYOP', 'WAMP', 'DASDV', 'CoV'\n",
    "    ],\n",
    "    'change': [\n",
    "        'WL', 'ZC', 'SSC', 'TD', 'MAVS'\n",
    "    ],\n",
    "    'moments': [\n",
    "        'VAR', 'SKEW', 'KURT'\n",
    "    ],\n",
    "    'std': [\n",
    "        'MAV_STD', 'IAV_STD', 'RMS_STD', 'WL_STD', 'ZC_STD',\n",
    "        'SSC_STD', 'VAR_STD', 'TD_STD', 'MAVS_STD'\n",
    "    ],\n",
    "    'freq': [\n",
    "        'MDF', 'PKF', 'MNF', 'TTP', 'SM1', 'SM2', 'SM3', 'MNPF'\n",
    "    ],\n",
    "    'wavelet': ['mDWT', 'mDWT_STD'],\n",
    "    'complexity': ['SampEn', 'CC', 'LE', 'HFD']\n",
    "}\n",
    "\n",
    "# Build a reverse index: feature -> group\n",
    "feature_to_group = {}\n",
    "for group, features in feature_groups.items():\n",
    "    for f in features:\n",
    "        feature_to_group[f] = group\n",
    "\n",
    "# Exclude metadata\n",
    "metadata_cols = ['subject', 'filename', 'grasp', 'relabeled', 'channel', 'window_id']\n",
    "feature_cols = [col for col in df_cleaned.columns if col not in metadata_cols]\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_threshold = 0.98\n",
    "corr_matrix = df_cleaned[feature_cols].corr().abs()\n",
    "\n",
    "# Track what gets removed\n",
    "to_remove = set()\n",
    "removed_pairs = []\n",
    "\n",
    "# Loop through upper triangle of correlation matrix\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        f1 = corr_matrix.columns[i]\n",
    "        f2 = corr_matrix.columns[j]\n",
    "        corr = corr_matrix.iloc[i, j]\n",
    "\n",
    "        if corr > corr_threshold:\n",
    "            group1 = feature_to_group.get(f1, 'unknown')\n",
    "            group2 = feature_to_group.get(f2, 'unknown')\n",
    "\n",
    "            # Prefer to keep features from different groups\n",
    "            if group1 != group2:\n",
    "                continue\n",
    "\n",
    "            # If from same group, remove the one with higher mean correlation to others\n",
    "            mean_corr_f1 = corr_matrix[f1].mean()\n",
    "            mean_corr_f2 = corr_matrix[f2].mean()\n",
    "\n",
    "            if mean_corr_f1 >= mean_corr_f2:\n",
    "                if f1 not in to_remove:\n",
    "                    to_remove.add(f1)\n",
    "                    removed_pairs.append((f1, f2, corr))\n",
    "            else:\n",
    "                if f2 not in to_remove:\n",
    "                    to_remove.add(f2)\n",
    "                    removed_pairs.append((f2, f1, corr))\n",
    "\n",
    "# Filter features\n",
    "final_features = [f for f in feature_cols if f not in to_remove]\n",
    "\n",
    "# --- Reporting ---\n",
    "print(f\"Initial feature count: {len(feature_cols)}\")\n",
    "print(f\"Removed {len(to_remove)} highly correlated features (>|{corr_threshold}|):\\n\")\n",
    "for f_rm, f_keep, corr in removed_pairs:\n",
    "    print(f\"  - Removed '{f_rm}' instead of '{f_keep}' (corr={corr:.4f}) [Group: {feature_to_group.get(f_rm)}]\")\n",
    "\n",
    "print(f\"\\nFinal retained features: {len(final_features)}\")\n",
    "\n",
    "# Optionally: update df\n",
    "df_selected = df_cleaned[metadata_cols + final_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48bde4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train/Test split: 109756 train, 27440 test\n",
      "[INFO] PyCaret setup completed\n",
      "[INFO] Model comparison completed\n",
      "\n",
      "=== MODEL COMPARISON ===\n",
      "                                    Model  Accuracy     AUC  Recall      F1\n",
      "rf               Random Forest Classifier    0.2711  0.6989  0.2711  0.2660\n",
      "et                 Extra Trees Classifier    0.2668  0.6961  0.2668  0.2618\n",
      "lightgbm  Light Gradient Boosting Machine    0.2649  0.6968  0.2649  0.2557\n",
      "gbc          Gradient Boosting Classifier    0.2472  0.0000  0.2472  0.2336\n",
      "lr                    Logistic Regression    0.2186  0.0000  0.2186  0.1969\n",
      "ridge                    Ridge Classifier    0.2154  0.0000  0.2154  0.1894\n",
      "ada                  Ada Boost Classifier    0.2089  0.0000  0.2089  0.1902\n",
      "knn                K Neighbors Classifier    0.2077  0.6084  0.2077  0.2081\n",
      "dt               Decision Tree Classifier    0.1897  0.5441  0.1897  0.1898\n",
      "qda       Quadratic Discriminant Analysis    0.1807  0.0000  0.1807  0.1568\n",
      "nb                            Naive Bayes    0.1715  0.5736  0.1715  0.1266\n",
      "svm                   SVM - Linear Kernel    0.1518  0.0000  0.1518  0.1394\n",
      "dummy                    Dummy Classifier    0.1192  0.5000  0.1192  0.0254\n",
      "[INFO] Model evaluation on test set completed\n",
      "\n",
      "Test Accuracy: 0.2866\n",
      "[INFO] Feature importance extraction completed\n",
      "\n",
      "=== TOP FEATURES ===\n",
      "     Feature  Importance\n",
      "29      SKEW    0.039519\n",
      "19       SM3    0.038834\n",
      "12   MNF_STD    0.037979\n",
      "23        LE    0.037888\n",
      "20      MNPF    0.037132\n",
      "18       SM2    0.036597\n",
      "10       CoV    0.036516\n",
      "5   MAVS_STD    0.036470\n",
      "14  mDWT_STD    0.035533\n",
      "21    SampEn    0.034718\n",
      "11       MNF    0.034684\n",
      "13      mDWT    0.034601\n",
      "8        SSC    0.033654\n",
      "30      KURT    0.033643\n",
      "22        CC    0.033528\n",
      "28       LOG    0.033425\n",
      "24     DASDV    0.033196\n",
      "4       MAVS    0.033117\n",
      "9    SSC_STD    0.031088\n",
      "7    MNP_STD    0.030144\n",
      "\n",
      "Total Runtime: 696.41 seconds\n"
     ]
    }
   ],
   "source": [
    "from pycaret.classification import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = Logger(verbose=True, log_to_file=False)\n",
    "\n",
    "# Measure total time\n",
    "start_time_total = time.time()\n",
    "\n",
    "# === Step 1: Filter data for selected classes ===\n",
    "filtered_labels = [55, 2, 4, 14, 10, 16, 17, 19, 32]\n",
    "df_reduced = df_selected[df_selected['relabeled'].isin(filtered_labels)].sample(frac=1, random_state=42)\n",
    "\n",
    "models_to_test = [\n",
    "    'dummy',    # Dummy classifier (baseline)\n",
    "    'lr',       # Logistic Regression\n",
    "    'dt',       # Decision Tree\n",
    "    'lightgbm', # LightGBM\n",
    "    'nb',       # Naive Bayes\n",
    "    'ridge',    # Ridge Classifier\n",
    "    'knn',      # K Nearest Neighbors\n",
    "    'ada',      # AdaBoost\n",
    "    'qda',      # Quadratic Discriminant Analysis\n",
    "    'et',       # Extra Trees Classifier\n",
    "    'rf',       # Random Forest\n",
    "    'gbc',      # Gradient Boosting Classifier\n",
    "    'svm'       # Support Vector Machine (Linear)\n",
    "]\n",
    "\n",
    "# === Step 2: Train/Test Split ===\n",
    "train_df, test_df = train_test_split(df_reduced, test_size=0.2, random_state=42)\n",
    "logger.log(f\"Train/Test split: {len(train_df)} train, {len(test_df)} test\")\n",
    "\n",
    "# === Step 3: PyCaret Setup (silent) ===\n",
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    s = setup(\n",
    "        data=train_df,\n",
    "        target='relabeled',\n",
    "        numeric_features=[col for col in train_df.columns \n",
    "                          if col not in metadata_cols and col != 'relabeled'],\n",
    "        ignore_features=['subject', 'filename', 'grasp', 'channel', 'window_id'],\n",
    "        normalize=True,\n",
    "        transformation=False,\n",
    "        feature_selection=False,\n",
    "        polynomial_features=False,\n",
    "        fold=2,\n",
    "        session_id=42,\n",
    "        verbose=False,\n",
    "        html=False,\n",
    "        n_jobs=-1,\n",
    "        use_gpu=True,\n",
    "        log_experiment=False,\n",
    "        profile=False\n",
    "    )\n",
    "logger.log(\"PyCaret setup completed\")\n",
    "\n",
    "# === Step 4: Compare Models ===\n",
    "with redirect_stdout(f):\n",
    "    best_models = compare_models(\n",
    "        include=models_to_test,\n",
    "        n_select=1,\n",
    "        fold=2,\n",
    "        sort='Accuracy',\n",
    "        verbose=False,\n",
    "        turbo=True\n",
    "    )\n",
    "logger.log(\"Model comparison completed\")\n",
    "\n",
    "# Pull comparison results\n",
    "comparison_df = pull()\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "# Check if 'Precision' exists in the DataFrame\n",
    "columns_to_display = ['Model', 'Accuracy', 'AUC', 'Recall', 'F1']\n",
    "if 'Precision' in comparison_df.columns:\n",
    "    columns_to_display.insert(4, 'Precision')  # Add 'Precision' if it exists\n",
    "print(comparison_df[columns_to_display])\n",
    "\n",
    "# === Step 5: Finalize Best Model ===\n",
    "selected_model = best_models[0] if isinstance(best_models, list) else best_models\n",
    "finalized_model = finalize_model(selected_model)\n",
    "\n",
    "# === Step 6: Evaluate on Test Set ===\n",
    "test_predictions = predict_model(finalized_model, data=test_df, verbose=False)\n",
    "logger.log(\"Model evaluation on test set completed\")\n",
    "\n",
    "# Manual Accuracy (PyCaret's column is often NaN)\n",
    "accuracy = (test_predictions['prediction_label'] == test_predictions['relabeled']).mean()\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# === Step 7: Feature Importance ===\n",
    "def get_model_feature_importance(model, X, y, top_n=20):\n",
    "    try:\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            imp = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            imp = np.abs(model.coef_[0]) if model.coef_.ndim > 1 else np.abs(model.coef_)\n",
    "        else:\n",
    "            from sklearn.inspection import permutation_importance\n",
    "            result = permutation_importance(model, X, y, n_repeats=3, random_state=42, n_jobs=-1)\n",
    "            imp = result.importances_mean\n",
    "        return pd.DataFrame({'Feature': X.columns, 'Importance': imp}).sort_values(by='Importance', ascending=False).head(top_n)\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame({'Feature': ['Error'], 'Importance': [0]})\n",
    "logger.log(\"Feature importance extraction completed\")\n",
    "\n",
    "X_test = test_df.drop(columns=['relabeled', 'subject', 'filename', 'grasp', 'channel', 'window_id'])\n",
    "y_test = test_df['relabeled']\n",
    "importance_df = get_model_feature_importance(finalized_model, X_test, y_test)\n",
    "\n",
    "print(\"\\n=== TOP FEATURES ===\")\n",
    "print(importance_df)\n",
    "\n",
    "# === Runtime Summary ===\n",
    "print(f\"\\nTotal Runtime: {(time.time() - start_time_total):.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c54ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RMS_E1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAV_E1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VARIANCE_E1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAMPLE_VARIANCE_E1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMS_E2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAV_E2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VARIANCE_E2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAMPLE_VARIANCE_E2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMS_E3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAV_E3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VARIANCE_E3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAMPLE_VARIANCE_E3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMS_E4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAV_E4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VARIANCE_E4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAMPLE_VARIANCE_E4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMS_E5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAV_E5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VARIANCE_E5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAMPLE_VARIANCE_E5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMS_E6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAV_E6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VARIANCE_E6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAMPLE_VARIANCE_E6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMS_E7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAV_E7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VARIANCE_E7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAMPLE_VARIANCE_E7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMS_E8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAV_E8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VARIANCE_E8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SAMPLE_VARIANCE_E8",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c0aec09d-c974-4a16-a245-1d1d1ab519f0",
       "rows": [
        [
         "0",
         "Base",
         "0.20001384835154465",
         "0.19997006084117153",
         "1.7514299572835765e-05",
         "1.7811152107968574e-05",
         "0.20246093487402514",
         "0.19748541338960374",
         "0.0019899416484015957",
         "0.0020236694729507754",
         "0.4597231112279025",
         "0.44644347019234504",
         "0.012033566919679044",
         "0.012237525681029536",
         "0.2266081878261101",
         "0.20985270796358335",
         "0.0073131117501846294",
         "0.007437062796797928",
         "0.029801979359259427",
         "0.028251628103063173",
         "9.000348325593653e-05",
         "9.15289660229863e-05",
         "0.046638792913833715",
         "0.046356739925196004",
         "2.6229667967203704e-05",
         "2.667423861071563e-05",
         "0.3734380509041634",
         "0.3669134568690585",
         "0.004830493031498027",
         "0.004912365794743756",
         "0.3220708708132969",
         "0.3182581562844591",
         "0.0024413917848522568",
         "0.002482771306629414"
        ],
        [
         "1",
         "Base",
         "0.19381643528425405",
         "0.19372042384054877",
         "3.720797333357596e-05",
         "3.783861694939928e-05",
         "0.15487304438773902",
         "0.15421415210942035",
         "0.00020365516709914958",
         "0.0002071069495923555",
         "0.34289584208361673",
         "0.3411958422949567",
         "0.0011629557188676668",
         "0.0011826668327467797",
         "0.13687753496663918",
         "0.13523287496302144",
         "0.00044752910777933673",
         "0.0004551143468942407",
         "0.019901526193625875",
         "0.01974154135915312",
         "6.3422896004238585e-06",
         "6.449786034329348e-06",
         "0.040112707738138485",
         "0.04002230085581729",
         "7.244756291761726e-06",
         "7.367548771283112e-06",
         "0.29452618456404406",
         "0.2932455278213296",
         "0.000752733806643116",
         "0.0007654920067557112",
         "0.2661489090246208",
         "0.2653411425859517",
         "0.00042931982617752857",
         "0.0004365964334008765"
        ],
        [
         "2",
         "Base",
         "0.18173552411805877",
         "0.18156147718911078",
         "6.323072737353067e-05",
         "6.430243461714984e-05",
         "0.13438295857785734",
         "0.13405735909885713",
         "8.740402757818666e-05",
         "8.888545177442711e-05",
         "0.2944030836844478",
         "0.2935989724913808",
         "0.0004728190349174314",
         "0.00048083291686518446",
         "0.10795197979346542",
         "0.10729360498237987",
         "0.00014171227121380028",
         "0.00014411417411572911",
         "0.016408234718667272",
         "0.016340169578657374",
         "2.229024723598106e-06",
         "2.266804803659091e-06",
         "0.03599905784210041",
         "0.03594065186785869",
         "4.201708832276011e-06",
         "4.272924236212894e-06",
         "0.25378494095813026",
         "0.2530418569137451",
         "0.00037661490676539313",
         "0.00038299821026989135",
         "0.23362679017128807",
         "0.23302119030161425",
         "0.0002826019561579642",
         "0.00028739181982165854"
        ],
        [
         "3",
         "Base",
         "0.1667252177396074",
         "0.1664547150424466",
         "9.012607045737768e-05",
         "9.165363097360443e-05",
         "0.12016657797268336",
         "0.11996016753662443",
         "4.956466624998159e-05",
         "5.040474533896433e-05",
         "0.26169497331956537",
         "0.2612085945633155",
         "0.0002543291869854733",
         "0.0002586398511716678",
         "0.09068480754744838",
         "0.09033731581532646",
         "6.290369119972032e-05",
         "6.39698554573427e-05",
         "0.014195997274377424",
         "0.014156792047589394",
         "1.1115775354410609e-06",
         "1.1304178326519262e-06",
         "0.032769347488507045",
         "0.03272662529240027",
         "2.798131793349807e-06",
         "2.845557755948956e-06",
         "0.2244141825794222",
         "0.22395380407308615",
         "0.00020641898398396927",
         "0.0002099176108311552",
         "0.20615116431071637",
         "0.20561117250941757",
         "0.00022234828596650078",
         "0.00022611690098288216"
        ],
        [
         "4",
         "Base",
         "0.14956350499526735",
         "0.14920154341113653",
         "0.00010814147020408623",
         "0.00010997437647873176",
         "0.10913932768547266",
         "0.1089914737765551",
         "3.225149165147424e-05",
         "3.279812710319414e-05",
         "0.23698480161657884",
         "0.23665228892079787",
         "0.00015749034579645176",
         "0.00016015967369130687",
         "0.07878985345239042",
         "0.07857816577973739",
         "3.331286974126322e-05",
         "3.387749465213208e-05",
         "0.012571924841862415",
         "0.01254525030092641",
         "6.699891165431616e-07",
         "6.813448642811813e-07",
         "0.030091964685836546",
         "0.030058723657647057",
         "1.999470726843126e-06",
         "2.033360061196399e-06",
         "0.20239842956615237",
         "0.2020996069099921",
         "0.0001208731776714106",
         "0.00012292187559804468",
         "0.18134014186353098",
         "0.18081906264629558",
         "0.00018871363480056565",
         "0.0001919121709836261"
        ]
       ],
       "shape": {
        "columns": 33,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>RMS_E1</th>\n",
       "      <th>MAV_E1</th>\n",
       "      <th>VARIANCE_E1</th>\n",
       "      <th>SAMPLE_VARIANCE_E1</th>\n",
       "      <th>RMS_E2</th>\n",
       "      <th>MAV_E2</th>\n",
       "      <th>VARIANCE_E2</th>\n",
       "      <th>SAMPLE_VARIANCE_E2</th>\n",
       "      <th>RMS_E3</th>\n",
       "      <th>...</th>\n",
       "      <th>VARIANCE_E6</th>\n",
       "      <th>SAMPLE_VARIANCE_E6</th>\n",
       "      <th>RMS_E7</th>\n",
       "      <th>MAV_E7</th>\n",
       "      <th>VARIANCE_E7</th>\n",
       "      <th>SAMPLE_VARIANCE_E7</th>\n",
       "      <th>RMS_E8</th>\n",
       "      <th>MAV_E8</th>\n",
       "      <th>VARIANCE_E8</th>\n",
       "      <th>SAMPLE_VARIANCE_E8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.200014</td>\n",
       "      <td>0.199970</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.202461</td>\n",
       "      <td>0.197485</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.459723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.373438</td>\n",
       "      <td>0.366913</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.004912</td>\n",
       "      <td>0.322071</td>\n",
       "      <td>0.318258</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.002483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.193816</td>\n",
       "      <td>0.193720</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.154873</td>\n",
       "      <td>0.154214</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.342896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.294526</td>\n",
       "      <td>0.293246</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.266149</td>\n",
       "      <td>0.265341</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.181736</td>\n",
       "      <td>0.181561</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.134383</td>\n",
       "      <td>0.134057</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.294403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.253785</td>\n",
       "      <td>0.253042</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.233627</td>\n",
       "      <td>0.233021</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.166725</td>\n",
       "      <td>0.166455</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.120167</td>\n",
       "      <td>0.119960</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.261695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.224414</td>\n",
       "      <td>0.223954</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.206151</td>\n",
       "      <td>0.205611</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.149564</td>\n",
       "      <td>0.149202</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.109139</td>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.236985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.202398</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.181340</td>\n",
       "      <td>0.180819</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label    RMS_E1    MAV_E1  VARIANCE_E1  SAMPLE_VARIANCE_E1    RMS_E2  \\\n",
       "0  Base  0.200014  0.199970     0.000018            0.000018  0.202461   \n",
       "1  Base  0.193816  0.193720     0.000037            0.000038  0.154873   \n",
       "2  Base  0.181736  0.181561     0.000063            0.000064  0.134383   \n",
       "3  Base  0.166725  0.166455     0.000090            0.000092  0.120167   \n",
       "4  Base  0.149564  0.149202     0.000108            0.000110  0.109139   \n",
       "\n",
       "     MAV_E2  VARIANCE_E2  SAMPLE_VARIANCE_E2    RMS_E3  ...  VARIANCE_E6  \\\n",
       "0  0.197485     0.001990            0.002024  0.459723  ...     0.000026   \n",
       "1  0.154214     0.000204            0.000207  0.342896  ...     0.000007   \n",
       "2  0.134057     0.000087            0.000089  0.294403  ...     0.000004   \n",
       "3  0.119960     0.000050            0.000050  0.261695  ...     0.000003   \n",
       "4  0.108991     0.000032            0.000033  0.236985  ...     0.000002   \n",
       "\n",
       "   SAMPLE_VARIANCE_E6    RMS_E7    MAV_E7  VARIANCE_E7  SAMPLE_VARIANCE_E7  \\\n",
       "0            0.000027  0.373438  0.366913     0.004830            0.004912   \n",
       "1            0.000007  0.294526  0.293246     0.000753            0.000765   \n",
       "2            0.000004  0.253785  0.253042     0.000377            0.000383   \n",
       "3            0.000003  0.224414  0.223954     0.000206            0.000210   \n",
       "4            0.000002  0.202398  0.202100     0.000121            0.000123   \n",
       "\n",
       "     RMS_E8    MAV_E8  VARIANCE_E8  SAMPLE_VARIANCE_E8  \n",
       "0  0.322071  0.318258     0.002441            0.002483  \n",
       "1  0.266149  0.265341     0.000429            0.000437  \n",
       "2  0.233627  0.233021     0.000283            0.000287  \n",
       "3  0.206151  0.205611     0.000222            0.000226  \n",
       "4  0.181340  0.180819     0.000189            0.000192  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Replace 'your_file.parquet' with the actual file name\n",
    "filename = 'signals_50OL_03Windowing.pkl'\n",
    "file_path = 'preprocessed_data'\n",
    "\n",
    "file_path = os.path.join(file_path, filename)\n",
    "\n",
    "# Load the Pickle file into a pandas DataFrame\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
