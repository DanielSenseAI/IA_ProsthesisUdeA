{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libreries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "from scipy.io import loadmat, whosmat\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from src import config\n",
    "from src import *\n",
    "import re\n",
    "from src import loadmatNina \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.io import whosmat\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the database to analyze\n",
    "database = 'DB4'\n",
    "\n",
    "data_path = f'data/{database}'\n",
    "\n",
    "# Find the folder named with the convention s + \"number\"\n",
    "folder = None\n",
    "for item in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', item) or re.match(r'Subject\\d+', item):\n",
    "        folder = item\n",
    "        break\n",
    "\n",
    "if folder:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all .mat files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.mat'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            info = whosmat(file_path)\n",
    "            results.append((file_name, info))\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    data = {}\n",
    "    for file_name, info in results:\n",
    "        for item in info:\n",
    "            if item[0] not in data:\n",
    "                data[item[0]] = {}\n",
    "            data[item[0]][file_name] = item[1:]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    df.columns.name = 'File Name'\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No folder found with the convention s + 'number'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For complete signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics(signal):\n",
    "    \"\"\"\n",
    "    Calcula las métricas de una señal EMG.\n",
    "\n",
    "    Parámetros:\n",
    "    - signal: Array de NumPy que contiene la señal EMG.\n",
    "\n",
    "    Retorna:\n",
    "    - Un diccionario con las métricas calculadas.\n",
    "    \"\"\"\n",
    "    # 1. Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # 2. Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # 3. Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # 4. Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # 5. Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # 6. Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # 7. Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # 8. Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Retornar las métricas en un diccionario\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"TD\": td,\n",
    "        \"RMS\": rms,\n",
    "        \"MAVS\": mavs,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"WL\": wl\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For signal with means and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_std(signal):\n",
    "    \"\"\"\n",
    "    Calcula las métricas de una señal EMG.\n",
    "\n",
    "    Parámetros:\n",
    "    - signal: Array de NumPy que contiene la señal EMG.\n",
    "\n",
    "    Retorna:\n",
    "    - Un diccionario con las métricas calculadas.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:  # Si la señal tiene múltiples canales\n",
    "        metrics_per_channel = [calculate_emg_metrics_std(signal[:, ch]) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # 1. Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    mav_mean = np.mean(mav)\n",
    "    mav_std = np.std(mav)\n",
    "\n",
    "    # 2. Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    iav_mean = np.mean(iav)\n",
    "    iav_std = np.std(iav)\n",
    "    \n",
    "    # 3. Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    td_mean = np.mean(td)\n",
    "    td_std = np.std(td)\n",
    "    \n",
    "    # 4. Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    \n",
    "    # 5. Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    mavs_mean = np.mean(mavs)\n",
    "    mavs_std = np.std(mavs)\n",
    "    \n",
    "    # 6. Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    zc_mean = np.mean(zc)\n",
    "    zc_std = np.std(zc)\n",
    "    \n",
    "    # 7. Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    ssc_mean = np.mean(ssc)\n",
    "    ssc_std = np.std(ssc)\n",
    "    \n",
    "    # 8. Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    wl_mean = np.mean(wl)\n",
    "    wl_std = np.std(wl)\n",
    "    \n",
    "    # Retornar las métricas en un diccionario\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"MAV_STD\" : mav_std,\n",
    "        \"MAV_MEAN\" : mav_mean,\n",
    "        \"IAV\": iav,\n",
    "        \"IAV_STD\" : iav_std,\n",
    "        \"IAV_MEAN\" : iav_mean,\n",
    "        \"TD\" : td,\n",
    "        \"TD_STD\" : td_std,\n",
    "        \"TD_MEAN\" : td_mean,\n",
    "        \"RMS\": rms,\n",
    "        \"RMS_STD\" : rms_std,\n",
    "        \"RMS_MEAN\" : rms_mean,\n",
    "        \"MAVS\": mavs,\n",
    "        \"MAVS_STD\" : mavs_std,\n",
    "        \"MAVS_MEAN\" : mavs_mean,\n",
    "        \"ZC\": zc,\n",
    "        \"ZC_STD\" : zc_std,\n",
    "        \"ZC_MEAN\" : zc_mean,\n",
    "        \"SSC\": ssc,\n",
    "        \"SSC_STD\" : ssc_std,\n",
    "        \"SSC_MEAN\" : ssc_mean,\n",
    "        \"WL\": wl,\n",
    "        \"WL_STD\" : wl_std,\n",
    "        \"WL_MEAN\" : wl_mean\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This functions calculate the metrics for channel and average the values for a complete result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_means(signal):\n",
    "    \"\"\"\n",
    "    Calcula las métricas de una señal EMG. Si hay múltiples canales, calcula las métricas\n",
    "    por canal y luego promedia los resultados.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:  # Si la señal tiene múltiples canales\n",
    "        metrics_per_channel = [calculate_emg_metrics_means(signal[:, ch]) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # 1. Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # 2. Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # 3. Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # 4. Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # 5. Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # 6. Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # 7. Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # 8. Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Retornar las métricas en un diccionario\n",
    "    return {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"TD\": td,\n",
    "        \"RMS\": rms,\n",
    "        \"MAVS\": mavs,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"WL\": wl\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots and metrics for complete grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name \n",
    "database = 'DB4'\n",
    "\n",
    "# Full path to the database folder\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "\n",
    "# List of subjects, generating names from 's1' to 's10'\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Archivos E1, E2, E3 para el sujeto actual\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Archivo no encontrado: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcesando: {filename}\")\n",
    "        \n",
    "        # Cargar datos del archivo .mat\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Verificar la estructura de mat_data\n",
    "        print(f\"Claves en mat_data: {mat_data.keys()}\")\n",
    "        \n",
    "        # Obtener datos reetiquetados\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterar sobre grasps etiquetados\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Verificar si 'emg_data' está en mat_data\n",
    "                if 'emg' not in mat_data:\n",
    "                    raise KeyError(f\"La clave 'emg_data' no está en mat_data. Claves disponibles: {mat_data.keys()}\")\n",
    "                \n",
    "                # Obtener la señal EMG para el grasp reetiquetado\n",
    "                emg_signal = mat_data['emg'][grasp]  # Ajusta esto según la estructura real\n",
    "                \n",
    "                # Calcular métricas de la señal EMG\n",
    "                metrics = calculate_emg_metrics(emg_signal)\n",
    "                \n",
    "                # Imprimir las métricas\n",
    "                print(f\"\\nMétricas para Grasp {grasp}:\")\n",
    "                for key, value in metrics.items():\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "                \n",
    "                # Graficar datos EMG para el grasp reetiquetado\n",
    "                src.plot_emg_data(\n",
    "                    database=database,\n",
    "                    mat_file=mat_data,\n",
    "                    grasp_number=grasp,\n",
    "                    interactive=False,\n",
    "                    include_rest=True,\n",
    "                    use_stimulus=False,\n",
    "                    addFourier=False,\n",
    "                    padding=100,\n",
    "                    title=f\"{filename} - Grasp {grasp}\"  # Usar el número del grasp actual\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                print(f\"    Error: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error procesando grasp {grasp}: {str(e)}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with metrics for a complete signal without discriminating by channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar todas las métricas\n",
    "metrics_data = []\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Saltar archivos no encontrados\n",
    "        \n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                metrics = calculate_emg_metrics(emg_signal)\n",
    "                \n",
    "                # Agregar metadatos + métricas a la lista\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Desempaqueta todas las métricas\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Crear DataFrame organizado\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reordenar columnas (opcional)\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "print(\"\\nDataFrame de Métricas:\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with average of metrics for channels in each grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lista para almacenar todas las métricas\n",
    "metrics_data = []\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Saltar archivos no encontrados\n",
    "        \n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                metrics = calculate_emg_metrics_means(emg_signal)\n",
    "                \n",
    "                # Agregar metadatos + métricas a la lista\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Desempaqueta todas las métricas\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Crear DataFrame organizado\n",
    "metrics_df_2 = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reordenar columnas (opcional)\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics_df_2.columns[4:])\n",
    "metrics_df_2 = metrics_df_2[column_order]\n",
    "\n",
    "print(\"\\nDataFrame de Métricas:\")\n",
    "print(metrics_df_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar todas las métricas\n",
    "metrics_data = []\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Saltar archivos no encontrados\n",
    "        \n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                metrics = calculate_emg_metrics_std(emg_signal)\n",
    "                \n",
    "                # Agregar metadatos + métricas a la lista\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Desempaqueta todas las métricas\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Crear DataFrame organizado\n",
    "metrics_df_std = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reordenar columnas (opcional)\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics.keys())\n",
    "metrics_df_std = metrics_df_std[column_order]\n",
    "\n",
    "print(\"\\nDataFrame de Métricas:\")\n",
    "metrics_df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe for every channels of data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar todos los DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Encontrar todas las carpetas que coincidan con el patrón s + \"number\" o Subject + \"number\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al cargar {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                try:\n",
    "                    test_df, grasps = src.build_dataframe(\n",
    "                        mat_file=mat_data,\n",
    "                        database=database,\n",
    "                        filename=file_name,\n",
    "                        rectify=False,\n",
    "                        normalize=True\n",
    "                    )\n",
    "                    \n",
    "                    # Agregar una columna con el sujeto (folder) al DataFrame\n",
    "                    test_df['subject'] = file_name\n",
    "                    \n",
    "                    # Agregar el DataFrame a la lista\n",
    "                    all_dataframes.append(test_df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al procesar {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "if all_dataframes:  \n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Mostrar el DataFrame combinado\n",
    "    print(\"\\nDataFrame Combinado:\")\n",
    "    display(combined_df)  \n",
    "\n",
    "else:\n",
    "    print(\"Advertencia: No se generaron DataFrames. Verifica los datos de entrada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with metrics for channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar las métricas por canal\n",
    "metrics_data = []\n",
    "\n",
    "# Iterar sobre cada sujeto y relabeled (o stimulus)\n",
    "for (subject, relabeled), group in combined_df.groupby(['subject', 'relabeled']):  # Cambia 'relabeled' por 'stimulus' si es necesario\n",
    "    # Iterar sobre cada canal\n",
    "    for channel in group.columns:  # Itera sobre todas las columnas\n",
    "        if channel.startswith('Channel'):  # Filtra solo las columnas de canales\n",
    "            # Obtener la señal del canal\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Calcular métricas para el canal actual\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Agregar metadatos + métricas a la lista\n",
    "            metrics_data.append({\n",
    "                \"subject\": subject,\n",
    "                \"relabeled\": relabeled,  # Cambia 'relabeled' por 'stimulus' si es necesario\n",
    "                \"channel\": channel,\n",
    "                **metrics  # Desempaqueta todas las métricas\n",
    "            })\n",
    "\n",
    "# Crear DataFrame con las métricas\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reordenar columnas (opcional)\n",
    "column_order = [\"subject\", \"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Mostrar el DataFrame de métricas\n",
    "print(\"\\nDataFrame de Métricas por Canal, Sujeto y Relabeled:\")\n",
    "display(metrics_df)  # Usar display en Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = metrics_df.drop(columns = ['channel'])\n",
    "\n",
    "df_mean = grouped_df.groupby(['subject','relabeled']).mean()\n",
    "df_std = grouped_df.groupby(['subject','relabeled']).std()\n",
    "\n",
    "df_mean.columns = [f\"{col} mean\" for col in df_mean.columns]\n",
    "df_std.columns = [f\"{col} std\" for col in df_std.columns]\n",
    "\n",
    "df_result = df_mean.merge(df_std, on=['subject', 'relabeled']).reset_index()\n",
    "\n",
    "# Agrupar por 'subject' y 'relabeled' y calcular el promedio y la desviación estándar de las métricas\n",
    "# grouped_df.columns = ['subject', 'relabeled'] + [f\"{col} mean\" for col in grouped_df.columns[2:]]\n",
    "\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendogram for grasp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Seleccionar solo las columnas de características\n",
    "features = df_result.iloc[:, 2:]  # Omitimos 'subject' y 'relabeled'\n",
    "\n",
    "# Normalizar los datos para mejor visualización\n",
    "df_scaled = StandardScaler().fit_transform(features)\n",
    "\n",
    "# Aplicar clustering jerárquico\n",
    "linked = sch.linkage(df_scaled, method='ward')\n",
    "\n",
    "# Crear el dendrograma\n",
    "plt.figure(figsize=(20, 10))\n",
    "sch.dendrogram(linked, labels=df_result['relabeled'].values, leaf_rotation=90, leaf_font_size=8)\n",
    "plt.title(\"Dendrograma basado en la relabeled\")\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Distancia Euclidiana\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar los datos (asumiendo que ya tienes el DataFrame 'data')\n",
    "# Agrupar por 'relabeled' y calcular media y desviación estándar\n",
    "grouped = df_result.select_dtypes(include=['number']).groupby(df_result['relabeled']).agg(['mean', 'std'])\n",
    "display(grouped)\n",
    "# Aplanar los nombres de columnas (opcional)\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "# Normalizar los datos para evitar que las magnitudes afecten la distancia\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# Aplicar clustering jerárquico\n",
    "linked = sch.linkage(scaled_features, method='ward')\n",
    "\n",
    "# Crear el dendrograma\n",
    "plt.figure(figsize=(12, 6))\n",
    "sch.dendrogram(linked, labels=grouped.index.tolist(), leaf_rotation=90, leaf_font_size=8)\n",
    "plt.title(\"Dendrograma basado en media y desviación estándar por tipo de grasp\")\n",
    "plt.xlabel(\"Grasps\")\n",
    "plt.ylabel(\"Distancia Euclidiana\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcular el promedio de las métricas por canal\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "average_metrics_df = metrics_df.groupby('channel')[metrics_columns].mean().reset_index()\n",
    "display(average_metrics_df)\n",
    "\n",
    "# 2. Preparar los datos para el clustering\n",
    "X = average_metrics_df[metrics_columns].values\n",
    "\n",
    "# 3. Calcular la matriz de distancias y realizar el clustering jerárquico\n",
    "Z = linkage(X, method='ward')  # 'ward' es un método común para clustering jerárquico\n",
    "\n",
    "# 4. Graficar el dendrograma con ajustes para mejor visualización\n",
    "plt.figure(figsize=(15, 8)) \n",
    "plt.title('Dendrograma de Canales EMG (Promedio de Métricas)', fontsize=16, pad=20)\n",
    "plt.xlabel('Canales', fontsize=14)\n",
    "plt.ylabel('Distancia', fontsize=14)\n",
    "\n",
    "# Ajustar el dendrograma para que las etiquetas no se superpongan\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=average_metrics_df['channel'].values,  # Etiquetas de los canales\n",
    "    leaf_rotation=90, \n",
    "    leaf_font_size=12,  \n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Umbral para colorear los clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calcular el promedio y la desviación estándar de las métricas por canal\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "\n",
    "agg_metrics_df = metrics_df.groupby('channel')[metrics_columns].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Aplanar las columnas para facilitar el acceso\n",
    "agg_metrics_df.columns = ['_'.join(col).strip('_') for col in agg_metrics_df.columns]\n",
    "\n",
    "display(agg_metrics_df)\n",
    "\n",
    "# 2. Preparar los datos para el clustering usando solo los promedios\n",
    "X = agg_metrics_df[[col for col in agg_metrics_df.columns if col.endswith('_mean')]].values\n",
    "\n",
    "# 3. Calcular la matriz de distancias y realizar el clustering jerárquico\n",
    "Z = linkage(X, method='ward')  # 'ward' es un método común para clustering jerárquico\n",
    "\n",
    "# 4. Graficar el dendrograma con ajustes para mejor visualización\n",
    "plt.figure(figsize=(15, 8)) \n",
    "plt.title('Dendrograma de Canales EMG (Promedio de Métricas)', fontsize=16, pad=20)\n",
    "plt.xlabel('Canales', fontsize=14)\n",
    "plt.ylabel('Distancia', fontsize=14)\n",
    "\n",
    "# Ajustar el dendrograma para que las etiquetas no se superpongan\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=agg_metrics_df['channel'].values,  # Etiquetas de los canales\n",
    "    leaf_rotation=90, \n",
    "    leaf_font_size=12,  \n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Umbral para colorear los clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos solo las métricas para el clustering, excluyendo las columnas de metadatos\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "X = metrics_df[metrics_columns].values\n",
    "\n",
    "# Calcular la matriz de distancias y realizar el clustering jerárquico\n",
    "Z = linkage(X, method='ward')  # 'ward' es un método común para clustering jerárquico\n",
    "\n",
    "# Graficar el dendrograma con ajustes para mejor visualización\n",
    "plt.figure(figsize=(20, 10))  # Aumentar el tamaño de la figura\n",
    "plt.title('Dendrograma de Canales EMG', fontsize=16, pad=20)\n",
    "plt.xlabel('Canales', fontsize=14)\n",
    "plt.ylabel('Distancia', fontsize=14)\n",
    "\n",
    "# Ajustar el dendrograma para que las etiquetas no se superpongan\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=metrics_df['channel'].values,  # Etiquetas de los canales\n",
    "    leaf_rotation=90,  # Rotar las etiquetas 90 grados\n",
    "    leaf_font_size=10,  # Tamaño de la fuente de las etiquetas\n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Umbral para colorear los clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  # Ajustar el layout para evitar cortes en las etiquetas\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
