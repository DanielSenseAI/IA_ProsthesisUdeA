{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libreries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat, whosmat\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import src\n",
    "from src import config, loadmatNina\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import src\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.multivariate.manova import MANOVA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the database to analyze\n",
    "database = 'DB4'\n",
    "\n",
    "data_path = f'data/{database}'\n",
    "\n",
    "# Find the folder named with the convention s + \"number\"\n",
    "folder = None\n",
    "for item in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', item) or re.match(r'Subject\\d+', item):\n",
    "        folder = item\n",
    "        break\n",
    "\n",
    "if folder:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all .mat files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.mat'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            info = whosmat(file_path)\n",
    "            results.append((file_name, info))\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    data = {}\n",
    "    for file_name, info in results:\n",
    "        for item in info:\n",
    "            if item[0] not in data:\n",
    "                data[item[0]] = {}\n",
    "            data[item[0]][file_name] = item[1:]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    df.columns.name = 'File Name'\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No folder found with the convention s + 'number'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For complete signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics(signal):\n",
    "    \"\"\"\n",
    "    Calculates metrics for an EMG signal.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    # 1. Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # 2. Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # 3. Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # 4. Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # 5. Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # 6. Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # 7. Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # 8. Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"TD\": td,\n",
    "        \"RMS\": rms,\n",
    "        \"MAVS\": mavs,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"WL\": wl\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For signal with means and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_std(signal):\n",
    "    \"\"\"\n",
    "    Calcula las métricas de una señal EMG.\n",
    "\n",
    "    Parámetros:\n",
    "    - signal: Array de NumPy que contiene la señal EMG.\n",
    "\n",
    "    Retorna:\n",
    "    - Un diccionario con las métricas calculadas.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:  # Si la señal tiene múltiples canales\n",
    "        metrics_per_channel = [calculate_emg_metrics_std(signal[:, ch]) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # 1. Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    mav_mean = np.mean(mav)\n",
    "    mav_std = np.std(mav)\n",
    "\n",
    "    # 2. Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    iav_mean = np.mean(iav)\n",
    "    iav_std = np.std(iav)\n",
    "    \n",
    "    # 3. Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    td_mean = np.mean(td)\n",
    "    td_std = np.std(td)\n",
    "    \n",
    "    # 4. Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    \n",
    "    # 5. Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    mavs_mean = np.mean(mavs)\n",
    "    mavs_std = np.std(mavs)\n",
    "    \n",
    "    # 6. Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    zc_mean = np.mean(zc)\n",
    "    zc_std = np.std(zc)\n",
    "    \n",
    "    # 7. Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    ssc_mean = np.mean(ssc)\n",
    "    ssc_std = np.std(ssc)\n",
    "    \n",
    "    # 8. Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    wl_mean = np.mean(wl)\n",
    "    wl_std = np.std(wl)\n",
    "    \n",
    "    # Retornar las métricas en un diccionario\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"MAV_STD\" : mav_std,\n",
    "        \"MAV_MEAN\" : mav_mean,\n",
    "        \"IAV\": iav,\n",
    "        \"IAV_STD\" : iav_std,\n",
    "        \"IAV_MEAN\" : iav_mean,\n",
    "        \"TD\" : td,\n",
    "        \"TD_STD\" : td_std,\n",
    "        \"TD_MEAN\" : td_mean,\n",
    "        \"RMS\": rms,\n",
    "        \"RMS_STD\" : rms_std,\n",
    "        \"RMS_MEAN\" : rms_mean,\n",
    "        \"MAVS\": mavs,\n",
    "        \"MAVS_STD\" : mavs_std,\n",
    "        \"MAVS_MEAN\" : mavs_mean,\n",
    "        \"ZC\": zc,\n",
    "        \"ZC_STD\" : zc_std,\n",
    "        \"ZC_MEAN\" : zc_mean,\n",
    "        \"SSC\": ssc,\n",
    "        \"SSC_STD\" : ssc_std,\n",
    "        \"SSC_MEAN\" : ssc_mean,\n",
    "        \"WL\": wl,\n",
    "        \"WL_STD\" : wl_std,\n",
    "        \"WL_MEAN\" : wl_mean\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This functions calculate the metrics for channel and average the values for a complete result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_means(signal):\n",
    "    \"\"\"\n",
    "    Calculates the metrics of an EMG signal. If there are multiple channels, it computes \n",
    "    the metrics for each channel and then averages the results.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:  # If the signal has multiple channels\n",
    "        metrics_per_channel = [calculate_emg_metrics_means(signal[:, ch]) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # 1. Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # 2. Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # 3. Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # 4. Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # 5. Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # 6. Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # 7. Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # 8. Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics in a dictionary\n",
    "    return {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"TD\": td,\n",
    "        \"RMS\": rms,\n",
    "        \"MAVS\": mavs,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"WL\": wl\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots and metrics for complete grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name\n",
    "database = 'DB4'\n",
    "\n",
    "# Full path to the database folder\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "\n",
    "# List of subjects, generating names from 's1' to 's10'\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# Iterate over each subject\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3 for the current subject\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Verify the structure of the loaded dictionary\n",
    "        print(f\"Keys in mat_data: {mat_data.keys()}\")\n",
    "        \n",
    "        # Retrieve re-labeled data and the list of labeled grasps\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over each labeled grasp\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Check if 'emg' key exists in mat_data\n",
    "                if 'emg' not in mat_data:\n",
    "                    raise KeyError(f\"The key 'emg' is not in mat_data. Available keys: {mat_data.keys()}\")\n",
    "                \n",
    "                # Get the EMG signal for the labeled grasp\n",
    "                emg_signal = mat_data['emg'][grasp]  # Adjust based on the actual structure\n",
    "                \n",
    "                # Compute EMG signal metrics\n",
    "                metrics = calculate_emg_metrics(emg_signal)\n",
    "                \n",
    "                # Print computed metrics\n",
    "                print(f\"\\nMetrics for Grasp {grasp}:\")\n",
    "                for key, value in metrics.items():\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "                \n",
    "                # Plot the EMG signal for the grasp\n",
    "                src.plot_emg_data(\n",
    "                    database=database,\n",
    "                    mat_file=mat_data,\n",
    "                    grasp_number=grasp,\n",
    "                    interactive=False,\n",
    "                    include_rest=True,\n",
    "                    use_stimulus=False,\n",
    "                    addFourier=False,\n",
    "                    padding=100,\n",
    "                    title=f\"{filename} - Grasp {grasp}\"\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                print(f\"    Error: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing grasp {grasp}: {str(e)}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with metrics for a complete signal without discriminating by channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all extracted metrics\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject in the database\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists before processing\n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Skip if file is not available\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Build DataFrame with re-labeled data\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over labeled grasps\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Retrieve the corresponding EMG signal\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                \n",
    "                # Compute EMG signal metrics\n",
    "                metrics = calculate_emg_metrics(emg_signal)\n",
    "                \n",
    "                # Append metrics with metadata to the list\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Unpack metrics into the dictionary\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Create a DataFrame with organized metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns (optional) for better visualization\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Print the final DataFrame with extracted metrics\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with average of metrics for channels in each grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all computed metrics\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject in the database\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists before processing\n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Skip if file is not available\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Build DataFrame with re-labeled data\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over labeled grasps\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Retrieve the corresponding EMG signal\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                \n",
    "                # Compute EMG signal metrics using averages\n",
    "                metrics = calculate_emg_metrics_means(emg_signal)\n",
    "                \n",
    "                # Append metrics with metadata to the list\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Unpack metrics into the dictionary\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Create a DataFrame with organized metrics\n",
    "metrics_df_2 = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns (optional) for better visualization\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics_df_2.columns[4:])\n",
    "metrics_df_2 = metrics_df_2[column_order]\n",
    "\n",
    "# Print the final DataFrame with extracted metrics\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "display(metrics_df_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import src\n",
    "\n",
    "# List to store all computed metrics\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject in the database\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists before processing\n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Skip if file is not available\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Build DataFrame with re-labeled data\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over labeled grasps\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Retrieve the corresponding EMG signal\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                \n",
    "                # Compute EMG signal metrics using standard deviation\n",
    "                metrics = calculate_emg_metrics_std(emg_signal)\n",
    "                \n",
    "                # Append metrics with metadata to the list\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Unpack metrics into the dictionary\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Create a DataFrame with organized metrics\n",
    "metrics_df_std = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns (optional) for better visualization\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics.keys())\n",
    "metrics_df_std = metrics_df_std[column_order]\n",
    "\n",
    "# Print the final DataFrame with extracted metrics\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "display(metrics_df_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe for every channels of data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all generated DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Look for folders matching the pattern \"s + number\" or \"Subject + number\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterate over all .mat files in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Attempt to load the .mat file\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\" Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Attempt to process the file with src.build_dataframe\n",
    "                try:\n",
    "                    test_df, grasps = src.build_dataframe(\n",
    "                        mat_file=mat_data,\n",
    "                        database=database,\n",
    "                        filename=file_name,\n",
    "                        rectify=False,\n",
    "                        normalize=True\n",
    "                    )\n",
    "                    \n",
    "                    # Add a column with the subject name (folder) to the DataFrame\n",
    "                    test_df['subject'] = folder  \n",
    "                    \n",
    "                    # Append the processed DataFrame to the list\n",
    "                    all_dataframes.append(test_df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\" Error processing {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Concatenate all DataFrames into a single one if data is available\n",
    "if all_dataframes:  \n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Display the combined DataFrame\n",
    "    print(\"\\n Combined DataFrame:\")\n",
    "    display(combined_df)  \n",
    "\n",
    "else:\n",
    "    print(\"Warning: No DataFrames were generated. Check the input data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with metrics for channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (subject, relabeled), group in combined_df.groupby(['subject', 'relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                \"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"subject\", \"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'channel' column to group data by subject and movement type\n",
    "grouped_df = metrics_df.drop(columns=['channel'])\n",
    "\n",
    "# Compute the mean value of each metric grouped by subject and movement\n",
    "df_mean = grouped_df.groupby(['subject', 'relabeled']).mean()\n",
    "\n",
    "# Compute the standard deviation of each metric grouped by subject and movement\n",
    "df_std = grouped_df.groupby(['subject', 'relabeled']).std()\n",
    "\n",
    "# Rename columns to indicate they contain mean values\n",
    "df_mean.columns = [f\"{col} mean\" for col in df_mean.columns]\n",
    "\n",
    "# Rename columns to indicate they contain standard deviation values\n",
    "df_std.columns = [f\"{col} std\" for col in df_std.columns]\n",
    "\n",
    "# Merge the mean and standard deviation DataFrames into a single DataFrame\n",
    "df_result = df_mean.merge(df_std, on=['subject', 'relabeled']).reset_index()\n",
    "\n",
    "# Display the final DataFrame with aggregated metrics\n",
    "display(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendogram for grasp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns containing numerical features\n",
    "features = df_result.iloc[:, 2:]  # Exclude 'subject' and 'relabeled'\n",
    "\n",
    "# Normalize the data to improve comparability and avoid bias due to different scales\n",
    "df_scaled = StandardScaler().fit_transform(features)\n",
    "\n",
    "# Apply hierarchical clustering using the Ward method (minimizes variance within clusters)\n",
    "linked = sch.linkage(df_scaled, method='ward')\n",
    "\n",
    "# Create and visualize the dendrogram\n",
    "plt.figure(figsize=(20, 10))\n",
    "sch.dendrogram(\n",
    "    linked, \n",
    "    labels=df_result['relabeled'].values,  # Labels on the x-axis based on the 'relabeled' variable\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=8  # Adjust font size\n",
    ")\n",
    "plt.title(\"Dendrogram based on the 'relabeled' variable\")\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Euclidean Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'relabeled' and calculate the mean and standard deviation of each numerical feature\n",
    "grouped = df_result.select_dtypes(include=['number']).groupby(df_result['relabeled']).agg(['mean', 'std'])\n",
    "display(grouped)\n",
    "\n",
    "# Flatten column names to make them easier to work with\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "# Normalize the data to prevent magnitude differences from affecting the clustering distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# Apply hierarchical clustering using the Ward method (minimizes variance within clusters)\n",
    "linked = sch.linkage(scaled_features, method='ward')\n",
    "\n",
    "# Create and visualize the dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sch.dendrogram(\n",
    "    linked, \n",
    "    labels=grouped.index.tolist(),  # Labels on the x-axis based on the 'relabeled' variable\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=8  # Adjust font size\n",
    ")\n",
    "plt.title(\"Dendrogram based on mean and standard deviation per grasp type\")\n",
    "plt.xlabel(\"Grasps\")\n",
    "plt.ylabel(\"Euclidean Distance\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the average of metrics per channel\n",
    "# Exclude 'subject', 'relabeled', and 'channel' to keep only the metric columns\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "\n",
    "# Group by 'channel' and compute the mean of each metric\n",
    "average_metrics_df = metrics_df.groupby('channel')[metrics_columns].mean().reset_index()\n",
    "display(average_metrics_df)\n",
    "\n",
    "# 2. Prepare data for clustering\n",
    "X = average_metrics_df[metrics_columns].values  # Extract metric values as an array for clustering\n",
    "\n",
    "# 3. Compute the distance matrix and perform hierarchical clustering\n",
    "Z = linkage(X, method='ward')  # 'ward' minimizes variance within clusters\n",
    "\n",
    "# 4. Plot the dendrogram with adjustments for better visualization\n",
    "plt.figure(figsize=(15, 8)) \n",
    "plt.title('Dendrogram of EMG Channels (Average Metrics)', fontsize=16, pad=20)\n",
    "plt.xlabel('Channels', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "\n",
    "# Adjust the dendrogram to prevent overlapping labels\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=average_metrics_df['channel'].values,  # Labels for each channel\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=12,  # Adjust font size\n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Threshold to color clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust layout for better fit\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the mean and standard deviation of metrics per channel\n",
    "# Exclude 'subject', 'relabeled', and 'channel' to keep only numerical metric columns\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "\n",
    "# Group by 'channel' and compute the mean and standard deviation for each metric\n",
    "agg_metrics_df = metrics_df.groupby('channel')[metrics_columns].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Flatten column names for easier access (concatenating \"metric_type\")\n",
    "agg_metrics_df.columns = ['_'.join(col).strip('_') for col in agg_metrics_df.columns]\n",
    "# columns_names = agg_metrics_df.columns.values\n",
    "# print(columns_names)\n",
    "agg_metrics_df= agg_metrics_df.sort_values(by='RMS_mean', ascending=False)\n",
    "\n",
    "display(agg_metrics_df)  # Display the aggregated metrics table\n",
    "\n",
    "# 2. Prepare data for clustering using only the metric averages\n",
    "X = agg_metrics_df[[col for col in agg_metrics_df.columns if col.endswith('_mean')]].values  # Extract only \"_mean\" columns\n",
    "\n",
    "# 3. Compute the distance matrix and perform hierarchical clustering\n",
    "Z = linkage(X, method='ward')  # 'ward' minimizes variance within clusters\n",
    "\n",
    "# 4. Plot the dendrogram with adjustments for better visualization\n",
    "plt.figure(figsize=(15, 8)) \n",
    "plt.title('Dendrogram of EMG Channels (Average Metrics)', fontsize=16, pad=20)\n",
    "plt.xlabel('Channels', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "\n",
    "# Adjust the dendrogram to prevent overlapping labels\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=agg_metrics_df['channel'].values,  # Labels for EMG channels\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=12,  # Adjust font size\n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Threshold to color clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust layout for better fit\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select only metric columns for clustering, excluding metadata\n",
    "# Exclude non-metric columns ('subject', 'relabeled', and 'channel')\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "X = metrics_df[metrics_columns].values  # Convert to a NumPy array for clustering\n",
    "\n",
    "# 2. Compute the distance matrix and perform hierarchical clustering\n",
    "Z = linkage(X, method='ward')  # 'ward' method minimizes variance within clusters\n",
    "\n",
    "# 3. Plot the dendrogram with adjustments for better visualization\n",
    "plt.figure(figsize=(20, 10))  # Increase figure size\n",
    "plt.title('Dendrogram of EMG Channels', fontsize=16, pad=20)\n",
    "plt.xlabel('Channels', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "\n",
    "# Configure the dendrogram\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=metrics_df['channel'].values,  # Labels for EMG channels\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=10,  # Adjust font size\n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Threshold to color clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust layout for better fit\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Group by 'relabeled' and compute the mean and standard deviation of each metric\n",
    "grouped = df_result.select_dtypes(include=['number']).groupby(df_result['relabeled']).agg(['mean', 'std'])\n",
    "\n",
    "# 2. Flatten column names for easier access\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "# 3. Normalize the data to avoid magnitude differences affecting clustering\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 4. Compute the covariance matrix and its pseudoinverse (for Mahalanobis distance)\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)  # Covariance matrix\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Pseudoinverse instead of inverse\n",
    "\n",
    "# 5. Compute Mahalanobis distances between each pair of groups\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 6. Convert to a square distance matrix\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 7. Apply hierarchical clustering using Mahalanobis distance\n",
    "linked = sch.linkage(distance_matrix, method='average')  # 'average' method for more stability\n",
    "\n",
    "# 8. Generate the dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sch.dendrogram(linked, labels=grouped.index.tolist(), leaf_rotation=90, leaf_font_size=8)\n",
    "plt.title(\"Dendrogram based on Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Mahalanobis Distance\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_result.head())  # Muestra las primeras filas\n",
    "print(df_result.info())  # Muestra el tipo de datos de cada columna\n",
    "print(df_result.columns) # Lista las columnas del DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "# 1. Verificar y eliminar columnas redundantes (correlación alta)\n",
    "dependent_vars = df_result.filter(like='mean').columns.tolist() + df_result.filter(like='std').columns.tolist()\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]  # Umbral de 0.95\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 2. Análisis MANOVA para evaluar la similitud entre los movimientos\n",
    "manova = MANOVA.from_formula(f\"{' + '.join(dependent_vars)} ~ relabeled\", data=df_result)\n",
    "print(manova.mv_test())\n",
    "\n",
    "# 3. Agrupar por 'relabeled' y calcular la media de cada métrica\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# 4. Normalizar las características (evitar efectos de magnitud)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 5. Calcular la distancia de Mahalanobis entre grupos\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)  # Matriz de covarianza\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Pseudoinversa para estabilidad numérica\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 6. Convertir a una matriz cuadrada de distancias\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 7. Clustering jerárquico aglomerativo (bottom-up)\n",
    "linked = sch.linkage(distance_matrix, method='average')  # 'average' reduce sesgo\n",
    "\n",
    "# 8. Normalizar las distancias del dendrograma dividiendo por el máximo valor\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 9. Graficar el dendrograma con colores para distinguir grupos\n",
    "plt.figure(figsize=(14, 6))\n",
    "dendrogram = sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.1 * max(linked[:, 2])  # Normalizado (ajustar para más/menos grupos)\n",
    ")\n",
    "plt.title(\"Dendrogram based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "# 🔹 1. Verificar columnas redundantes\n",
    "dependent_vars = df_result.filter(like='mean').columns.tolist() + df_result.filter(like='std').columns.tolist()\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]  # Umbral de correlación\n",
    "print(f\"Columnas a eliminar debido a alta correlación: {to_drop}\")\n",
    "\n",
    "# Eliminar columnas redundantes\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 🔹 2. Realizar el análisis MANOVA\n",
    "manova = MANOVA.from_formula(f\"{' + '.join(dependent_vars)} ~ relabeled\", data=df_result)\n",
    "print(manova.mv_test())\n",
    "\n",
    "# 🔹 3. Agrupar por 'relabeled' y calcular la media de cada métrica\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# 🔹 4. Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 🔹 5. Calcular la matriz de covarianza y su pseudoinversa\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "\n",
    "# 🔹 6. Calcular distancias de Mahalanobis\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 🔹 7. Normalizar las distancias para mantenerlas en [0,1]\n",
    "mahalanobis_distances = MinMaxScaler().fit_transform(mahalanobis_distances.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 🔹 8. Aplicar clustering jerárquico\n",
    "linked = sch.linkage(mahalanobis_distances, method='average')\n",
    "\n",
    "# 🔹 9. Generar el dendrograma con colores\n",
    "plt.figure(figsize=(14, 6))\n",
    "dendrogram = sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.3 * max(linked[:, 2]),  # Umbral automático para colorear ramas\n",
    ")\n",
    "\n",
    "plt.title(\"Dendrogram based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)  # Asegurar que todas las etiquetas sean visibles\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Verificar y eliminar columnas redundantes (correlación alta)\n",
    "dependent_vars = df_result.filter(like='mean').columns.tolist() + df_result.filter(like='std').columns.tolist()\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]  # Umbral de 0.95\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 2. Análisis MANOVA para evaluar la similitud entre los movimientos\n",
    "manova = MANOVA.from_formula(f\"{' + '.join(dependent_vars)} ~ relabeled\", data=df_result)\n",
    "print(manova.mv_test())\n",
    "\n",
    "# 3. Agrupar por 'relabeled' y calcular la media de cada métrica\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# 4. Normalizar las características (evitar efectos de magnitud)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 5. Calcular la distancia de Mahalanobis entre grupos\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)  # Matriz de covarianza\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Pseudoinversa para estabilidad numérica\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 6. Convertir a una matriz cuadrada de distancias\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 7. Clustering jerárquico aglomerativo (bottom-up)\n",
    "linked = sch.linkage(distance_matrix, method='average')  # 'average' reduce sesgo\n",
    "\n",
    "# 8. Normalizar las distancias del dendrograma dividiendo por el máximo valor\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 9. Graficar el dendrograma con colores para distinguir grupos\n",
    "plt.figure(figsize=(14, 6))\n",
    "dendrogram = sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.4  # Normalizado (ajustar para más/menos grupos)\n",
    ")\n",
    "plt.title(\"Dendrogram based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Análisis MANOVA para evaluar la similitud entre los movimientos\n",
    "manova = MANOVA.from_formula(f\"{' + '.join(dependent_vars)} ~ relabeled\", data=df_result)\n",
    "print(manova.mv_test())\n",
    "\n",
    "# 3. Agrupar por 'relabeled' y calcular la media de cada métrica\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# 4. Normalizar las características (evitar efectos de magnitud)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 5. Calcular la distancia de Mahalanobis entre grupos\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)  # Matriz de covarianza\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Pseudoinversa para estabilidad numérica\n",
    "\n",
    "# 🔹 **Plot de la Matriz de Covarianza**\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cov_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", xticklabels=dependent_vars, yticklabels=dependent_vars)\n",
    "plt.title(\"Covariance Matrix Heatmap\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Features\", fontsize=12)\n",
    "plt.ylabel(\"Features\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# 6. Calcular la distancia de Mahalanobis\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 7. Convertir a una matriz cuadrada de distancias\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 8. Clustering jerárquico aglomerativo (bottom-up)\n",
    "linked = sch.linkage(distance_matrix, method='average')  # 'average' reduce sesgo\n",
    "\n",
    "# 9. Normalizar las distancias del dendrograma dividiendo por el máximo valor\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 10. Graficar el dendrograma con colores para distinguir grupos\n",
    "plt.figure(figsize=(14, 6))\n",
    "dendrogram = sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.4  # Normalizado (ajustar para más/menos grupos)\n",
    ")\n",
    "plt.title(\"Dendrogram based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the relevant columns for clustering analysis\n",
    "X = df_result.iloc[:, 3:35].values  # Assuming df_result is equivalent to ypolfqrt in R\n",
    "\n",
    "# Normalize the data to prevent bias due to differences in variable scales\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Compute the covariance matrix and its inverse\n",
    "cov_matrix = np.cov(X_scaled, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Use pseudoinverse to avoid singularity issues\n",
    "\n",
    "# Compute the Mahalanobis distance between samples\n",
    "mahalanobis_distances = pdist(X_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# Convert the distance vector into a square distance matrix for clustering\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# Apply hierarchical clustering using the Ward method\n",
    "# This method minimizes the variance within the formed clusters.\n",
    "linked = sch.linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Create and visualize the dendrogram with label and font size adjustments\n",
    "plt.figure(figsize=(12, 6))  # Adjust figure size\n",
    "sch.dendrogram(\n",
    "    linked, \n",
    "    labels=df_result['relabeled'].values,  # Use grasp movement labels\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=8  # Adjust font size for labels\n",
    ")\n",
    "plt.title(\"Dendrogram based on Mahalanobis Distance\")  # Set plot title\n",
    "plt.xlabel(\"Grasps\")  # X-axis label\n",
    "plt.ylabel(\"Mahalanobis Distance\")  # Y-axis label\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by the 'relabeled' column and compute the mean and standard deviation\n",
    "grouped = df_result.select_dtypes(include=['number']).groupby(df_result['relabeled']).agg(['mean', 'std'])\n",
    "\n",
    "# Flatten column names for easier data access\n",
    "# Add '_mean' and '_std' suffixes to identify each statistic\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "# Normalize the data so that all features are on the same scale\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# Compute the covariance matrix and invert it\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Use the pseudo-inverse to avoid issues with singular matrices\n",
    "\n",
    "# Compute the Mahalanobis distance between samples\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# Convert the distance vector into a square matrix\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# Apply hierarchical clustering using the Ward method\n",
    "linked = sch.linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Generate the dendrogram to visualize the clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "sch.dendrogram(linked, labels=grouped.index.tolist(), leaf_rotation=90, leaf_font_size=8)\n",
    "plt.title(\"Dendrogram based on mean and standard deviation with Mahalanobis distance\")\n",
    "plt.xlabel(\"Grasps\")\n",
    "plt.ylabel(\"Mahalanobis Distance\") \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
