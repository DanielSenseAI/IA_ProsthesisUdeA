{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat, whosmat\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.linalg import inv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "import pywt\n",
    "\n",
    "import src\n",
    "from src import config, loadmatNina\n",
    "from src.preprocessing_utils import get_envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the database to analyze\n",
    "database = 'DB4'\n",
    "\n",
    "data_path = f'data/{database}'\n",
    "\n",
    "# Find the folder named with the convention s + \"number\"\n",
    "folder = None\n",
    "for item in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', item) or re.match(r'Subject\\d+', item):\n",
    "        folder = item\n",
    "        break\n",
    "\n",
    "if folder:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all .mat files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.mat'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            info = whosmat(file_path)\n",
    "            results.append((file_name, info))\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    data = {}\n",
    "    for file_name, info in results:\n",
    "        for item in info:\n",
    "            if item[0] not in data:\n",
    "                data[item[0]] = {}\n",
    "            data[item[0]][file_name] = item[1:]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    df.columns.name = 'File Name'\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No folder found with the convention s + 'number'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics(signal, fs=2000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    # Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # Variance (VAR)\n",
    "    var = np.var(signal)\n",
    "    \n",
    "    # Coefficient of Variation (CoV)\n",
    "    mean_signal = np.mean(signal)\n",
    "    cov = (np.std(signal) / mean_signal) if mean_signal != 0 else 0\n",
    "    \n",
    "    # Mean Frequency (MNF)\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "    fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "    mnf = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    \n",
    "    # Marginal Discrete Wavelet Transform (mDWT)\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "    mdwt = np.sum([np.sum(np.abs(c)) for c in coeffs])\n",
    "    \n",
    "    # Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"RMS\": rms,\n",
    "        \"WL\": wl,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"VAR\": var,\n",
    "        \"CoV\": cov,\n",
    "        \"MNF\": mnf,\n",
    "        \"mDWT\": mdwt,\n",
    "        \"TD\": td,\n",
    "        \"MAVS\": mavs\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_std(signal, fs=2000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal, including mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:\n",
    "        metrics_per_channel = [calculate_emg_metrics(signal[:, ch], fs) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    mav_std = np.std(np.abs(signal))\n",
    "    \n",
    "    # Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    iav_std = np.std(np.abs(signal))\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    rms_std = np.std(signal)\n",
    "    \n",
    "    # Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    wl_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    zc_std = np.std(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    ssc_std = np.std((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # Variance (VAR)\n",
    "    var = np.var(signal)\n",
    "    var_std = np.std(signal)\n",
    "    \n",
    "    # Coefficient of Variation (CoV)\n",
    "    mean_signal = np.mean(signal)\n",
    "    cov = (np.std(signal) / mean_signal) if mean_signal != 0 else 0\n",
    "    cov_std = np.std(cov)\n",
    "    \n",
    "    # Mean Frequency (MNF)\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "    fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "    mnf = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    mnf_std = np.std(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    \n",
    "    # Marginal Discrete Wavelet Transform (mDWT)\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "    mdwt = np.sum([np.sum(np.abs(c)) for c in coeffs])\n",
    "    mdwt_std = np.std([np.sum(np.abs(c)) for c in coeffs])\n",
    "    \n",
    "    # Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    td_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    mavs_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav, \"MAV_STD\": mav_std,\n",
    "        \"IAV\": iav, \"IAV_STD\": iav_std,\n",
    "        \"RMS\": rms, \"RMS_STD\": rms_std,\n",
    "        \"WL\": wl, \"WL_STD\": wl_std,\n",
    "        \"ZC\": zc, \"ZC_STD\": zc_std,\n",
    "        \"SSC\": ssc, \"SSC_STD\": ssc_std,\n",
    "        \"VAR\": var, \"VAR_STD\": var_std,\n",
    "        \"CoV\": cov, \"CoV_STD\": cov_std,\n",
    "        \"MNF\": mnf, \"MNF_STD\": mnf_std,\n",
    "        \"mDWT\": mdwt, \"mDWT_STD\": mdwt_std,\n",
    "        \"TD\": td, \"TD_STD\": td_std,\n",
    "        \"MAVS\": mavs, \"MAVS_STD\": mavs_std\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_means(signal, fs=2000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal, including mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if signal.ndim == 2:\n",
    "            metrics_per_channel = [calculate_emg_metrics(signal[:, ch], fs) for ch in range(signal.shape[1])]\n",
    "            averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "            return averaged_metrics\n",
    "        \n",
    "        abs_signal = np.abs(signal)\n",
    "        diff_signal = np.diff(signal)\n",
    "        diff_abs_signal = np.abs(diff_signal)\n",
    "        \n",
    "        # Compute Metrics\n",
    "        metrics = {\n",
    "            \"MAV\": np.mean(abs_signal), \"MAV_STD\": np.std(abs_signal),\n",
    "            \"IAV\": np.sum(abs_signal), \"IAV_STD\": np.std(abs_signal),\n",
    "            \"RMS\": np.sqrt(np.mean(signal**2)), \"RMS_STD\": np.std(signal),\n",
    "            \"WL\": np.sum(diff_abs_signal), \"WL_STD\": np.std(diff_abs_signal),\n",
    "            \"ZC\": np.sum(np.diff(np.sign(signal)) != 0), \"ZC_STD\": np.std(np.diff(np.sign(signal)) != 0),\n",
    "            \"SSC\": np.sum((diff_signal[1:] * diff_signal[:-1]) < 0), \"SSC_STD\": np.std((diff_signal[1:] * diff_signal[:-1]) < 0),\n",
    "            \"VAR\": np.var(signal), \"VAR_STD\": np.std(signal),\n",
    "            \"CoV\": (np.std(signal) / np.mean(signal)) if np.mean(signal) != 0 else 0,\n",
    "            \"TD\": np.sum(diff_abs_signal), \"TD_STD\": np.std(diff_abs_signal),\n",
    "            \"MAVS\": np.mean(diff_abs_signal), \"MAVS_STD\": np.std(diff_abs_signal),\n",
    "            \"MNP\": np.mean(signal**2), \"MNP_STD\": np.std(signal**2),\n",
    "        }\n",
    "        \n",
    "        # Spectral Metrics\n",
    "        freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "        fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "        metrics[\"MNF\"] = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude) if np.sum(fft_magnitude) != 0 else 0\n",
    "        metrics[\"MNF_STD\"] = np.std(freqs * fft_magnitude) / np.sum(fft_magnitude) if np.sum(fft_magnitude) != 0 else 0\n",
    "        \n",
    "        # Wavelet Transform\n",
    "        coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "        mdwt_values = np.array([np.sum(np.abs(c)) for c in coeffs])\n",
    "        metrics[\"mDWT\"] = np.sum(mdwt_values)\n",
    "        metrics[\"mDWT_STD\"] = np.std(mdwt_values)\n",
    "        \n",
    "        # Kurtosis\n",
    "        std_signal = np.std(signal)\n",
    "        metrics[\"Kurt\"] = np.mean((signal - np.mean(signal)) ** 4) / (std_signal ** 4) if std_signal != 0 else 0\n",
    "        metrics[\"Kurt_STD\"] = np.std(metrics[\"Kurt\"])\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_emg_metrics: {e}\")\n",
    "        return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined dataframe for all database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name\n",
    "database = 'DB4'\n",
    "\n",
    "# Full path to the database folder\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "\n",
    "# List of subjects, generating names from 's1' to 's10'\n",
    "subjects = [f's{i}' for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all generated DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Look for folders matching the pattern \"s + number\" or \"Subject + number\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterate over all .mat files in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Attempt to load the .mat file\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Attempt to process the file with src.build_dataframe\n",
    "                try:\n",
    "                    test_df, grasps = src.build_dataframe(\n",
    "                        mat_file=mat_data,\n",
    "                        database=database,\n",
    "                        filename=file_name,\n",
    "                        rectify=False,\n",
    "                        normalize=True\n",
    "                    )\n",
    "                    \n",
    "                    # Add a column with the subject name (folder) to the DataFrame\n",
    "                    test_df['subject'] = folder  \n",
    "                    \n",
    "                    # Append the processed DataFrame to the list\n",
    "                    all_dataframes.append(test_df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Concatenate all DataFrames into a single one if data is available\n",
    "if all_dataframes:  \n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Display the combined DataFrame\n",
    "    print(\"\\n Combined DataFrame:\")\n",
    "    display(combined_df)  \n",
    "\n",
    "else:\n",
    "    print(\"Warning: No DataFrames were generated. Check the input data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics with std for every channel, for every grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (subject, relabeled), group in combined_df.groupby(['subject', 'relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics_means(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                \"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"subject\", \"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'channel' column to group data by subject and movement type\n",
    "grouped_df = metrics_df.drop(columns=['channel'])\n",
    "\n",
    "# Compute the mean value of each metric grouped by subject and movement\n",
    "df_mean = grouped_df.groupby(['subject', 'relabeled']).mean()\n",
    "\n",
    "# Compute the standard deviation of each metric grouped by subject and movement\n",
    "df_std = grouped_df.groupby(['subject', 'relabeled']).std()\n",
    "\n",
    "# Rename columns to indicate they contain mean values\n",
    "df_mean.columns = [f\"{col} mean\" for col in df_mean.columns]\n",
    "\n",
    "# Rename columns to indicate they contain standard deviation values\n",
    "df_std.columns = [f\"{col} std\" for col in df_std.columns]\n",
    "\n",
    "# Merge the mean and standard deviation DataFrames into a single DataFrame\n",
    "df_result = df_mean.merge(df_std, on=['subject', 'relabeled']).reset_index()\n",
    "\n",
    "# Display the final DataFrame with aggregated metrics\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered labels from mahalanobis analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specific 'relabeled' values we want to filter\n",
    "filtered_labels = [55, 2, 14, 19, 32, 0]\n",
    "\n",
    "# Filter the grouped DataFrame\n",
    "dataframe_windowing = grouped_df.loc[filtered_labels]\n",
    "\n",
    "dataframe_windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered for channel 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (subject, relabeled), group in combined_df.groupby(['subject', 'relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel == 'Channel 10':  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics_means(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                \"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"subject\", \"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (subject, relabeled), group in combined_df.groupby(['subject', 'relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel == 'Channel 10':  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics_means(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                \"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df = metrics_df[metrics_df['relabeled'].isin(filtered_labels)]\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"subject\", \"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_channel = \"Channel_10\"  # Canal objetivo a graficar\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3 for the current subject\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        try:\n",
    "            mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "            \n",
    "            # Verify the structure of the loaded dictionary\n",
    "            print(f\"Keys in mat_data: {mat_data.keys()}\")\n",
    "            \n",
    "            # Retrieve re-labeled data and the list of labeled grasps\n",
    "            df_norm, grasps_etiquetados = src.build_dataframe(\n",
    "                mat_file=mat_data,\n",
    "                database=database,\n",
    "                filename=filename,\n",
    "                rectify=False,\n",
    "                normalize=True\n",
    "            )\n",
    "            df_norm = df_norm[df_norm['relabeled'].isin(filtered_labels)]\n",
    "            # Configuración para la extracción de envoltorios\n",
    "            fm = 2000\n",
    "            normalize_envelope = False\n",
    "            \n",
    "            # Iterate over each labeled grasp\n",
    "            for grasp in grasps_etiquetados:\n",
    "                try:\n",
    "                    #print(f\"\\nProcessing Grasp {grasp}:\")\n",
    "                    \n",
    "                    # Filtrar el DataFrame para el agarre específico\n",
    "                    grasp_df = df_norm[df_norm['stimulus'] == grasp].copy()\n",
    "                    \n",
    "                    if grasp_df.empty:\n",
    "                    #     print(f\"No data found for grasp {grasp}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Select EMG columns\n",
    "                    emg_columns = [col for col in grasp_df.columns if \"Channel\" in col]\n",
    "                    \n",
    "                    if not emg_columns:\n",
    "                        print(f\"No EMG channels found. Available columns: {grasp_df.columns.tolist()}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Verificar si el canal objetivo existe\n",
    "                    if target_channel not in emg_columns:\n",
    "                        print(f\"Target channel {target_channel} not found. Available channels: {emg_columns}\")\n",
    "                        # Si el canal 10 específico no existe, intentar encontrar un canal equivalente\n",
    "                        if \"Channel_10\" not in emg_columns and any(col.endswith(\"10\") for col in emg_columns):\n",
    "                            # Buscar cualquier canal que termine en \"10\"\n",
    "                            target_channel = next(col for col in emg_columns if col.endswith(\"10\"))\n",
    "                            print(f\"Using alternative channel: {target_channel}\")\n",
    "                        else:\n",
    "                            print(\"Cannot find any channel equivalent to Channel_10, skipping this grasp\")\n",
    "                            continue\n",
    "                        \n",
    "                    #print(f\"Processing channel: {target_channel}\")\n",
    "                    \n",
    "                    # Lista de tipos de envoltorios a iterar\n",
    "                    envelope_types = [1]\n",
    "                    \n",
    "                    # Lista para almacenar DataFrames transformados\n",
    "                    transformed_dfs = []\n",
    "                    \n",
    "                    for envelope_type in envelope_types:\n",
    "                        print(f\"  Applying envelope type {envelope_type}\")\n",
    "                        \n",
    "                        # Extraer canales EMG y aplicar envoltorios\n",
    "                        emg_channels_df = src.extract_emg_channels(grasp_df)\n",
    "                        envelope_df = src.get_envelope_lowpass(\n",
    "                            emg_channels_df, \n",
    "                            fm, \n",
    "                            envelope_type=envelope_type, \n",
    "                            cutoff_freq=0.6\n",
    "                        )\n",
    "                        \n",
    "                        # Normalizar por el valor máximo absoluto en todos los canales\n",
    "                        if normalize_envelope:\n",
    "                            global_max = envelope_df.abs().values.max()\n",
    "                            if global_max != 0:\n",
    "                                envelope_df = envelope_df / global_max\n",
    "                        \n",
    "                        # Preservar columnas no EMG\n",
    "                        meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                        available_meta_columns = [col for col in meta_columns if col in grasp_df.columns]\n",
    "                        \n",
    "                        result_df = pd.concat([envelope_df, grasp_df[available_meta_columns]], axis=1)\n",
    "                        transformed_dfs.append(result_df)\n",
    "\n",
    "                    try:\n",
    "                        print(f\"Creating combined plot for {target_channel}\")\n",
    "\n",
    "                        # Crear figura y ejes\n",
    "                        plt.figure(figsize=(12, 6))\n",
    "                        \n",
    "                        # Graficar la señal original\n",
    "                        plt.plot(grasp_df[\"Time (s)\"], grasp_df[target_channel], 'b-', \n",
    "                                label='Original Signal', linewidth=1, color = 'c')\n",
    "                        \n",
    "                        # Graficar los dos envoltorios en el mismo gráfico\n",
    "                        colors = ['r-', 'g-']\n",
    "                        for i, (df, envelope_type) in enumerate(zip(transformed_dfs, envelope_types)):\n",
    "                            plt.plot(df[\"Time (s)\"], df[target_channel], colors[i], \n",
    "                                    label=f'Envelope Type {envelope_type}', linewidth=3, color = 'm')\n",
    "                        # Configurar el gráfico\n",
    "                        plt.title(f\"{filename} - Grasp {grasp} - {target_channel}\")\n",
    "                        plt.xlabel(\"Time (s)\")\n",
    "                        plt.ylabel(\"Amplitude\")\n",
    "                        plt.legend()\n",
    "                        plt.grid(True)\n",
    "                        \n",
    "                        # Ajustar la visualización\n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        # Guardar y mostrar\n",
    "                        output_filename = f\"{subject}_{exercise}_grasp{grasp}_{target_channel}_combined.png\"\n",
    "                        #plt.savefig(output_filename)\n",
    "                        plt.show()\n",
    "                        print(f\"Plot saved as {output_filename}\")\n",
    "                        \n",
    "                    except Exception as plot_error:\n",
    "                        print(f\"Error creating combined plot: {str(plot_error)}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                        \n",
    "                except KeyError as e:\n",
    "                    print(f\"    Error: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error processing grasp {grasp}: {str(e)}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 100 ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para la ventaneo\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 200  # 100 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"\n",
    "\n",
    "# Lista para almacenar todos los DataFrames generados\n",
    "all_dataframes = []\n",
    "\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                df_norm, grasps_etiquetados = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                df_norm = df_norm[df_norm['relabeled'].isin(filtered_labels)]\n",
    "                print(f\"Columnas disponibles en {file_name}: {df_norm.columns.tolist()}\")\n",
    "\n",
    "                # Verificar si el canal objetivo está presente en el DataFrame\n",
    "                if target_channel not in df_norm.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Aplicar extracción del envelope solo a Channel_10\n",
    "                envelope_df = src.get_envelope_lowpass(df_norm[[target_channel]], fm = 2000,cutoff_freq = 0.6 ,envelope_type=1)  \n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, df_norm[meta_columns]], axis=1)\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps_etiquetados:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Guardar cada ventana como un DataFrame individual, solo si tiene el tamaño completo\n",
    "                        ventanas_df = [pd.DataFrame(ventana, columns=[target_channel]) for ventana in ventanas if len(ventana) == window_length]\n",
    "                        \n",
    "                        if not ventanas_df:\n",
    "                            print(f\"No hay ventanas válidas para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Agregar a la lista general de DataFrames\n",
    "                        all_dataframes.extend(ventanas_df)\n",
    "                        \n",
    "                        # Graficar la primera ventana de este grasp\n",
    "                        plt.figure(figsize=(10, 4))\n",
    "                        plt.plot(np.linspace(0, window_length / fm, window_length), ventanas_df[0][target_channel], label=target_channel)\n",
    "                        plt.xlabel(\"Tiempo (s)\")\n",
    "                        plt.ylabel(\"Señal EMG\")\n",
    "                        plt.title(f\"{file_name} - Grasp {grasp} - {target_channel}\")\n",
    "                        plt.legend()\n",
    "                        plt.grid()\n",
    "                        plt.show()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para la ventaneo\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 200  # 100 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"\n",
    "\n",
    "# Lista para almacenar todos los DataFrames generados\n",
    "all_dataframes = []\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                df_norm, grasps_etiquetados = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                df_norm = df_norm[df_norm['relabeled'].isin(filtered_labels)]\n",
    "                print(f\"Columnas disponibles en {file_name}: {df_norm.columns.tolist()}\")\n",
    "\n",
    "                # Verificar si el canal objetivo está presente en el DataFrame\n",
    "                if target_channel not in df_norm.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Guardar la señal cruda antes de aplicar el envelope\n",
    "                raw_signal = df_norm[[target_channel]].copy()\n",
    "                \n",
    "                # Aplicar extracción del envelope solo a Channel_10\n",
    "                envelope_df = src.get_envelope_lowpass(df_norm[[target_channel]], fm=2000, cutoff_freq=0.6, envelope_type=1)  \n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                # Combinar señal envolvente con metadatos\n",
    "                result_df = pd.concat([envelope_df, df_norm[meta_columns]], axis=1)\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps_etiquetados:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        raw_grasp_df = raw_signal[df_norm['stimulus'] == grasp]  # Datos crudos correspondientes\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        ventanas_raw = src.create_windows_with_overlap(raw_grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Guardar cada ventana como un DataFrame individual, solo si tiene el tamaño completo\n",
    "                        ventanas_df = [pd.DataFrame(ventana, columns=[target_channel]) for ventana in ventanas if len(ventana) == window_length]\n",
    "                        ventanas_raw_df = [pd.DataFrame(ventana, columns=[target_channel]) for ventana in ventanas_raw if len(ventana) == window_length]\n",
    "                        \n",
    "                        if not ventanas_df or not ventanas_raw_df:\n",
    "                            print(f\"No hay ventanas válidas para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Agregar a la lista general de DataFrames\n",
    "                        all_dataframes.extend(ventanas_df)\n",
    "                        \n",
    "                        # Graficar la primera ventana de este grasp con ambas señales\n",
    "                        plt.figure(figsize=(12, 5))\n",
    "                        tiempo = np.linspace(0, window_length / fm, window_length)\n",
    "                        \n",
    "                        # Graficar señal cruda\n",
    "                        plt.plot(tiempo, ventanas_raw_df[0][target_channel], color='c', alpha=0.7, \n",
    "                                label=f\"{target_channel} - Señal Cruda\")\n",
    "                        \n",
    "                        # Graficar envolvente\n",
    "                        plt.plot(tiempo, ventanas_df[0][target_channel], color='m', linewidth=2, \n",
    "                                label=f\"{target_channel} - Envolvente\")\n",
    "                        \n",
    "                        plt.xlabel(\"Tiempo (s)\")\n",
    "                        plt.ylabel(\"Amplitud\")\n",
    "                        plt.title(f\"{file_name} - Grasp {grasp} - {target_channel}\")\n",
    "                        plt.legend()\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 200 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para la ventaneo\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 400  # 100 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"\n",
    "\n",
    "# Lista para almacenar todos los DataFrames generados\n",
    "all_dataframes = []\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                df_norm, grasps_etiquetados = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                df_norm = df_norm[df_norm['relabeled'].isin(filtered_labels)]\n",
    "                print(f\"Columnas disponibles en {file_name}: {df_norm.columns.tolist()}\")\n",
    "\n",
    "                # Verificar si el canal objetivo está presente en el DataFrame\n",
    "                if target_channel not in df_norm.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Guardar la señal cruda antes de aplicar el envelope\n",
    "                raw_signal = df_norm[[target_channel]].copy()\n",
    "                \n",
    "                # Aplicar extracción del envelope solo a Channel_10\n",
    "                envelope_df = src.get_envelope_lowpass(df_norm[[target_channel]], fm=2000, cutoff_freq=0.6, envelope_type=1)  \n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                # Combinar señal envolvente con metadatos\n",
    "                result_df = pd.concat([envelope_df, df_norm[meta_columns]], axis=1)\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps_etiquetados:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        raw_grasp_df = raw_signal[df_norm['stimulus'] == grasp]  # Datos crudos correspondientes\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            #print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        ventanas_raw = src.create_windows_with_overlap(raw_grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Guardar cada ventana como un DataFrame individual, solo si tiene el tamaño completo\n",
    "                        ventanas_df = [pd.DataFrame(ventana, columns=[target_channel]) for ventana in ventanas if len(ventana) == window_length]\n",
    "                        ventanas_raw_df = [pd.DataFrame(ventana, columns=[target_channel]) for ventana in ventanas_raw if len(ventana) == window_length]\n",
    "                        \n",
    "                        if not ventanas_df or not ventanas_raw_df:\n",
    "                            print(f\"No hay ventanas válidas para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Agregar a la lista general de DataFrames\n",
    "                        all_dataframes.extend(ventanas_df)\n",
    "                        \n",
    "                        # Graficar la primera ventana de este grasp con ambas señales\n",
    "                        plt.figure(figsize=(12, 5))\n",
    "                        tiempo = np.linspace(0, window_length / fm, window_length)\n",
    "                        \n",
    "                        # Graficar señal cruda\n",
    "                        plt.plot(tiempo, ventanas_raw_df[0][target_channel], color='c', alpha=0.7, \n",
    "                                label=f\"{target_channel} - Señal Cruda\")\n",
    "                        \n",
    "                        # Graficar envolvente\n",
    "                        plt.plot(tiempo, ventanas_df[0][target_channel], color='m', linewidth=2, \n",
    "                                label=f\"{target_channel} - Envolvente\")\n",
    "                        \n",
    "                        plt.xlabel(\"Tiempo (s)\")\n",
    "                        plt.ylabel(\"Amplitud\")\n",
    "                        plt.title(f\"{file_name} - Grasp {grasp} - {target_channel}\")\n",
    "                        plt.legend()\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 300 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para la ventaneo\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 600  # 100 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"\n",
    "\n",
    "# Lista para almacenar todos los DataFrames generados\n",
    "all_dataframes = []\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                \n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                df_norm, grasps_etiquetados = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                df_norm = df_norm[df_norm['relabeled'].isin(filtered_labels)]\n",
    "                print(f\"Columnas disponibles en {file_name}: {df_norm.columns.tolist()}\")\n",
    "\n",
    "                # Verificar si el canal objetivo está presente en el DataFrame\n",
    "                if target_channel not in df_norm.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Guardar la señal cruda antes de aplicar el envelope\n",
    "                raw_signal = df_norm[[target_channel]].copy()\n",
    "                \n",
    "                # Aplicar extracción del envelope solo a Channel_10\n",
    "                envelope_df = src.get_envelope_lowpass(df_norm[[target_channel]], fm=2000, cutoff_freq=0.6, envelope_type=1)  \n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                # Combinar señal envolvente con metadatos\n",
    "                result_df = pd.concat([envelope_df, df_norm[meta_columns]], axis=1)\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps_etiquetados:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        raw_grasp_df = raw_signal[df_norm['stimulus'] == grasp]  # Datos crudos correspondientes\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        ventanas_raw = src.create_windows_with_overlap(raw_grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Guardar cada ventana como un DataFrame individual, solo si tiene el tamaño completo\n",
    "                        ventanas_df = [pd.DataFrame(ventana, columns=[target_channel]) for ventana in ventanas if len(ventana) == window_length]\n",
    "                        ventanas_raw_df = [pd.DataFrame(ventana, columns=[target_channel]) for ventana in ventanas_raw if len(ventana) == window_length]\n",
    "                        \n",
    "                        if not ventanas_df or not ventanas_raw_df:\n",
    "                            print(f\"No hay ventanas válidas para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Agregar a la lista general de DataFrames\n",
    "                        all_dataframes.extend(ventanas_df)\n",
    "                        \n",
    "                        # Graficar la primera ventana de este grasp con ambas señales\n",
    "                        plt.figure(figsize=(12, 5))\n",
    "                        tiempo = np.linspace(0, window_length / fm, window_length)\n",
    "                        \n",
    "                        # Graficar señal cruda\n",
    "                        plt.plot(tiempo, ventanas_raw_df[0][target_channel], color='c', alpha=0.7, \n",
    "                                label=f\"{target_channel} - Señal Cruda\")\n",
    "                        \n",
    "                        # Graficar envolvente\n",
    "                        plt.plot(tiempo, ventanas_df[0][target_channel], color='m', linewidth=2, \n",
    "                                label=f\"{target_channel} - Envolvente\")\n",
    "                        \n",
    "                        plt.xlabel(\"Tiempo (s)\")\n",
    "                        plt.ylabel(\"Amplitud\")\n",
    "                        plt.title(f\"{file_name} - Grasp {grasp} - {target_channel}\")\n",
    "                        plt.legend()\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 100 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el ventaneado\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 200  # Ventana de 300 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"  # Canal específico para extraer métricas\n",
    "\n",
    "# Lista para almacenar las métricas de todas las ventanas\n",
    "all_metrics = []\n",
    "\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                test_df, grasps = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "                \n",
    "                # Verificar si el canal objetivo está presente\n",
    "                if target_channel not in test_df.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Aplicar extracción del envelope a todos los canales EMG o solo al canal objetivo\n",
    "                emg_columns = [target_channel]  # Solo procesamos el canal objetivo\n",
    "                envelope_df = src.get_envelope_lowpass(test_df[emg_columns], fm=2000, cutoff_freq=0.6, envelope_type=1)\n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                \n",
    "                window_count = 0  # Contador de ventanas para este archivo\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp} in file {file_name}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Procesar cada ventana\n",
    "                        for i, ventana in enumerate(ventanas):\n",
    "                            if len(ventana) == window_length:  # Solo procesar ventanas completas\n",
    "                                # Extraer señal del canal objetivo\n",
    "                                signal = ventana[target_channel].values\n",
    "                                \n",
    "                                # Calcular métricas para esta ventana\n",
    "                                metrics = calculate_emg_metrics_means(signal)\n",
    "                                \n",
    "                                # Agregar metadata\n",
    "                                metrics_with_meta = {\n",
    "                                    \"subject\": folder,\"relabeled\": grasp_df['relabeled'].iloc[0],  # Tomamos el primer valor\n",
    "                                    \"stimulus\": grasp,\"channel\": target_channel,\"window_id\": f\"{file_name}_{grasp}_{i}\",\"file_name\": file_name,\"window_number\": window_count,**metrics  # Desempaquetar todas las métricas calculadas\n",
    "                                }\n",
    "                                \n",
    "                                # Añadir a la lista general de métricas\n",
    "                                all_metrics.append(metrics_with_meta)\n",
    "                                window_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"Procesadas {window_count} ventanas para el archivo {file_name}\")\n",
    "\n",
    "# Crear DataFrame con todas las métricas calculadas\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Reordenar columnas para mejor visualización\n",
    "meta_cols = [\"subject\", \"relabeled\", \"stimulus\", \"channel\", \"window_id\", \"file_name\", \"window_number\"]\n",
    "metric_cols = [col for col in metrics_df.columns if col not in meta_cols]\n",
    "column_order = meta_cols + sorted(metric_cols)\n",
    "metrics_df = metrics_df[column_order]\n",
    "display(metrics_df)\n",
    "\n",
    "# Mostrar estadísticas resumidas\n",
    "print(\"\\nResumen de métricas por sujeto y tipo de movimiento:\")\n",
    "grouped_df = metrics_df.drop(columns=['channel'])\n",
    "#summary_by_subject_movement = grouped_df.select_dtypes(include=['number']).groupby(['relabeled']).mean()\n",
    "summary_by_subject_movement = grouped_df.select_dtypes(include=['number']).groupby(['relabeled']).mean()\n",
    "summary_by_subject_movement.drop('channel', axis=1, inplace=True, errors='ignore')\n",
    "#summary_by_subject_movement = grouped_df.groupby\n",
    "display(summary_by_subject_movement)\n",
    "\n",
    "# # Guardar los resultados en un archivo CSV\n",
    "# output_file = \"emg_metrics_windowed.csv\"\n",
    "# #metrics_df.to_csv(output_file, index=False)\n",
    "# print(f\"\\nMétricas guardadas en {output_file}\")\n",
    "\n",
    "# Mostrar información sobre los datos recolectados\n",
    "print(f\"\\nTotal de ventanas procesadas: {len(metrics_df)}\")\n",
    "print(f\"Distribución por sujeto:\\n{metrics_df['subject'].value_counts()}\")\n",
    "print(f\"Distribución por movimiento:\\n{metrics_df['relabeled'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 200 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el ventaneado\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 400  # Ventana de 300 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"  # Canal específico para extraer métricas\n",
    "\n",
    "# Lista para almacenar las métricas de todas las ventanas\n",
    "all_metrics = []\n",
    "\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                test_df, grasps = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "                \n",
    "                # Verificar si el canal objetivo está presente\n",
    "                if target_channel not in test_df.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Aplicar extracción del envelope a todos los canales EMG o solo al canal objetivo\n",
    "                emg_columns = [target_channel]  # Solo procesamos el canal objetivo\n",
    "                envelope_df = src.get_envelope_lowpass(test_df[emg_columns], fm=2000, cutoff_freq=0.6, envelope_type=1)\n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                \n",
    "                window_count = 0  # Contador de ventanas para este archivo\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp} in file {file_name}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Procesar cada ventana\n",
    "                        for i, ventana in enumerate(ventanas):\n",
    "                            if len(ventana) == window_length:  # Solo procesar ventanas completas\n",
    "                                # Extraer señal del canal objetivo\n",
    "                                signal = ventana[target_channel].values\n",
    "                                \n",
    "                                # Calcular métricas para esta ventana\n",
    "                                metrics = calculate_emg_metrics_means(signal)\n",
    "                                \n",
    "                                # Agregar metadata\n",
    "                                metrics_with_meta = {\n",
    "                                    \"subject\": folder,\"relabeled\": grasp_df['relabeled'].iloc[0],  # Tomamos el primer valor\n",
    "                                    \"stimulus\": grasp,\"channel\": target_channel,\"window_id\": f\"{file_name}_{grasp}_{i}\",\"file_name\": file_name,\"window_number\": window_count,**metrics  # Desempaquetar todas las métricas calculadas\n",
    "                                }\n",
    "                                \n",
    "                                # Añadir a la lista general de métricas\n",
    "                                all_metrics.append(metrics_with_meta)\n",
    "                                window_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"Procesadas {window_count} ventanas para el archivo {file_name}\")\n",
    "\n",
    "# Crear DataFrame con todas las métricas calculadas\n",
    "metrics_df_200 = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Reordenar columnas para mejor visualización\n",
    "meta_cols = [\"subject\", \"relabeled\", \"stimulus\", \"channel\", \"window_id\", \"file_name\", \"window_number\"]\n",
    "metric_cols = [col for col in metrics_df_200.columns if col not in meta_cols]\n",
    "column_order = meta_cols + sorted(metric_cols)\n",
    "metrics_df_200 = metrics_df_200[column_order]\n",
    "display(metrics_df_200)\n",
    "\n",
    "# Mostrar estadísticas resumidas\n",
    "print(\"\\nResumen de métricas por sujeto y tipo de movimiento:\")\n",
    "grouped_df = metrics_df_200.drop(columns=['channel'])\n",
    "summary_by_subject_movement_200 = grouped_df.select_dtypes(include=['number']).groupby(['relabeled']).mean()\n",
    "\n",
    "summary_by_subject_movement_200.drop('channel', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "display(summary_by_subject_movement_200)\n",
    "\n",
    "# # Guardar los resultados en un archivo CSV\n",
    "# output_file = \"emg_metrics_windowed.csv\"\n",
    "# #metrics_df.to_csv(output_file, index=False)\n",
    "# print(f\"\\nMétricas guardadas en {output_file}\")\n",
    "\n",
    "# Mostrar información sobre los datos recolectados\n",
    "print(f\"\\nTotal de ventanas procesadas: {len(metrics_df)}\")\n",
    "print(f\"Distribución por sujeto:\\n{metrics_df['subject'].value_counts()}\")\n",
    "print(f\"Distribución por movimiento:\\n{metrics_df['relabeled'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 300 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el ventaneado\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 600  # Ventana de 300 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"  # Canal específico para extraer métricas\n",
    "\n",
    "# Lista para almacenar las métricas de todas las ventanas\n",
    "all_metrics = []\n",
    "\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                test_df, grasps = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "                \n",
    "                # Verificar si el canal objetivo está presente\n",
    "                if target_channel not in test_df.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Aplicar extracción del envelope a todos los canales EMG o solo al canal objetivo\n",
    "                emg_columns = [target_channel]  # Solo procesamos el canal objetivo\n",
    "                envelope_df = src.get_envelope_lowpass(test_df[emg_columns], fm=2000, cutoff_freq=0.6, envelope_type=1)\n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                \n",
    "                window_count = 0  # Contador de ventanas para este archivo\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp} in file {file_name}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Procesar cada ventana\n",
    "                        for i, ventana in enumerate(ventanas):\n",
    "                            if len(ventana) == window_length:  # Solo procesar ventanas completas\n",
    "                                # Extraer señal del canal objetivo\n",
    "                                signal = ventana[target_channel].values\n",
    "                                \n",
    "                                # Calcular métricas para esta ventana\n",
    "                                metrics = calculate_emg_metrics_means(signal)\n",
    "                                \n",
    "                                # Agregar metadata\n",
    "                                metrics_with_meta = {\n",
    "                                    \"subject\": folder,\"relabeled\": grasp_df['relabeled'].iloc[0],  # Tomamos el primer valor\n",
    "                                    \"stimulus\": grasp,\"channel\": target_channel,\"window_id\": f\"{file_name}_{grasp}_{i}\",\"file_name\": file_name,\"window_number\": window_count,**metrics  # Desempaquetar todas las métricas calculadas\n",
    "                                }\n",
    "                                \n",
    "                                # Añadir a la lista general de métricas\n",
    "                                all_metrics.append(metrics_with_meta)\n",
    "                                window_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"Procesadas {window_count} ventanas para el archivo {file_name}\")\n",
    "\n",
    "# Crear DataFrame con todas las métricas calculadas\n",
    "metrics_df_300 = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Reordenar columnas para mejor visualización\n",
    "meta_cols = [\"subject\", \"relabeled\", \"stimulus\", \"channel\", \"window_id\", \"file_name\", \"window_number\"]\n",
    "metric_cols = [col for col in metrics_df_300.columns if col not in meta_cols]\n",
    "column_order = meta_cols + sorted(metric_cols)\n",
    "metrics_df_200 = metrics_df_300[column_order]\n",
    "display(metrics_df_300)\n",
    "\n",
    "# Mostrar estadísticas resumidas\n",
    "print(\"\\nResumen de métricas por sujeto y tipo de movimiento:\")\n",
    "grouped_df = metrics_df_300.drop(columns=['channel'])\n",
    "summary_by_subject_movement_300 = grouped_df.select_dtypes(include=['number']).groupby(['relabeled']).mean()\n",
    "\n",
    "summary_by_subject_movement_300.drop('channel', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "display(summary_by_subject_movement_300)\n",
    "\n",
    "# # Guardar los resultados en un archivo CSV\n",
    "# output_file = \"emg_metrics_windowed.csv\"\n",
    "# #metrics_df.to_csv(output_file, index=False)\n",
    "# print(f\"\\nMétricas guardadas en {output_file}\")\n",
    "\n",
    "# Mostrar información sobre los datos recolectados\n",
    "print(f\"\\nTotal de ventanas procesadas: {len(metrics_df)}\")\n",
    "print(f\"Distribución por sujeto:\\n{metrics_df['subject'].value_counts()}\")\n",
    "print(f\"Distribución por movimiento:\\n{metrics_df['relabeled'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes por grasp with enveloped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 300 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el ventaneado\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 600  # Ventana de 300 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"  # Canal específico para extraer métricas\n",
    "\n",
    "# Diccionario para almacenar DataFrames por cada valor único de 'relabeled'\n",
    "metrics_dfs_by_relabeled = {}\n",
    "\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                test_df, grasps = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "                \n",
    "                # Verificar si el canal objetivo está presente\n",
    "                if target_channel not in test_df.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Aplicar extracción del envelope\n",
    "                emg_columns = [target_channel]  # Solo procesamos el canal objetivo\n",
    "                envelope_df = src.get_envelope_lowpass(test_df[emg_columns], fm=2000, cutoff_freq=0.6, envelope_type=1)\n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps:\n",
    "                    try:\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Procesar cada ventana\n",
    "                        for i, ventana in enumerate(ventanas):\n",
    "                            if len(ventana) == window_length:  # Solo procesar ventanas completas\n",
    "                                # Extraer señal del canal objetivo\n",
    "                                signal = ventana[target_channel].values\n",
    "                                \n",
    "                                # Calcular métricas para esta ventana\n",
    "                                metrics = calculate_emg_metrics_means(signal)\n",
    "                                \n",
    "                                # Agregar metadata\n",
    "                                relabeled_value = grasp_df['relabeled'].iloc[0]\n",
    "                                metrics_with_meta = {\n",
    "                                    \"subject\": folder,\n",
    "                                    \"relabeled\": relabeled_value,\n",
    "                                    \"stimulus\": grasp,\n",
    "                                    **metrics  # Desempaquetar todas las métricas calculadas\n",
    "                                }\n",
    "                                \n",
    "                                # Añadir al DataFrame correspondiente\n",
    "                                if relabeled_value not in metrics_dfs_by_relabeled:\n",
    "                                    metrics_dfs_by_relabeled[relabeled_value] = []\n",
    "                                metrics_dfs_by_relabeled[relabeled_value].append(metrics_with_meta)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "# Convertir listas en DataFrames, promediar por sujeto y asignar variables\n",
    "for relabeled_value, data in metrics_dfs_by_relabeled.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df_mean = df.groupby([\"subject\", \"relabeled\", \"stimulus\"]).mean().reset_index()\n",
    "    var_name = f\"df_relabeled_{relabeled_value}_300\"\n",
    "    globals()[var_name] = df_mean\n",
    "    display(globals()[var_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 200 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el ventaneado\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 400  # Ventana de 300 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"  # Canal específico para extraer métricas\n",
    "\n",
    "# Diccionario para almacenar DataFrames por cada valor único de 'relabeled'\n",
    "metrics_dfs_by_relabeled = {}\n",
    "\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                test_df, grasps = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "                \n",
    "                # Verificar si el canal objetivo está presente\n",
    "                if target_channel not in test_df.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Aplicar extracción del envelope\n",
    "                emg_columns = [target_channel]  # Solo procesamos el canal objetivo\n",
    "                envelope_df = src.get_envelope_lowpass(test_df[emg_columns], fm=2000, cutoff_freq=0.6, envelope_type=1)\n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps:\n",
    "                    try:\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Procesar cada ventana\n",
    "                        for i, ventana in enumerate(ventanas):\n",
    "                            if len(ventana) == window_length:  # Solo procesar ventanas completas\n",
    "                                # Extraer señal del canal objetivo\n",
    "                                signal = ventana[target_channel].values\n",
    "                                \n",
    "                                # Calcular métricas para esta ventana\n",
    "                                metrics = calculate_emg_metrics_means(signal)\n",
    "                                \n",
    "                                # Agregar metadata\n",
    "                                relabeled_value = grasp_df['relabeled'].iloc[0]\n",
    "                                metrics_with_meta = {\n",
    "                                    \"subject\": folder,\n",
    "                                    \"relabeled\": relabeled_value,\n",
    "                                    \"stimulus\": grasp,\n",
    "                                    **metrics  # Desempaquetar todas las métricas calculadas\n",
    "                                }\n",
    "                                \n",
    "                                # Añadir al DataFrame correspondiente\n",
    "                                if relabeled_value not in metrics_dfs_by_relabeled:\n",
    "                                    metrics_dfs_by_relabeled[relabeled_value] = []\n",
    "                                metrics_dfs_by_relabeled[relabeled_value].append(metrics_with_meta)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "# Convertir listas en DataFrames, promediar por sujeto y asignar variables\n",
    "for relabeled_value, data in metrics_dfs_by_relabeled.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df_mean = df.groupby([\"subject\", \"relabeled\", \"stimulus\"]).mean().reset_index()\n",
    "    var_name = f\"df_relabeled_{relabeled_value}_200\"\n",
    "    globals()[var_name] = df_mean\n",
    "    display(globals()[var_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 100 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros para el ventaneado\n",
    "fm = 2000  # Frecuencia de muestreo en Hz\n",
    "window_length = 200  # Ventana de 300 ms en muestras\n",
    "overlap = 0  # Sin superposición\n",
    "target_channel = \"Channel 10\"  # Canal específico para extraer métricas\n",
    "\n",
    "# Diccionario para almacenar DataFrames por cada valor único de 'relabeled'\n",
    "metrics_dfs_by_relabeled = {}\n",
    "\n",
    "# Buscar carpetas que coincidan con el patrón \"s + número\" o \"Subject + número\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterar sobre todos los archivos .mat en la carpeta\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Intentar cargar el archivo .mat\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Procesar el archivo con src.build_dataframe\n",
    "                test_df, grasps = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "                \n",
    "                # Verificar si el canal objetivo está presente\n",
    "                if target_channel not in test_df.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "                \n",
    "                # Aplicar extracción del envelope\n",
    "                emg_columns = [target_channel]  # Solo procesamos el canal objetivo\n",
    "                envelope_df = src.get_envelope_lowpass(test_df[emg_columns], fm=2000, cutoff_freq=0.6, envelope_type=1)\n",
    "                \n",
    "                # Conservar columnas meta\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                \n",
    "                # Procesar cada grasp\n",
    "                for grasp in grasps:\n",
    "                    try:\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "                        \n",
    "                        if grasp_df.empty:\n",
    "                            continue\n",
    "                        \n",
    "                        # Crear ventanas con overlap a partir del DataFrame filtrado\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "                        \n",
    "                        # Procesar cada ventana\n",
    "                        for i, ventana in enumerate(ventanas):\n",
    "                            if len(ventana) == window_length:  # Solo procesar ventanas completas\n",
    "                                # Extraer señal del canal objetivo\n",
    "                                signal = ventana[target_channel].values\n",
    "                                \n",
    "                                # Calcular métricas para esta ventana\n",
    "                                metrics = calculate_emg_metrics_means(signal)\n",
    "                                \n",
    "                                # Agregar metadata\n",
    "                                relabeled_value = grasp_df['relabeled'].iloc[0]\n",
    "                                metrics_with_meta = {\n",
    "                                    \"subject\": folder,\n",
    "                                    \"relabeled\": relabeled_value,\n",
    "                                    \"stimulus\": grasp,\n",
    "                                    **metrics  # Desempaquetar todas las métricas calculadas\n",
    "                                }\n",
    "                                \n",
    "                                # Añadir al DataFrame correspondiente\n",
    "                                if relabeled_value not in metrics_dfs_by_relabeled:\n",
    "                                    metrics_dfs_by_relabeled[relabeled_value] = []\n",
    "                                metrics_dfs_by_relabeled[relabeled_value].append(metrics_with_meta)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "# Convertir listas en DataFrames, promediar por sujeto y asignar variables\n",
    "for relabeled_value, data in metrics_dfs_by_relabeled.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df_mean = df.groupby([\"subject\", \"relabeled\", \"stimulus\"]).mean().reset_index()\n",
    "    var_name = f\"df_relabeled_{relabeled_value}_100\"\n",
    "    globals()[var_name] = df_mean\n",
    "    display(globals()[var_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOXPLOTS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analysis for average of grasps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que los DataFrames ya están cargados\n",
    "dataframes = {\n",
    "    '300': summary_by_subject_movement_300,\n",
    "    '200': summary_by_subject_movement_200,\n",
    "    '100': summary_by_subject_movement\n",
    "}\n",
    "\n",
    "# Definir las métricas excluyendo 'window_number'\n",
    "metrics = [col for col in summary_by_subject_movement.columns if col != 'window_number']\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = MinMaxScaler()\n",
    "normalized_dataframes = {}\n",
    "for label, df in dataframes.items():\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized[metrics] = scaler.fit_transform(df[metrics])\n",
    "    normalized_dataframes[label] = df_normalized\n",
    "\n",
    "# Crear un solo DataFrame para facilitar el boxplot\n",
    "merged_data = []\n",
    "for label, df in normalized_dataframes.items():\n",
    "    df_melted = df[metrics].melt(var_name='Métrica', value_name='Valor')\n",
    "    df_melted['Fuente'] = label\n",
    "    merged_data.append(df_melted)\n",
    "\n",
    "df_final = pd.concat(merged_data, ignore_index=True)\n",
    "\n",
    "# Configurar el gráfico\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=df_final, x='Métrica', y='Valor', hue='Fuente')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribución de métricas normalizadas en los diferentes DataFrames')\n",
    "plt.xlabel('Métrica')\n",
    "plt.ylabel('Valor normalizado')\n",
    "plt.legend(title='Fuente')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analysis for each grasp with all metrics in each windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Organizar los datos por relabeled y tamaño de ventana\n",
    "ventanas = {\n",
    "    '100': {},  # DataFrames para ventana de 100ms\n",
    "    '200': {},  # DataFrames para ventana de 200ms\n",
    "    '300': {}   # DataFrames para ventana de 300ms\n",
    "}\n",
    "\n",
    "# Buscar todos los DataFrames globales que siguen el patrón df_relabeled_*_100\n",
    "for var_name in globals():\n",
    "    # Revisamos para las ventanas de 100ms\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_100'):\n",
    "        relabeled_value = var_name.replace('df_relabeled_', '').replace('_100', '')\n",
    "        ventanas['100'][relabeled_value] = globals()[var_name]\n",
    "    \n",
    "    # Para ventanas de 200ms (_200) y 300ms (_300)\n",
    "    elif var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.replace('df_relabeled_', '').replace('_200', '')\n",
    "        ventanas['200'][relabeled_value] = globals()[var_name]\n",
    "    \n",
    "    elif var_name.startswith('df_relabeled_') and var_name.endswith('_300'):\n",
    "        relabeled_value = var_name.replace('df_relabeled_', '').replace('_300', '')\n",
    "        ventanas['300'][relabeled_value] = globals()[var_name]\n",
    "\n",
    "# 2. Identificar todas las métricas (columnas comunes excluyendo metadatos)\n",
    "metadata_cols = ['subject', 'relabeled', 'stimulus', 'channel', 'window_id', 'file_name']\n",
    "first_df = next(iter(next(iter(ventanas.values())).values()), None)\n",
    "\n",
    "if first_df is not None:\n",
    "    metrics = [col for col in first_df.columns if col not in metadata_cols]\n",
    "else:\n",
    "    metrics = []  \n",
    "\n",
    "# 3. Crear un DataFrame combinado para cada tamaño de ventana\n",
    "combined_data = []\n",
    "\n",
    "for window_size, relabeled_dict in ventanas.items():\n",
    "    for relabeled_value, df in relabeled_dict.items():\n",
    "        # Normalizar los datos de métricas para este relabeled\n",
    "        df_norm = df.copy()\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        # Solo normalizar columnas numéricas de métricas si hay datos\n",
    "        if len(df) > 0 and not df[metrics].empty:\n",
    "            df_norm[metrics] = scaler.fit_transform(df[metrics])\n",
    "        \n",
    "        # Convertir a formato largo para seaborn\n",
    "        df_melted = df_norm[metrics].melt(var_name='Métrica', value_name='Valor')\n",
    "        df_melted['Relabeled'] = relabeled_value\n",
    "        df_melted['Ventana'] = f'{window_size}'  # Mantener solo el número para la leyenda\n",
    "        \n",
    "        combined_data.append(df_melted)\n",
    "\n",
    "# Combinar todos los DataFrames\n",
    "df_final = pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "# 4. Crear un gráfico separado para cada relabeled\n",
    "relabeled_values = df_final['Relabeled'].unique()\n",
    "\n",
    "for relabeled in relabeled_values:\n",
    "    # Filtrar datos para este relabeled\n",
    "    relabeled_data = df_final[df_final['Relabeled'] == relabeled]\n",
    "    \n",
    "    # Crear figura\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Crear boxplot con métricas en el eje x y comparando ventanas\n",
    "    ax = sns.boxplot(\n",
    "        data=relabeled_data, \n",
    "        x='Métrica', \n",
    "        y='Valor', \n",
    "        hue='Ventana',\n",
    "        palette=['#3274A1', '#E1812C', '#3A923A']  # Colores similares al ejemplo (azul, naranja, verde)\n",
    "    )\n",
    "    \n",
    "    # Configurar el gráfico\n",
    "    plt.title(f'Distribución de métricas normalizadas para {relabeled}', fontsize=16)\n",
    "    plt.xlabel('Métrica', fontsize=14)\n",
    "    plt.ylabel('Valor normalizado', fontsize=14)\n",
    "    plt.xticks(rotation=90)  # Rotar etiquetas para mejorar legibilidad\n",
    "    \n",
    "    # Personalizar la leyenda para que coincida con el formato del ejemplo\n",
    "    plt.legend(title='Fuente')\n",
    "    \n",
    "    # Ajustar límites del eje y para que sea de 0 a 1 como en el ejemplo\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    # Añadir cuadrícula para mejor lectura\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar el gráfico (opcional)\n",
    "    # plt.savefig(f'metricas_relabeled_{relabeled}.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 5. Opcionalmente, gráfico combinado para todos los relabeled\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Crear boxplot con todas las métricas y ventanas\n",
    "sns.boxplot(\n",
    "    data=df_final, \n",
    "    x='Métrica', \n",
    "    y='Valor', \n",
    "    hue='Ventana',\n",
    "    palette=['#3274A1', '#E1812C', '#3A923A'] \n",
    ")\n",
    "\n",
    "# Configurar el gráfico\n",
    "plt.title('Distribución de métricas normalizadas en los diferentes DataFrames', fontsize=16)\n",
    "plt.xlabel('Métrica', fontsize=14)\n",
    "plt.ylabel('Valor normalizado', fontsize=14)\n",
    "plt.xticks(rotation=90)  # Rotar etiquetas para mejorar legibilidad\n",
    "plt.legend(title='Fuente')\n",
    "plt.ylim(0, 1.05)  # Límites del eje y para que sea de 0 a 1 como en el ejemplo\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Añadir cuadrícula\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowed Selection: 200 ms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los dataframes en uno solo para facilitar la visualización\n",
    "all_data = []\n",
    "relabeled_dfs = {}\n",
    "\n",
    "# Buscar todas las variables df_relabeled_X_200 en el espacio global\n",
    "for var_name in list(globals().keys()):\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.split('_')[2]  # Extraer el valor de relabeled\n",
    "        relabeled_dfs[relabeled_value] = globals()[var_name]\n",
    "        \n",
    "        # Añadir los datos al conjunto combinado\n",
    "        df_copy = globals()[var_name].copy()\n",
    "        all_data.append(df_copy)\n",
    "\n",
    "# Combinar todos los dataframes\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Identificar columnas de métricas (excluyendo columnas de metadatos)\n",
    "    metric_columns = [col for col in combined_df.columns \n",
    "                    if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "    \n",
    "    # Crear una figura con subplots para cada métrica\n",
    "    n_metrics = len(metric_columns)\n",
    "    fig, axes = plt.subplots(nrows=(n_metrics+1)//2, ncols=2, figsize=(14, 3*((n_metrics+1)//2)), \n",
    "                            constrained_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Crear boxplots para cada métrica\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        if i < len(axes):\n",
    "            # Crear boxplot usando seaborn\n",
    "            sns.boxplot(x='relabeled', y=metric, data=combined_df, ax=axes[i], palette='viridis')\n",
    "            \n",
    "            # Añadir títulos y etiquetas\n",
    "            axes[i].set_title(f'Comparación de {metric} por categoría \"relabeled\"')\n",
    "            axes[i].set_xlabel('Categoría')\n",
    "            axes[i].set_ylabel(metric)\n",
    "            \n",
    "            # Rotar etiquetas del eje x si hay muchas categorías\n",
    "            if len(combined_df['relabeled'].unique()) > 5:\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Eliminar subplots vacíos\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    # Añadir título general\n",
    "    plt.suptitle('Comparación de métricas EMG entre diferentes categorías', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Mostrar la figura\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análisis estadístico básico (opcional)\n",
    "    print(\"Stadistic for grasp:\")\n",
    "    for metric in metric_columns:\n",
    "        print(f\"\\nMétrica: {metric}\")\n",
    "        display(combined_df.groupby('relabeled')[metric].describe())\n",
    "else:\n",
    "    print(\"No se encontraron variables df_relabeled_X_200 en el espacio global.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los dataframes en uno solo para facilitar la visualización\n",
    "all_data = []\n",
    "relabeled_dfs = {}\n",
    "\n",
    "# Buscar todas las variables df_relabeled_X_200 en el espacio global\n",
    "for var_name in list(globals().keys()):\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.split('_')[2]  # Extraer el valor de relabeled\n",
    "        relabeled_dfs[relabeled_value] = globals()[var_name]\n",
    "        \n",
    "        # Añadir los datos al conjunto combinado\n",
    "        df_copy = globals()[var_name].copy()\n",
    "        all_data.append(df_copy)\n",
    "\n",
    "# Combinar todos los dataframes\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Identificar columnas de métricas (excluyendo columnas de metadatos)\n",
    "    metric_columns = [col for col in combined_df.columns \n",
    "                    if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "    \n",
    "    # Crear una figura con subplots para cada métrica\n",
    "    n_metrics = len(metric_columns)\n",
    "    fig, axes = plt.subplots(nrows=(n_metrics+1)//2, ncols=2, figsize=(14, 3*((n_metrics+1)//2)), \n",
    "                            constrained_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Crear violin plots para cada métrica\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        if i < len(axes):\n",
    "            sns.violinplot(x='relabeled', y=metric, data=combined_df, ax=axes[i],\n",
    "                            palette='viridis', inner='box')  # 'box' muestra la caja dentro del violín\n",
    "            \n",
    "            axes[i].set_title(f'Distribución de {metric} por categoría \"relabeled\"')\n",
    "            axes[i].set_xlabel('Categoría')\n",
    "            axes[i].set_ylabel(metric)\n",
    "            \n",
    "            if len(combined_df['relabeled'].unique()) > 5:\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Eliminar subplots vacíos\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.suptitle('Distribución de métricas EMG entre diferentes categorías', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análisis estadístico básico (opcional)\n",
    "    print(\"Stadistic for grasp:\")\n",
    "    for metric in metric_columns:\n",
    "        print(f\"\\nMétrica: {metric}\")\n",
    "        display(combined_df.groupby('relabeled')[metric].describe())\n",
    "else:\n",
    "    print(\"No se encontraron variables df_relabeled_X_200 en el espacio global.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = combined_df[metric_columns].corr()\n",
    "\n",
    "# Crear el heatmap de correlación\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Mapa de Correlación de Métricas')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definir las variables predictoras (métricas) y la variable objetivo ('relabeled')\n",
    "X = combined_df[metric_columns]\n",
    "y = combined_df['relabeled']\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Crear un gráfico de barras con la importancia de las características\n",
    "plt.figure(figsize=(11, 6))\n",
    "sns.barplot(x=metric_columns, y=importances, palette='viridis')\n",
    "plt.title('Importancia de Características para la Clasificación de Agarre')\n",
    "plt.xlabel('Métricas')\n",
    "plt.ylabel('Importancia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
