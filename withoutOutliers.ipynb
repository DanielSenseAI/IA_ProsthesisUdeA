{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae246ea6",
   "metadata": {},
   "source": [
    "## With outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d405da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los dataframes en uno solo para facilitar la visualizaci√≥n\n",
    "all_data = []\n",
    "relabeled_dfs = {}\n",
    "\n",
    "# Buscar todas las variables df_relabeled_X_200 en el espacio global\n",
    "for var_name in list(globals().keys()):\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.split('_')[2]  # Extraer el valor de relabeled\n",
    "        relabeled_dfs[relabeled_value] = globals()[var_name]\n",
    "        \n",
    "        # A√±adir los datos al conjunto combinado\n",
    "        df_copy = globals()[var_name].copy()\n",
    "        all_data.append(df_copy)\n",
    "\n",
    "# Combinar todos los dataframes\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    metric_columns = [col for col in combined_df.columns if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "\n",
    "# Aplicar MinMaxScaler a las columnas de m√©tricas\n",
    "    scaler = MinMaxScaler()\n",
    "    combined_df[metric_columns] = scaler.fit_transform(combined_df[metric_columns])\n",
    "    # Identificar columnas de m√©tricas (excluyendo columnas de metadatos)\n",
    "    metric_columns = [col for col in combined_df.columns \n",
    "                    if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "    \n",
    "    # Crear una figura con subplots para cada m√©trica\n",
    "    n_metrics = len(metric_columns)\n",
    "    fig, axes = plt.subplots(nrows=(n_metrics+1)//2, ncols=2, figsize=(14, 3*((n_metrics+1)//2)), \n",
    "                            constrained_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Crear boxplots para cada m√©trica\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        if i < len(axes):\n",
    "            # Crear boxplot usando seaborn\n",
    "            sns.boxplot(x='relabeled', y=metric, data=combined_df, ax=axes[i], palette='viridis')\n",
    "            \n",
    "            # A√±adir t√≠tulos y etiquetas\n",
    "            axes[i].set_title(f'Comparaci√≥n de {metric} por grasp', fontsize=14)\n",
    "            axes[i].set_xlabel('Categor√≠a')\n",
    "            axes[i].set_ylabel(metric)\n",
    "            \n",
    "            # Rotar etiquetas del eje x si hay muchas categor√≠as\n",
    "            if len(combined_df['relabeled'].unique()) > 5:\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Eliminar subplots vac√≠os\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    # A√±adir t√≠tulo general\n",
    "    plt.suptitle('Comparaci√≥n de m√©tricas EMG entre diferentes categor√≠as', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Mostrar la figura\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lisis estad√≠stico b√°sico (opcional)\n",
    "    print(\"Stadistic for grasp:\")\n",
    "    for metric in metric_columns:\n",
    "        print(f\"\\nM√©trica: {metric}\")\n",
    "        display(combined_df.groupby('relabeled')[metric].describe())\n",
    "else:\n",
    "    print(\"No se encontraron variables df_relabeled_X_200 en el espacio global.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ab73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Verifica que combined_df est√© definido correctamente\n",
    "if not all_data:\n",
    "    print(\"No se encontraron variables df_relabeled_X_200 en el espacio global.\")\n",
    "else:\n",
    "    # Crear un boxplot interactivo para cada m√©trica\n",
    "    for metric in metric_columns:\n",
    "        fig = px.box(\n",
    "            combined_df,\n",
    "            x='relabeled',\n",
    "            y=metric,\n",
    "            color='relabeled',\n",
    "            points='all',  # Muestra los puntos individuales\n",
    "            hover_data=['subject'],  # Mostrar 'subject' al pasar el cursor\n",
    "            title=f'Boxplot interactivo para {metric} por grasp'\n",
    "        )\n",
    "        fig.update_layout(\n",
    "            xaxis_title='Grasp (relabeled)',\n",
    "            yaxis_title=metric,\n",
    "            boxmode='group',\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f5bc4",
   "metadata": {},
   "source": [
    "- Fisher Score analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe86c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "import numpy as np\n",
    "\n",
    "# Combinar todos los dataframes en uno solo para facilitar la visualizaci√≥n\n",
    "all_data = []\n",
    "relabeled_dfs = {}\n",
    "\n",
    "# Buscar todas las variables df_relabeled_X_200 en el espacio global\n",
    "for var_name in list(globals().keys()):\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.split('_')[2]  # Extraer el valor de relabeled\n",
    "        relabeled_dfs[relabeled_value] = globals()[var_name]\n",
    "        \n",
    "        # A√±adir los datos al conjunto combinado\n",
    "        df_copy = globals()[var_name].copy()\n",
    "        all_data.append(df_copy)\n",
    "\n",
    "# Combinar todos los dataframes\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    metric_columns = [col for col in combined_df.columns if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "\n",
    "# Aplicar MinMaxScaler a las columnas de m√©tricas\n",
    "    scaler = MinMaxScaler()\n",
    "    combined_df[metric_columns] = scaler.fit_transform(combined_df[metric_columns])\n",
    "    # Identificar columnas de m√©tricas (excluyendo columnas de metadatos)\n",
    "    metric_columns = [col for col in combined_df.columns \n",
    "                    if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "    \n",
    "    # Crear una figura con subplots para cada m√©trica\n",
    "    n_metrics = len(metric_columns)\n",
    "    fig, axes = plt.subplots(nrows=(n_metrics+1)//2, ncols=2, figsize=(14, 3*((n_metrics+1)//2)), \n",
    "                            constrained_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Crear boxplots para cada m√©trica\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        if i < len(axes):\n",
    "            # Crear boxplot usando seaborn\n",
    "            sns.boxplot(x='relabeled', y=metric, data=combined_df, ax=axes[i], palette='viridis')\n",
    "            \n",
    "            # A√±adir t√≠tulos y etiquetas\n",
    "            axes[i].set_title(f'Comparaci√≥n de {metric} por grasp', fontsize=14)\n",
    "            axes[i].set_xlabel('Categor√≠a')\n",
    "            axes[i].set_ylabel(metric)\n",
    "            \n",
    "            # Rotar etiquetas del eje x si hay muchas categor√≠as\n",
    "            if len(combined_df['relabeled'].unique()) > 5:\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Eliminar subplots vac√≠os\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    # A√±adir t√≠tulo general\n",
    "    plt.suptitle('Comparaci√≥n de m√©tricas EMG entre diferentes categor√≠as', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Mostrar la figura\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # üß™ Calcular ANOVA y Fisher Score\n",
    "    anova_results = {}\n",
    "    fisher_scores = {}\n",
    "\n",
    "    categories = combined_df['relabeled'].unique()\n",
    "\n",
    "    for metric in metric_columns:\n",
    "        # Agrupar por categor√≠a para ANOVA\n",
    "        groups = [combined_df[combined_df['relabeled'] == cat][metric].dropna().values for cat in categories]\n",
    "        \n",
    "        # ANOVA\n",
    "        try:\n",
    "            f_stat, p_val = f_oneway(*groups)\n",
    "        except:\n",
    "            f_stat, p_val = np.nan, np.nan\n",
    "        \n",
    "        anova_results[metric] = {'F-statistic': f_stat, 'p-value': p_val}\n",
    "        \n",
    "        # Fisher Score\n",
    "        overall_mean = combined_df[metric].mean()\n",
    "        num = 0\n",
    "        den = 0\n",
    "        \n",
    "        for cat in categories:\n",
    "            class_data = combined_df[combined_df['relabeled'] == cat][metric]\n",
    "            ni = len(class_data)\n",
    "            class_mean = class_data.mean()\n",
    "            class_var = class_data.var()\n",
    "            \n",
    "            num += ni * (class_mean - overall_mean) ** 2\n",
    "            den += ni * class_var\n",
    "        \n",
    "        fisher = num / den if den != 0 else 0\n",
    "        fisher_scores[metric] = fisher\n",
    "\n",
    "    # üìã Mostrar resultados ordenados por Fisher Score\n",
    "    results_df = pd.DataFrame.from_dict(anova_results, orient='index')\n",
    "    results_df['Fisher Score'] = pd.Series(fisher_scores)\n",
    "    results_df_sorted = results_df.sort_values(by='Fisher Score', ascending=False)\n",
    "\n",
    "    print(\"\\nüìä Resultados ANOVA y Fisher Score ordenados:\\n\")\n",
    "    display(results_df_sorted)\n",
    "\n",
    "else:\n",
    "    print(\"No se encontraron variables df_relabeled_X_200 en el espacio global.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96034e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_df_sorted[results_df_sorted['F-statistic'] > 5]\n",
    "df = df[df['p-value'] < 0.05]\n",
    "df = df[df['Fisher Score'] > 0.5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_subject_stability = {}\n",
    "\n",
    "for metric in metric_columns:\n",
    "    stability_per_class = {}\n",
    "    for label in combined_df['relabeled'].unique():\n",
    "        subset = combined_df[combined_df['relabeled'] == label]\n",
    "        # Media por sujeto\n",
    "        subject_means = subset.groupby('subject')[metric].mean()\n",
    "        # STD entre sujetos\n",
    "        std_across_subjects = subject_means.std()\n",
    "        stability_per_class[label] = std_across_subjects\n",
    "    inter_subject_stability[metric] = stability_per_class\n",
    "\n",
    "# Convertir a DataFrame para visualizar\n",
    "stability_df = pd.DataFrame(inter_subject_stability).T\n",
    "display(stability_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6757cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Excluir m√©tricas espec√≠ficas\n",
    "excluded_features = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "filtered_metric_columns = [col for col in metric_columns if col not in excluded_features]\n",
    "\n",
    "# Filtrar columnas relevantes en el DataFrame (sin eliminar at√≠picos)\n",
    "filtered_df = combined_df[['relabeled'] + filtered_metric_columns].copy()\n",
    "\n",
    "# 1. Calcular la mediana por grupo (relabeled) para cada m√©trica\n",
    "median_df = filtered_df.groupby('relabeled')[filtered_metric_columns].median()\n",
    "\n",
    "# 2. Calcular la varianza entre las medianas para cada m√©trica\n",
    "median_variance = median_df.var()\n",
    "\n",
    "# 3. Convertir a DataFrame para visualizaci√≥n\n",
    "median_variance_df = median_variance.reset_index()\n",
    "median_variance_df.columns = ['m√©trica', 'varianza_entre_medianas']\n",
    "\n",
    "# 4. Normalizar las varianzas (Min-Max)\n",
    "scaler = MinMaxScaler()\n",
    "median_variance_df['varianza_normalizada'] = scaler.fit_transform(\n",
    "    median_variance_df[['varianza_entre_medianas']]\n",
    ")\n",
    "\n",
    "# 5. Mostrar la tabla ordenada (opcional)\n",
    "print(\"Varianza entre medianas y su normalizaci√≥n:\")\n",
    "display(median_variance_df.sort_values(by='varianza_normalizada', ascending=False))\n",
    "\n",
    "# 6. Visualizaci√≥n con barplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='varianza_normalizada',\n",
    "    y='m√©trica',\n",
    "    data=median_variance_df.sort_values(by='varianza_normalizada', ascending=True),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Varianza normalizada entre medianas por m√©trica (con at√≠picos)', fontsize=14)\n",
    "plt.xlabel('Varianza normalizada')\n",
    "plt.ylabel('M√©trica')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7875cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = combined_df[metric_columns].corr()\n",
    "\n",
    "# Crear el heatmap de correlaci√≥n\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Mapa de Correlaci√≥n de M√©tricas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la matriz en forma de pares con sus correlaciones\n",
    "corr_pairs = corr_matrix.unstack()\n",
    "\n",
    "# Eliminar duplicados y la diagonal (correlaci√≥n de una variable consigo misma)\n",
    "# corr_pairs = corr_pairs[corr_pairs.index.get_level_values(0) != corr_pairs.index.get_level_values(1)]\n",
    "# corr_pairs = corr_pairs.drop_duplicates()\n",
    "\n",
    "# Filtrar pares con alta correlaci√≥n\n",
    "high_corr = corr_pairs[abs(corr_pairs) > 0.89].sort_values(ascending=False)\n",
    "\n",
    "# Crear el DataFrame de correlaciones altas\n",
    "high_corr_df = high_corr.reset_index()\n",
    "high_corr_df.columns = ['M√©trica 1', 'M√©trica 2', 'Correlaci√≥n']\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "display(high_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca30934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 1. Eliminar m√©tricas espec√≠ficas ----------\n",
    "excluded_metrics = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "filtered_metrics = [col for col in metric_columns if col not in excluded_metrics]\n",
    "\n",
    "# ---------- 2. Entrenamiento del modelo con todos los datos (sin eliminar at√≠picos) ----------\n",
    "X = combined_df[filtered_metrics]\n",
    "y = combined_df['relabeled']\n",
    "\n",
    "# Crear y entrenar el modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Obtener importancias de caracter√≠sticas\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# ---------- 3. Visualizaci√≥n ----------\n",
    "plt.figure(figsize=(11, 6))\n",
    "sns.barplot(x=filtered_metrics, y=importances, palette='viridis')\n",
    "plt.title('Importancia de Caracter√≠sticas para la Clasificaci√≥n de Agarre (con at√≠picos y sin ZC/Kurtosis)')\n",
    "plt.xlabel('M√©tricas')\n",
    "plt.ylabel('Importancia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16681a",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23326be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 1. Definir las m√©tricas a usar ----------\n",
    "# Aseg√∫rate de que 'filtered_metrics' est√© definido, por ejemplo:\n",
    "# filtered_metrics = [col for col in combined_df.columns if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "\n",
    "# ---------- 2. Definir variables X e y ----------\n",
    "X = combined_df[filtered_metrics]\n",
    "y = combined_df['relabeled']\n",
    "\n",
    "# ---------- 3. Entrenar modelo ----------\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# ---------- 4. Importancia de caracter√≠sticas ----------\n",
    "importances = rf_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'M√©trica': filtered_metrics,\n",
    "    'Importancia': importances\n",
    "}).sort_values(by='Importancia', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# ---------- 5. Mostrar resultados ----------\n",
    "print(\"Importancia de las caracter√≠sticas sin eliminar at√≠picos:\")\n",
    "display(importance_df)\n",
    "\n",
    "# ---------- 6. Visualizaci√≥n ----------\n",
    "plt.figure(figsize=(11, 6))\n",
    "sns.barplot(x='M√©trica', y='Importancia', data=importance_df, palette='viridis')\n",
    "plt.title('Importancia de Caracter√≠sticas para la Clasificaci√≥n de Agarre (con at√≠picos)')\n",
    "plt.xlabel('M√©tricas')\n",
    "plt.ylabel('Importancia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e944dc",
   "metadata": {},
   "source": [
    "- Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- 1. Definir las m√©tricas a usar ----------\n",
    "# Aseg√∫rate de tener esta lista definida:\n",
    "# filtered_metrics = [col for col in combined_df.columns if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "\n",
    "# ---------- 2. Definir variables X e y ----------\n",
    "X = combined_df[filtered_metrics]\n",
    "y = combined_df['relabeled']\n",
    "\n",
    "# ---------- 3. Entrenar modelo ----------\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "# ---------- 4. Importancia de caracter√≠sticas ----------\n",
    "importances = tree_model.feature_importances_\n",
    "importances_percentage = 100 * importances / importances.sum()\n",
    "\n",
    "importances_df = pd.DataFrame({\n",
    "    'M√©trica': filtered_metrics,\n",
    "    'Importancia (%)': importances_percentage\n",
    "}).sort_values(by='Importancia (%)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nüìä Importancia de las caracter√≠sticas (%):\")\n",
    "display(importances_df)\n",
    "\n",
    "# Verificaci√≥n: la suma deber√≠a ser aproximadamente 100%\n",
    "print(f\"\\n‚úÖ Suma total de importancias: {importances_percentage.sum():.2f}%\")\n",
    "\n",
    "# ---------- 5. Visualizaci√≥n ----------\n",
    "plt.figure(figsize=(11, 6))\n",
    "sns.barplot(data=importances_df, x='M√©trica', y='Importancia (%)', palette='viridis')\n",
    "plt.title('Importancia de Caracter√≠sticas para la Clasificaci√≥n de Agarre (√Årbol de Decisi√≥n con at√≠picos)')\n",
    "plt.xlabel('M√©tricas')\n",
    "plt.ylabel('Importancia (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965c35e",
   "metadata": {},
   "source": [
    "- Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad466dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Funci√≥n para calcular CV ----------\n",
    "def calculate_cv(metrics_df):\n",
    "    cv_values = {}\n",
    "    for metric in metrics_df.columns:\n",
    "        mean_value = metrics_df[metric].mean()\n",
    "        std_value = metrics_df[metric].std()\n",
    "        if mean_value != 0:\n",
    "            cv_values[metric] = (std_value / mean_value) * 100\n",
    "        else:\n",
    "            cv_values[metric] = np.nan\n",
    "    return cv_values\n",
    "\n",
    "# ---------- Par√°metros ----------\n",
    "fm = 2000\n",
    "window_length = 400\n",
    "overlap = 0\n",
    "target_channel = \"Channel 8\"\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# ---------- Iterar sobre los archivos ----------\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                test_df, grasps = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "\n",
    "                if target_channel not in test_df.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "\n",
    "                emg_columns = [target_channel]\n",
    "                envelope_df = src.get_envelope_lowpass(\n",
    "                    test_df[emg_columns], fm=2000, cutoff_freq=0.6, envelope_type=1\n",
    "                )\n",
    "\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "\n",
    "                window_count = 0\n",
    "\n",
    "                for grasp in grasps:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp} in file {file_name}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "\n",
    "                        if grasp_df.empty:\n",
    "                            print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "\n",
    "                        for i, ventana in enumerate(ventanas):\n",
    "                            if len(ventana) == window_length:\n",
    "                                signal = ventana[target_channel].values\n",
    "                                metrics = calculate_emg_metrics_means(signal)\n",
    "\n",
    "                                metrics_with_meta = {\n",
    "                                    \"subject\": folder,\n",
    "                                    \"relabeled\": grasp_df['relabeled'].iloc[0],\n",
    "                                    \"stimulus\": grasp,\n",
    "                                    \"channel\": target_channel,\n",
    "                                    \"window_id\": f\"{file_name}_{grasp}_{i}\",\n",
    "                                    \"file_name\": file_name,\n",
    "                                    \"window_number\": window_count,\n",
    "                                    **metrics\n",
    "                                }\n",
    "\n",
    "                                all_metrics.append(metrics_with_meta)\n",
    "                                window_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"Procesadas {window_count} ventanas para el archivo {file_name}\")\n",
    "\n",
    "# ---------- Crear DataFrame general ----------\n",
    "metrics_df_200 = pd.DataFrame(all_metrics)\n",
    "\n",
    "# ---------- Reordenar columnas ----------\n",
    "meta_cols = [\"subject\", \"relabeled\", \"stimulus\", \"channel\", \"window_id\", \"file_name\", \"window_number\"]\n",
    "metric_cols = [col for col in metrics_df_200.columns if col not in meta_cols]\n",
    "column_order = meta_cols + sorted(metric_cols)\n",
    "metrics_df_200 = metrics_df_200[column_order]\n",
    "\n",
    "# ---------- Filtrar m√©tricas no deseadas ----------\n",
    "excluded_metrics = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "filtered_metrics = [col for col in metric_cols if col not in excluded_metrics]\n",
    "\n",
    "# ---------- Calcular CV directamente ----------\n",
    "cv_values = calculate_cv(metrics_df_200[filtered_metrics])\n",
    "cv_df = pd.DataFrame.from_dict(cv_values, orient='index', columns=['Coeficiente de Variaci√≥n'])\n",
    "cv_df = cv_df.sort_values(by='Coeficiente de Variaci√≥n', ascending=False)\n",
    "\n",
    "# ---------- Mostrar resultados ----------\n",
    "print(\"\\nüìä Coeficiente de variaci√≥n de las m√©tricas (con at√≠picos, sin ZC/Kurtosis):\")\n",
    "display(cv_df)\n",
    "\n",
    "print(\"\\nüìà Resumen de m√©tricas por tipo de movimiento (completo):\")\n",
    "grouped_df = metrics_df_200.drop(columns=['channel'], errors='ignore')\n",
    "summary_by_subject_movement_200 = grouped_df.select_dtypes(include=['number']).groupby(['relabeled']).mean()\n",
    "display(summary_by_subject_movement_200)\n",
    "\n",
    "print(f\"\\n‚úÖ Total de ventanas procesadas: {len(metrics_df_200)}\")\n",
    "print(f\"üìå Distribuci√≥n por sujeto:\\n{metrics_df_200['subject'].value_counts()}\")\n",
    "print(f\"üìå Distribuci√≥n por movimiento:\\n{metrics_df_200['relabeled'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1def170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n Min-Max del Coeficiente de Variaci√≥n\n",
    "\n",
    "cv_df['Coeficiente de Variaci√≥n Normalizado'] = (cv_df['Coeficiente de Variaci√≥n'] - cv_df['Coeficiente de Variaci√≥n'].min()) / (cv_df['Coeficiente de Variaci√≥n'].max() - cv_df['Coeficiente de Variaci√≥n'].min())\n",
    "# Mostrar los valores normalizados\n",
    "\n",
    "display(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3493d88",
   "metadata": {},
   "source": [
    "- PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd80ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_by_movement_200.columns.tolist())\n",
    "summary_by_subject_movement_200 = summary_by_subject_movement_200.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import plotly.express as px\n",
    "\n",
    "# 1. Seleccionar m√©tricas (excluir ZC, ZC_STD, Kurt, Kurt_STD)\n",
    "excluded = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "features = [c for c in summary_by_relabeled_200.columns \n",
    "            if c not in ['subject','relabeled','stimulus'] \n",
    "            and not any(exc.upper() in c.upper() for exc in excluded)]\n",
    "\n",
    "X = summary_by_relabeled_200[features].values\n",
    "y = summary_by_relabeled_200['relabeled'].values\n",
    "\n",
    "# 2. Escalado (PCA es sensible a la escala)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. PCA\n",
    "n_components = min(len(features), 5)  # por ejemplo, primeros 10 PCs\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 4. Carga de cada feature en cada componente\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=features,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components)]\n",
    ")\n",
    "\n",
    "# Mostrar los features m√°s influyentes en PC1 y PC2\n",
    "print(\"Top 5 features por carga absoluta en PC1:\")\n",
    "display(loadings['PC1'].abs().sort_values(ascending=False).head(5))\n",
    "\n",
    "print(\"Top 5 features por carga absoluta en PC2:\")\n",
    "display(loadings['PC2'].abs().sort_values(ascending=False).head(5))\n",
    "\n",
    "# 5. Visualizar los dos primeros PCs coloreados por relabeled\n",
    "pca_df = pd.DataFrame(X_pca[:, :2], columns=['PC1','PC2'])\n",
    "pca_df['relabeled'] = y\n",
    "pca_df['subject']   = summary_by_relabeled_200['subject'].values\n",
    "\n",
    "fig = px.scatter(\n",
    "    pca_df, x='PC1', y='PC2',\n",
    "    color='relabeled', symbol='subject',\n",
    "    title='Proyecci√≥n PCA (PC1 vs PC2)',\n",
    "    hover_data=['subject','relabeled']\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 6. Entrenar un clasificador sencillo en el espacio PCA\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "scores = cross_val_score(clf, X_pca, y, cv=5, scoring='accuracy')\n",
    "print(f\"\\nAccuracy 5-fold CV en espacio PCA ({n_components} componentes): {scores.mean():.3f} ¬± {scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2665655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_scores = cross_val_score(rf_model, X_pca, y, cv=5)\n",
    "\n",
    "print(f\"üéØ Accuracy 5-fold CV con Random Forest: {rf_scores.mean():.3f} ¬± {rf_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Crear el modelo KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Evaluaci√≥n utilizando validaci√≥n cruzada\n",
    "knn_scores = cross_val_score(knn_model, X_pca, y, cv=5)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"üéØ Accuracy 5-fold CV con KNN: {knn_scores.mean():.3f} ¬± {knn_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbde6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_scores = cross_val_score(svm_model, X_pca, y, cv=5)\n",
    "\n",
    "print(f\"üéØ Accuracy 5-fold CV con SVM (RBF kernel): {svm_scores.mean():.3f} ¬± {svm_scores.std():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Guardar resultados de validaci√≥n cruzada\n",
    "models = {\n",
    "    'SVM (RBF kernel)': svm_scores,\n",
    "    'Random Forest': rf_scores,\n",
    "    'KNN': knn_scores\n",
    "}\n",
    "\n",
    "# Crear gr√°fico de las puntuaciones\n",
    "plt.figure(figsize=(10,6))\n",
    "for model_name, scores in models.items():\n",
    "    plt.plot(range(1, 6), scores, label=f'{model_name}')\n",
    "\n",
    "plt.title('Puntuaciones de validaci√≥n cruzada')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c32cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea937e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn plotly pycaret xgboost\n",
    "from pycaret.classification import *\n",
    "\n",
    "# Crear el entorno de pycaret\n",
    "data = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "data['relabeled'] = y\n",
    "\n",
    "# Iniciar la configuraci√≥n de PyCaret\n",
    "clf = setup(data=data, target='relabeled', session_id=42, fold=5, \n",
    "            normalize=True, feature_selection=True, pca=True)\n",
    "\n",
    "# Comparar modelos, incluyendo KNN\n",
    "best_model = compare_models()\n",
    "\n",
    "# Entrenar el mejor modelo\n",
    "final_model = finalize_model(best_model)\n",
    "\n",
    "# Evaluar el modelo final\n",
    "evaluate_model(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c643d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Crear modelo KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Evaluaci√≥n utilizando validaci√≥n cruzada\n",
    "knn_scores = cross_val_score(knn_model, X_pca, y, cv=5)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"üéØ Accuracy 5-fold CV con KNN: {knn_scores.mean():.3f} ¬± {knn_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9801c48",
   "metadata": {},
   "source": [
    "## Without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Combinar todos los dataframes en uno solo para facilitar la visualizaci√≥n\n",
    "all_data = []\n",
    "relabeled_dfs = {}\n",
    "\n",
    "# Buscar todas las variables df_relabeled_X_200 en el espacio global\n",
    "for var_name in list(globals().keys()):\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.split('_')[2]\n",
    "        relabeled_dfs[relabeled_value] = globals()[var_name]\n",
    "        \n",
    "        # A√±adir los datos al conjunto combinado\n",
    "        df_copy = globals()[var_name].copy()\n",
    "        all_data.append(df_copy)\n",
    "\n",
    "# Combinar todos los dataframes\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Identificar columnas de m√©tricas excluyendo ZC, ZC_STD, KURT, KURT_STD\n",
    "    metric_columns = [\n",
    "        col for col in combined_df.columns \n",
    "        if col not in ['subject', 'relabeled', 'stimulus'] \n",
    "        and not any(exclude in col.upper() for exclude in ['ZC', 'Kurt'])\n",
    "    ]\n",
    "    \n",
    "    # Eliminar outliers usando el rango intercuart√≠lico (IQR)\n",
    "    for col in metric_columns:\n",
    "        Q1 = combined_df[col].quantile(0.25)\n",
    "        Q3 = combined_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        combined_df = combined_df[(combined_df[col] >= lower_bound) & (combined_df[col] <= upper_bound)]\n",
    "    \n",
    "    # Aplicar MinMaxScaler a las columnas de m√©tricas\n",
    "    scaler = MinMaxScaler()\n",
    "    combined_df[metric_columns] = scaler.fit_transform(combined_df[metric_columns])\n",
    "    \n",
    "    # Crear una figura con subplots para cada m√©trica\n",
    "    n_metrics = len(metric_columns)\n",
    "    fig, axes = plt.subplots(nrows=(n_metrics+1)//2, ncols=2, figsize=(14, 3*((n_metrics+1)//2)), \n",
    "                             constrained_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Crear boxplots para cada m√©trica\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(x='relabeled', y=metric, data=combined_df, ax=axes[i], palette='viridis')\n",
    "            axes[i].set_title(f'Comparaci√≥n de {metric} por grasp', fontsize=14)\n",
    "            axes[i].set_xlabel('Categor√≠a')\n",
    "            axes[i].set_ylabel(metric)\n",
    "            if len(combined_df['relabeled'].unique()) > 5:\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Eliminar subplots vac√≠os\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.suptitle('Comparaci√≥n de m√©tricas EMG entre diferentes categor√≠as', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ANOVA y Fisher Score\n",
    "    anova_results = {}\n",
    "    fisher_scores = {}\n",
    "\n",
    "    categories = combined_df['relabeled'].unique()\n",
    "\n",
    "    for metric in metric_columns:\n",
    "        groups = [combined_df[combined_df['relabeled'] == cat][metric].dropna().values for cat in categories]\n",
    "        try:\n",
    "            f_stat, p_val = f_oneway(*groups)\n",
    "        except:\n",
    "            f_stat, p_val = np.nan, np.nan\n",
    "\n",
    "        anova_results[metric] = {'F-statistic': f_stat, 'p-value': p_val}\n",
    "\n",
    "        overall_mean = combined_df[metric].mean()\n",
    "        num, den = 0, 0\n",
    "        for cat in categories:\n",
    "            class_data = combined_df[combined_df['relabeled'] == cat][metric]\n",
    "            ni = len(class_data)\n",
    "            class_mean = class_data.mean()\n",
    "            class_var = class_data.var()\n",
    "            num += ni * (class_mean - overall_mean) ** 2\n",
    "            den += ni * class_var\n",
    "        fisher = num / den if den != 0 else 0\n",
    "        fisher_scores[metric] = fisher\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(anova_results, orient='index')\n",
    "    results_df['Fisher Score'] = pd.Series(fisher_scores)\n",
    "    results_df_sorted = results_df.sort_values(by='Fisher Score', ascending=False)\n",
    "\n",
    "    print(\"\\nüìä Resultados ANOVA y Fisher Score ordenados:\\n\")\n",
    "    display(results_df_sorted)\n",
    "\n",
    "else:\n",
    "    print(\"No se encontraron variables df_relabeled_X_200 en el espacio global.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24abb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.express as px\n",
    "\n",
    "# Combinar todos los dataframes en uno solo para facilitar la visualizaci√≥n\n",
    "all_data = []\n",
    "relabeled_dfs = {}\n",
    "\n",
    "# Buscar todas las variables df_relabeled_X_200 en el espacio global\n",
    "for var_name in list(globals().keys()):\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.split('_')[2]\n",
    "        relabeled_dfs[relabeled_value] = globals()[var_name]\n",
    "\n",
    "        # A√±adir los datos al conjunto combinado\n",
    "        df_copy = globals()[var_name].copy()\n",
    "        all_data.append(df_copy)\n",
    "\n",
    "# Combinar todos los dataframes\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Identificar columnas de m√©tricas excluyendo ZC, ZC_STD, KURT, KURT_STD\n",
    "    metric_columns = [\n",
    "        col for col in combined_df.columns \n",
    "        if col not in ['subject', 'relabeled', 'stimulus'] \n",
    "        and not any(exclude in col.upper() for exclude in ['ZC', 'KURT'])\n",
    "    ]\n",
    "\n",
    "    # Guardar copia original antes de eliminar outliers\n",
    "    original_df = combined_df.copy()\n",
    "\n",
    "    # Eliminar outliers usando el rango intercuart√≠lico (IQR)\n",
    "    for col in metric_columns:\n",
    "        Q1 = combined_df[col].quantile(0.25)\n",
    "        Q3 = combined_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        combined_df = combined_df[(combined_df[col] >= lower_bound) & (combined_df[col] <= upper_bound)]\n",
    "\n",
    "    # Obtener outliers eliminados\n",
    "    outliers_removed = pd.concat([original_df, combined_df]).drop_duplicates(keep=False)\n",
    "\n",
    "    # Normalizar datos\n",
    "    scaler = MinMaxScaler()\n",
    "    combined_df[metric_columns] = scaler.fit_transform(combined_df[metric_columns])\n",
    "\n",
    "    # Mostrar algunos outliers eliminados con subject\n",
    "    print(f\"\\nüìå Total de datos at√≠picos eliminados: {len(outliers_removed)}\")\n",
    "    display(outliers_removed[['subject', 'relabeled'] + metric_columns].head())\n",
    "\n",
    "    # Crear boxplots interactivos con Plotly\n",
    "    for metric in metric_columns:\n",
    "        fig = px.box(\n",
    "            combined_df,\n",
    "            x=\"relabeled\",\n",
    "            y=metric,\n",
    "            points=\"all\",  # Mostrar todos los puntos\n",
    "            title=f\"Boxplot Interactivo de {metric} por Categor√≠a\",\n",
    "            labels={\"relabeled\": \"Categor√≠a\", metric: metric},\n",
    "            color=\"relabeled\",\n",
    "            hover_data=['subject']  # Mostrar subject en tooltip\n",
    "        )\n",
    "        fig.update_layout(showlegend=False)\n",
    "        fig.show()\n",
    "\n",
    "    # ANOVA y Fisher Score\n",
    "    anova_results = {}\n",
    "    fisher_scores = {}\n",
    "    categories = combined_df['relabeled'].unique()\n",
    "\n",
    "    for metric in metric_columns:\n",
    "        groups = [combined_df[combined_df['relabeled'] == cat][metric].dropna().values for cat in categories]\n",
    "        try:\n",
    "            f_stat, p_val = f_oneway(*groups)\n",
    "        except:\n",
    "            f_stat, p_val = np.nan, np.nan\n",
    "\n",
    "        anova_results[metric] = {'F-statistic': f_stat, 'p-value': p_val}\n",
    "\n",
    "        overall_mean = combined_df[metric].mean()\n",
    "        num, den = 0, 0\n",
    "        for cat in categories:\n",
    "            class_data = combined_df[combined_df['relabeled'] == cat][metric]\n",
    "            ni = len(class_data)\n",
    "            class_mean = class_data.mean()\n",
    "            class_var = class_data.var()\n",
    "            num += ni * (class_mean - overall_mean) ** 2\n",
    "            den += ni * class_var\n",
    "        fisher = num / den if den != 0 else 0\n",
    "        fisher_scores[metric] = fisher\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(anova_results, orient='index')\n",
    "    results_df['Fisher Score'] = pd.Series(fisher_scores)\n",
    "    results_df_sorted = results_df.sort_values(by='Fisher Score', ascending=False)\n",
    "\n",
    "    print(\"\\nüìä Resultados ANOVA y Fisher Score ordenados:\\n\")\n",
    "    display(results_df_sorted)\n",
    "\n",
    "else:\n",
    "    print(\"No se encontraron variables df_relabeled_X_200 en el espacio global.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Combinar todos los dataframes\n",
    "all_data = []\n",
    "relabeled_dfs = {}\n",
    "\n",
    "for var_name in list(globals().keys()):\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.split('_')[2]\n",
    "        relabeled_dfs[relabeled_value] = globals()[var_name]\n",
    "        df_copy = globals()[var_name].copy()\n",
    "        all_data.append(df_copy)\n",
    "\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Eliminar columnas espec√≠ficas si existen\n",
    "    drop_cols = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "    combined_df.drop(columns=[col for col in drop_cols if col in combined_df.columns], inplace=True)\n",
    "\n",
    "    metric_columns = [col for col in combined_df.columns if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "\n",
    "    # Eliminar outliers por IQR\n",
    "    for col in metric_columns:\n",
    "        Q1 = combined_df[col].quantile(0.25)\n",
    "        Q3 = combined_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        combined_df = combined_df[(combined_df[col] >= lower_bound) & (combined_df[col] <= upper_bound)]\n",
    "\n",
    "    # Escalamiento\n",
    "    scaler = MinMaxScaler()\n",
    "    combined_df[metric_columns] = scaler.fit_transform(combined_df[metric_columns])\n",
    "\n",
    "    # Gr√°ficas\n",
    "    n_metrics = len(metric_columns)\n",
    "    fig, axes = plt.subplots(nrows=(n_metrics+1)//2, ncols=2, figsize=(14, 3*((n_metrics+1)//2)), constrained_layout=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        if i < len(axes):\n",
    "            sns.violinplot(x='relabeled', y=metric, data=combined_df, ax=axes[i], palette='viridis', inner='box')\n",
    "            axes[i].set_title(f'Comparaci√≥n de {metric} por grasp', fontsize=14)\n",
    "            axes[i].set_xlabel('Categor√≠a')\n",
    "            axes[i].set_ylabel(metric)\n",
    "            if len(combined_df['relabeled'].unique()) > 5:\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.suptitle('Comparaci√≥n de m√©tricas EMG entre diferentes categor√≠as', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Estad√≠sticas por grasp\n",
    "    print(\"Stadistic for grasp:\")\n",
    "    for metric in metric_columns:\n",
    "        print(f\"\\nM√©trica: {metric}\")\n",
    "        display(combined_df.groupby('relabeled')[metric].describe())\n",
    "else:\n",
    "    print(\"No se encontraron variables df_relabeled_X_200 en el espacio global.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e01fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_subject_stability = {}\n",
    "\n",
    "for metric in metric_columns:\n",
    "    stability_per_class = {}\n",
    "    for label in combined_df['relabeled'].unique():\n",
    "        subset = combined_df[combined_df['relabeled'] == label]\n",
    "        # Media por sujeto\n",
    "        subject_means = subset.groupby('subject')[metric].mean()\n",
    "        # STD entre sujetos\n",
    "        std_across_subjects = subject_means.std()\n",
    "        stability_per_class[label] = std_across_subjects\n",
    "    inter_subject_stability[metric] = stability_per_class\n",
    "\n",
    "# Convertir a DataFrame para visualizar\n",
    "stability_df = pd.DataFrame(inter_subject_stability).T\n",
    "display(stability_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b10aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paso 1: Preparar los datos (igual que antes)\n",
    "all_data = []\n",
    "relabeled_dfs = {}\n",
    "\n",
    "for var_name in list(globals().keys()):\n",
    "    if var_name.startswith('df_relabeled_') and var_name.endswith('_200'):\n",
    "        relabeled_value = var_name.split('_')[2]\n",
    "        relabeled_dfs[relabeled_value] = globals()[var_name]\n",
    "        df_copy = globals()[var_name].copy()\n",
    "        all_data.append(df_copy)\n",
    "\n",
    "if not all_data:\n",
    "    print(\"No se encontraron dataframes.\")\n",
    "else:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    metric_columns = [col for col in combined_df.columns if col not in ['subject', 'relabeled', 'stimulus']]\n",
    "\n",
    "# Aplicar MinMaxScaler a las columnas de m√©tricas\n",
    "    scaler = MinMaxScaler()\n",
    "    combined_df[metric_columns] = scaler.fit_transform(combined_df[metric_columns])\n",
    "\n",
    "    anova_results = {}\n",
    "    fisher_scores = {}\n",
    "    inter_subject_stability = {}\n",
    "\n",
    "    categories = combined_df['relabeled'].unique()\n",
    "\n",
    "    for metric in metric_columns:\n",
    "        # --- ANOVA ---\n",
    "        groups = [combined_df[combined_df['relabeled'] == cat][metric].dropna().values for cat in categories]\n",
    "        try:\n",
    "            f_stat, p_val = f_oneway(*groups)\n",
    "        except:\n",
    "            f_stat, p_val = np.nan, np.nan\n",
    "        anova_results[metric] = {'F-statistic': f_stat, 'p-value': p_val}\n",
    "\n",
    "        # --- Fisher Score ---\n",
    "        overall_mean = combined_df[metric].mean()\n",
    "        num = 0\n",
    "        den = 0\n",
    "        for cat in categories:\n",
    "            class_data = combined_df[combined_df['relabeled'] == cat][metric]\n",
    "            ni = len(class_data)\n",
    "            class_mean = class_data.mean()\n",
    "            class_var = class_data.var()\n",
    "            num += ni * (class_mean - overall_mean) ** 2\n",
    "            den += ni * class_var\n",
    "        fisher_scores[metric] = num / den if den != 0 else 0\n",
    "\n",
    "        # --- Estabilidad Inter-Sujeto ---\n",
    "        stability_per_class = {}\n",
    "        for label in categories:\n",
    "            subset = combined_df[combined_df['relabeled'] == label]\n",
    "            subject_means = subset.groupby('subject')[metric].mean()\n",
    "            std_across_subjects = subject_means.std()\n",
    "            stability_per_class[label] = std_across_subjects\n",
    "        inter_subject_stability[metric] = stability_per_class\n",
    "\n",
    "    # Paso 2: Armar el DataFrame resumen\n",
    "    results_df = pd.DataFrame.from_dict(anova_results, orient='index')\n",
    "    results_df['Fisher Score'] = pd.Series(fisher_scores)\n",
    "    stability_df = pd.DataFrame(inter_subject_stability).T\n",
    "    results_df['Inter-Subject STD (mean)'] = stability_df.mean(axis=1)\n",
    "\n",
    "    # Paso 3: Ordenar y mostrar\n",
    "    results_df_sorted = results_df.sort_values(by='Fisher Score', ascending=False)\n",
    "\n",
    "    # Paso 4: Interpretaci√≥n adicional (opcional)\n",
    "    def interpretar_fila(row):\n",
    "        if row['p-value'] < 0.05 and row['Fisher Score'] > 1.5 and row['Inter-Subject STD (mean)'] < 0.5:\n",
    "            return 'Excelente'\n",
    "        elif row['p-value'] < 0.05 and row['Fisher Score'] > 1.0:\n",
    "            return 'Buena'\n",
    "        elif row['Fisher Score'] < 0.5 or row['p-value'] > 0.1:\n",
    "            return 'Pobre'\n",
    "        else:\n",
    "            return 'Moderada'\n",
    "\n",
    "    results_df_sorted['Interpretaci√≥n'] = results_df_sorted.apply(interpretar_fila, axis=1)\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(\"\\nTabla resumen con Fisher, ANOVA y Estabilidad inter-sujeto:\\n\")\n",
    "    display(results_df_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Excluir m√©tricas espec√≠ficas\n",
    "excluded_features = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "filtered_metric_columns = [col for col in metric_columns if col not in excluded_features]\n",
    "\n",
    "# Filtrar columnas en el DataFrame\n",
    "filtered_df = combined_df[['relabeled'] + filtered_metric_columns].copy()\n",
    "\n",
    "# Eliminar at√≠picos usando el m√©todo del IQR para cada m√©trica\n",
    "for col in filtered_metric_columns:\n",
    "    Q1 = filtered_df[col].quantile(0.25)\n",
    "    Q3 = filtered_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    filtered_df = filtered_df[(filtered_df[col] >= lower_bound) & (filtered_df[col] <= upper_bound)]\n",
    "\n",
    "# 1. Calcular la mediana por grupo (relabeled) para cada m√©trica\n",
    "median_df = filtered_df.groupby('relabeled')[filtered_metric_columns].median()\n",
    "\n",
    "# 2. Calcular la varianza entre las medianas para cada m√©trica\n",
    "median_variance = median_df.var()\n",
    "\n",
    "# 3. Convertir a DataFrame para visualizaci√≥n\n",
    "median_variance_df = median_variance.reset_index()\n",
    "median_variance_df.columns = ['m√©trica', 'varianza_entre_medianas']\n",
    "\n",
    "# 4. Normalizar las varianzas (Min-Max)\n",
    "scaler = MinMaxScaler()\n",
    "median_variance_df['varianza_normalizada'] = scaler.fit_transform(\n",
    "    median_variance_df[['varianza_entre_medianas']]\n",
    ")\n",
    "\n",
    "# 5. Mostrar la tabla ordenada (opcional)\n",
    "print(\"Varianza entre medianas y su normalizaci√≥n:\")\n",
    "display(median_variance_df.sort_values(by='varianza_normalizada', ascending=False))\n",
    "\n",
    "# 6. Visualizaci√≥n con barplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='varianza_normalizada',\n",
    "    y='m√©trica',\n",
    "    data=median_variance_df.sort_values(by='varianza_normalizada', ascending=True),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Varianza normalizada entre medianas por m√©trica (sin at√≠picos)', fontsize=14)\n",
    "plt.xlabel('Varianza normalizada')\n",
    "plt.ylabel('M√©trica')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30177a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 1. Eliminar m√©tricas espec√≠ficas ----------\n",
    "excluded_metrics = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "filtered_metrics = [col for col in metric_columns if col not in excluded_metrics]\n",
    "\n",
    "# ---------- 2. Filtrar outliers usando el m√©todo IQR ----------\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "    return df_clean\n",
    "\n",
    "# Aplicar filtro de outliers sobre las columnas m√©tricas filtradas\n",
    "clean_df = remove_outliers_iqr(combined_df, filtered_metrics)\n",
    "\n",
    "# ---------- 3. Entrenamiento del modelo con datos limpios ----------\n",
    "X = clean_df[filtered_metrics]\n",
    "y = clean_df['relabeled']\n",
    "\n",
    "# Crear y entrenar el modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Obtener importancias de caracter√≠sticas\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# ---------- 4. Visualizaci√≥n ----------\n",
    "plt.figure(figsize=(11, 6))\n",
    "sns.barplot(x=filtered_metrics, y=importances, palette='viridis')\n",
    "plt.title('Importancia de Caracter√≠sticas para la Clasificaci√≥n de Agarre (sin outliers y sin ZC/Kurtosis)')\n",
    "plt.xlabel('M√©tricas')\n",
    "plt.ylabel('Importancia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Funci√≥n para calcular CV ----------\n",
    "def calculate_cv(metrics_df):\n",
    "    cv_values = {}\n",
    "    for metric in metrics_df.columns:\n",
    "        mean_value = metrics_df[metric].mean()\n",
    "        std_value = metrics_df[metric].std()\n",
    "        if mean_value != 0:\n",
    "            cv_values[metric] = (std_value / mean_value) * 100\n",
    "        else:\n",
    "            cv_values[metric] = np.nan\n",
    "    return cv_values\n",
    "\n",
    "# ---------- Funci√≥n para eliminar outliers usando IQR ----------\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "    return df_clean\n",
    "\n",
    "# ---------- Par√°metros ----------\n",
    "fm = 2000\n",
    "window_length = 400\n",
    "overlap = 0\n",
    "target_channel = \"Channel 8\"\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# ---------- Iterar sobre los archivos ----------\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                test_df, grasps = src.build_dataframe(\n",
    "                    mat_file=mat_data,\n",
    "                    database=database,\n",
    "                    filename=file_name,\n",
    "                    rectify=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "                test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "\n",
    "                if target_channel not in test_df.columns:\n",
    "                    print(f\"{target_channel} no encontrado en {file_name}, omitiendo.\")\n",
    "                    continue\n",
    "\n",
    "                emg_columns = [target_channel]\n",
    "                envelope_df = src.get_envelope_lowpass(\n",
    "                    test_df[emg_columns], fm=2000, cutoff_freq=0.6, envelope_type=1\n",
    "                )\n",
    "\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "\n",
    "                window_count = 0\n",
    "\n",
    "                for grasp in grasps:\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing Grasp {grasp} in file {file_name}:\")\n",
    "                        grasp_df = result_df[result_df['stimulus'] == grasp]\n",
    "\n",
    "                        if grasp_df.empty:\n",
    "                            print(f\"No hay datos para el grasp {grasp} en {file_name}.\")\n",
    "                            continue\n",
    "\n",
    "                        ventanas = src.create_windows_with_overlap(grasp_df, window_length, overlap)\n",
    "\n",
    "                        for i, ventana in enumerate(ventanas):\n",
    "                            if len(ventana) == window_length:\n",
    "                                signal = ventana[target_channel].values\n",
    "                                metrics = calculate_emg_metrics_means(signal)\n",
    "\n",
    "                                metrics_with_meta = {\n",
    "                                    \"subject\": folder,\n",
    "                                    \"relabeled\": grasp_df['relabeled'].iloc[0],\n",
    "                                    \"stimulus\": grasp,\n",
    "                                    \"channel\": target_channel,\n",
    "                                    \"window_id\": f\"{file_name}_{grasp}_{i}\",\n",
    "                                    \"file_name\": file_name,\n",
    "                                    \"window_number\": window_count,\n",
    "                                    **metrics\n",
    "                                }\n",
    "\n",
    "                                all_metrics.append(metrics_with_meta)\n",
    "                                window_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"Procesadas {window_count} ventanas para el archivo {file_name}\")\n",
    "\n",
    "# ---------- Crear DataFrame general ----------\n",
    "metrics_df_200 = pd.DataFrame(all_metrics)\n",
    "\n",
    "# ---------- Reordenar columnas ----------\n",
    "meta_cols = [\"subject\", \"relabeled\", \"stimulus\", \"channel\", \"window_id\", \"file_name\", \"window_number\"]\n",
    "metric_cols = [col for col in metrics_df_200.columns if col not in meta_cols]\n",
    "column_order = meta_cols + sorted(metric_cols)\n",
    "metrics_df_200 = metrics_df_200[column_order]\n",
    "\n",
    "# ---------- Filtrar m√©tricas no deseadas ----------\n",
    "excluded_metrics = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "filtered_metrics = [col for col in metric_cols if col not in excluded_metrics]\n",
    "\n",
    "# ---------- Eliminar outliers ----------\n",
    "clean_df_200 = remove_outliers_iqr(metrics_df_200, filtered_metrics)\n",
    "\n",
    "# ---------- Calcular CV sobre m√©tricas limpias ----------\n",
    "cv_values = calculate_cv(clean_df_200[filtered_metrics])\n",
    "cv_df = pd.DataFrame.from_dict(cv_values, orient='index', columns=['Coeficiente de Variaci√≥n'])\n",
    "cv_df = cv_df.sort_values(by='Coeficiente de Variaci√≥n', ascending=False)\n",
    "\n",
    "# ---------- Mostrar resultados ----------\n",
    "print(\"\\nüìä Coeficiente de variaci√≥n de las m√©tricas (sin outliers, sin ZC/Kurtosis):\")\n",
    "display(cv_df)\n",
    "\n",
    "print(\"\\nüìà Resumen de m√©tricas por tipo de movimiento (limpio):\")\n",
    "grouped_df = clean_df_200.drop(columns=['channel'], errors='ignore')\n",
    "summary_by_subject_movement_200 = grouped_df.select_dtypes(include=['number']).groupby(['relabeled']).mean()\n",
    "display(summary_by_subject_movement_200)\n",
    "\n",
    "print(f\"\\n‚úÖ Total de ventanas limpias procesadas: {len(clean_df_200)}\")\n",
    "print(f\"üìå Distribuci√≥n por sujeto:\\n{clean_df_200['subject'].value_counts()}\")\n",
    "print(f\"üìå Distribuci√≥n por movimiento:\\n{clean_df_200['relabeled'].value_counts()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
