{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prótesis avanzadas\n",
    "## Modelos de Inteligencia Artificial para control de prótesis con EMG\n",
    "\n",
    "### Autor: Carlos José Muñoz Cabrera\n",
    "### Asesor: Daniel Escobar\n",
    "\n",
    "Este proyecto busca definir un modelo de inteligencia artificial que permita, a partir de la extracción de de caracteristicas de señales EMG, obtener una clasificación de movimientos específicos que puede realizar el usuario y que son los más comunes y usados por la empresa Protesis Avanzadas. Las bases de datos que se usarán para poder realizar este proyecto son las compartidas por NINAPRO, cuya descripción se encuentra en el siguiente documento: ArticuloExplicaciónDB1&DB2.pdf en la carpeta de drive compaertida por la empresa. \n",
    "\n",
    "Inicialmente se hace la extracción de datos contenidos en los registros para el primer paciente. Según la documentación y explicación de la base de datos, los registros tienen ciertas características a tener en cuenta y que se van a ir describiendo a lo largo del desarrollo del presente trabajo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import src\n",
    "from scipy.io import loadmat, whosmat\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import pandas as pd\n",
    "from src import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importante:\n",
    "\n",
    "La linea a continuación se usa para descargar los datos de las bases de datos de ninapro. El proceso tarda bastante dado que depende del internet por lo que es mejor explorar la opción de realizarlo manualmente desde la pag de nina-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src.prepare_download_data(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a verificar los items que contiene el archivo matlab que contiene la información del primer sujeto S1_A1_E1. Estos datos se encuentran registrados en un archivo con extensión .mat. Hacemos uso de la librería scipy ya que es la que se usa parfa el tratamiento de este tipo de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('emg', (227493, 10), 'double')\n",
      "('stimulus', (227493, 1), 'double')\n",
      "('glove', (227493, 22), 'double')\n",
      "('subject', (1, 1), 'double')\n",
      "('exercise', (1, 1), 'double')\n",
      "('repetition', (227493, 1), 'double')\n",
      "('restimulus', (227493, 1), 'double')\n",
      "('rerepetition', (227493, 1), 'double')\n"
     ]
    }
   ],
   "source": [
    "file = 'Ninapro/DB1/s1/S1_A1_E3.mat'\n",
    "#Get the description of the information cantained in the file\n",
    "info = whosmat(file) \n",
    "#Get the information contained in the file\n",
    "mat_file = loadmat(file)\n",
    "#Print every component in the description of the file\n",
    "for item in info:\n",
    "    print(item)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access each of the matrices by its name\n",
    "emg = mat_file['emg']\n",
    "stimulus = mat_file['stimulus']\n",
    "glove = mat_file['glove']\n",
    "subject = mat_file['subject']\n",
    "exercise = mat_file['exercise']\n",
    "repetition = mat_file['repetition']\n",
    "restimulus = mat_file['restimulus']\n",
    "rerepetition = mat_file['rerepetition']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se procede a explorar cada una de las matrices contenidas en el archivo, dando una breve descripción de la información que contienen. Con el objetivo de conocer con más detalle que es lo que cada archivo contiene.\n",
    "\n",
    "No se tiene una base de tiempo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EMG\n",
    "Los valores de emg incluyen la señal sEMG obtenida por los electrodos. La columna 1 a la 8 incluyen la señal obtenida de los electrodos igualmente espaciados en el antebrazo.\n",
    "La columna 9 y 10 hace referencia a los electrodos ubicados en la principal zona de activación del músculo flexor superficial de los dedos y el musculo extensor superficial de los dedos. Si existe un electrodo 11 y 12, pertenece a la señal de activación del músculo biceps braquial y triceps braquial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print emg Matrix\n",
    "print(\"Valores de 'emg':\")\n",
    "print(f'Tipo de dato: {type(emg)}')\n",
    "print(emg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the emg signal\n",
    "fig, axs = plt.subplots(10, 1, figsize=(10, 20), sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(emg[:, i])\n",
    "    ax.set_ylabel(f'Electrodo {i+1}')\n",
    "    ax.grid=True\n",
    "\n",
    "axs[-1].set_xlabel('Tiempo')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stimulus\n",
    "Representa la etiqueta original del movimiento que está realizando el sujeto. Estos movimientos deben clasificarse en los que se va a usar en protesis avanzadas y los que no, por esta razón es necesario hacer la clasificación de dichas etiquetas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores de 'stimulus':\")\n",
    "unique_values = np.unique(stimulus)\n",
    "\n",
    "# Print unique values\n",
    "print(unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the stimulus\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(stimulus[1000:4000])\n",
    "\n",
    "ax.set_xlabel('Tiempo')\n",
    "ax.set_ylabel('Valor de Estímulo')\n",
    "ax.set_title('Valor del estímulo')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Glove\n",
    "Los valores de glove hacen referencia a la señal no calibrada de los 22 sensores de cyberglove que son úbicados en las manos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores de 'glove':\")\n",
    "print(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Subject\n",
    "\n",
    "El subject se refiere al sujeto en cuestión que está realizando la prueba y de quien pertencen los registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valor de 'subject':\")\n",
    "print(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Excercise\n",
    "\n",
    "Se refiere al numero del ejercicio que fue realizado por el sujeto. En esta base de datos se cuenta con 3 tipos de ejercicios que tienen diferentes movimientos según sea el caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valor de 'exercise':\")\n",
    "print(exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Repetition:\n",
    "Se refiere al indice de repetición del estímulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores de 'repetition':\")\n",
    "unique_values = np.unique(repetition)\n",
    "\n",
    "# Print unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the repetition\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(repetition)\n",
    "\n",
    "ax.set_xlabel('Tiempo')\n",
    "ax.set_ylabel('Valor de Repetition')\n",
    "ax.set_title('Valor del Repetition')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Restimulus\n",
    "\n",
    "Se refiere a la etiqueta refinada a-posteriori del movimiento realizado por el sujeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valores de 'restimulus':\n",
      "[ 0 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValores de 'restimulus':\")\n",
    "unique_values = np.unique(restimulus)\n",
    "\n",
    "# Print unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the restimulus\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(restimulus)\n",
    "\n",
    "ax.set_xlabel('Tiempo')\n",
    "ax.set_ylabel('Valor de restimulus')\n",
    "ax.set_title('Valor del restimulus')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Rerepetition\n",
    "\n",
    "Se refiere al indice de repetición del estimulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores de 'rerepetition':\")\n",
    "unique_values = np.unique(rerepetition)\n",
    "\n",
    "# Print unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Rerepetition\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(rerepetition)\n",
    "\n",
    "ax.set_xlabel('Tiempo')\n",
    "ax.set_ylabel('Valor de Rerepetition')\n",
    "ax.set_title('Valor del Rerepetition')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en las gráficas. Para este primer ejercicio, por cada movimiento se realizaron 10 repeticiones y volviendo al estado de reposo que se indica en el articulo de explicación de la base de datos. Dado que no se van a usar todos los movimientos, solo los que se asocian con los que usa la empresa, es necesario hacer una división de las señales de emg y glove para poder definir cuales sonlas señales correspondientes al movimiento y su repetición. \n",
    "\n",
    "Dado que tenemos una señal de restimulus, vamos a usar esta como punto de partida para hacer dicha división, tomando la sincronía que debe tener la señal. Existen dos opciones que se deben discutir, la primera es que se pueden segmentar las señales de inicio de estímulo a inicio de estímulo o de fin de estímulo a fin de estímulo.\n",
    "\n",
    "En este mommento se va a realizar la segmentación solo cuando el estímulo se encuentra activo. Sin embargo, es algo que se debe discutir con el asesor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, para el proceso. Primero encontramos los indices donde se realiza el estímulo. La idea es encontras los indices donde el estímulo es diferente de 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index_stimulus, end_index_stimulus = src.get_stimulus_index(restimulus)\n",
    "# print('start index stimulus: ', start_index_stimulus)\n",
    "# print('start index stimulus len: ', len(start_index_stimulus))\n",
    "# print('end index stimulus: ', end_index_stimulus)\n",
    "# print('end index stimulus len : ', len(end_index_stimulus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los indices de inicio indican el momento antes de comenzar el estímulo, por lo que el inicio comienza en n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Start index: {restimulus[391][0]}')\n",
    "print(f'Start real: {restimulus[717][0]}')\n",
    "print(f'End real index: {restimulus[1259][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se procede a segmentar la señal de los sensores EMG para poder así obtener la señal con su respectiva etiqueta. Según  lo conversado en las reuniones de segumiento, los movimientos que se van a tener en cuenta en el primer ejercicio son los 9,10,11,12. De esta manera se procede a segmentar estos indices de las señales de emg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "movements = np.array([5,6,9,10])\n",
    "\n",
    "filtered_start_index_stimulus = src.get_start_end_index(start_index_stimulus, end_index_stimulus, movements, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_start_index_stimulus['5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in filtered_start_index_stimulus:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este primer ejemplo, la señal está iniciando con el primer end index +1 ya que es aquí donde comienza el reposo y despúes se obtiene la señal dónde comienza el estimulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Este es el inicio del reposo: {restimulus[37397+1][0]}')\n",
    "print(f'Aquí es donde comienza el estímulo: {restimulus[39668+1][0]}')\n",
    "print(f'Aquí es donde termina el estímulo: {restimulus[47584][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa anteriormente, hemos guardado en el diccionario 10 indices que indican el inicio del estímulo en cuestion y 11 que indican el fin del estímulo o el comienzo del reposo. Dado que tenemos la señal de emg sincronizada con la señal de restimulus, entonces usaremos el mismo indice para hacer la segmentación de dicha señal logrando que una señal empiece en el reposo y termine cuando se termina el estímulo. Sin embargo, es preciso determinar si es mejor segmentar la señal cuando se tiene solo estímulo o cuando se tiene solo reposo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_signals = src.get_signal_by_movement_complete(emg[:,0], filtered_start_index_stimulus['5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_restimulus = src.get_signal_by_movement_complete(restimulus, filtered_start_index_stimulus['5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segmented_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a Graficarlas para poder verificar visualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(segmented_signals)\n",
    "ax.plot(emg[37397+1:47584, 0])\n",
    "ax.plot(segmented_restimulus*1/9)\n",
    "\n",
    "ax.set_ylabel('EMG')\n",
    "ax.set_title('Primera señal obtenida en la función con restimulus')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que ya tenemos las señales segmentadas, se procede a obtener la envolvente, primero sin aplicar filtros y después aplicando un filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envelope = src.get_envelope(segmented_signals)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(segmented_signals)\n",
    "ax.plot(envelope)\n",
    "\n",
    "ax.set_ylabel('EMG')\n",
    "ax.set_title('Primera señal obtenida en la función con restimulus')\n",
    "ax.legend(['Raw signal', 'Envelope'])\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_signal = src.get_filtered_signal(segmented_signals, 10, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(segmented_signals)\n",
    "ax.plot(filtered_signal)\n",
    "ax.plot(restimulus[37397+1:47584]/(2*9))\n",
    "\n",
    "ax.set_ylabel('EMG')\n",
    "ax.set_title('Primera señal obtenida en la función con restimulus')\n",
    "ax.legend(['Raw signal', 'Filtered signal'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envelope = src.get_envelope(filtered_signal)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(segmented_signals)\n",
    "ax.plot(filtered_signal)\n",
    "ax.plot(envelope)\n",
    "ax.plot(restimulus[37397+1:47584]/(2*9))\n",
    "\n",
    "ax.set_ylabel('EMG')\n",
    "ax.set_title('Primera señal obtenida en la función con restimulus')\n",
    "ax.legend(['Raw signal', 'Filtered signal', 'Envelope'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = f'Ninapro/DB1/'\n",
    "signal_paths = []\n",
    "for folder in os.listdir(main_folder_path):\n",
    "    folder_path = os.path.join(main_folder_path, folder)  \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if os.path.isfile(file_path) and 'E3' in file and file.endswith('.mat'):\n",
    "                signal_paths.append(file_path)\n",
    "\n",
    "print(signal_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una rutina para iterar sobre cada BD con el ejercicio 3 que son los que usaremos para la clasificación. Aunque, se va a explorar sobre los demás ejercicios, buscando obtener los movimientos que más se diferencian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All column titles are created with features to classify signals.\n",
    "column_titles = ['label']\n",
    "features = config.FEATURES\n",
    "for i in range(8):\n",
    "    for feature in features.keys():\n",
    "        column_titles.append(feature+str(i+1))\n",
    "signals_features = pd.DataFrame(columns=column_titles)\n",
    "print(signals_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "movements_label = config.MOVEMENTS_LABEL\n",
    "movements_as_integers = sorted(list(map(int, movements_label.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected movements for our database construction are [0, 5, 6, 9, 10, 13, 17] of S3 of DB1.\n",
    "\n",
    "The excercise numbering is different in DB1 and the following DBs, as movement number is not reset between excercises.  And the exercises contained vary:\n",
    "https://www.nature.com/articles/sdata201453/tables/2\n",
    "\n",
    "That means that number 5 in DB1-E3 corresponds to number 22 in the following DBs.\n",
    "(https://ninapro.hevs.ch/figures/SData_Movements.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Routine to create data frame with characteristics and labels.\n",
    "databases = config.DATABASES\n",
    "fc= config.FC #Cutoff frequency in Hz\n",
    "windowing = config.WINDOWING #s time of the window // Ej: 0.2s = 200 ms\n",
    "overlapping = config.OVERLAPPING #% overlapping percentage\n",
    "repetitions = config.REPETITIONS #Number of times a movement is repeated\n",
    "electrodes = config.ELECTRODES #Number of electrodes used in the experiment\n",
    "threshold = config.THRESHOLD # Characterization percentage\n",
    "for database, fm in databases.items(): #Iterates over databases\n",
    "    window_length = int(windowing * fm)\n",
    "    subjects = src.get_file_path_database(database)\n",
    "    for individual in subjects: #Iterates over individuals in the experiments\n",
    "        mat_file = loadmat(individual)\n",
    "        emg = mat_file['emg']\n",
    "        re_stimulus = mat_file['restimulus']\n",
    "        re_repetition = mat_file['rerepetition']\n",
    "        start_index_re_stimulus, end_index_re_stimulus = src.get_stimulus_index(re_stimulus) # Get the start and end index of the entire set of repetitions for an exercise.\n",
    "        filtered_start_index_re_stimulus = src.get_start_end_index(\n",
    "            start_index_re_stimulus, \n",
    "            end_index_re_stimulus, \n",
    "            movements_as_integers, \n",
    "            repetitions\n",
    "        ) #Get the segments of the entire set of repetitions for an exercise. \n",
    "        for movement in movements_as_integers: #Iterates over the movements implied in the classification \n",
    "            if movement == 0:\n",
    "                continue\n",
    "            segmented_re_stimulus = src.get_signal_by_movement_complete(re_stimulus, filtered_start_index_re_stimulus[str(movement)]) #Gets the signal of the movement\n",
    "            windows_re_stimulus = src.create_windows_with_overlap(segmented_re_stimulus, window_length, overlapping)\n",
    "            labels = []\n",
    "            for window in windows_re_stimulus:\n",
    "                label = src.get_label(window, threshold, movements_label, database)\n",
    "                labels.append(label)\n",
    "            emg_signals = []\n",
    "            for electrode in range(electrodes):\n",
    "                segmented_emg_signal = src.get_signal_by_movement_complete(emg[:, electrode], filtered_start_index_re_stimulus[str(movement)])\n",
    "                filtered_segmented_emg_signal = src.get_envelope_filtered(segmented_emg_signal, fc, fm)\n",
    "                windows_emg_electrode = src.create_windows_with_overlap(filtered_segmented_emg_signal, window_length, overlapping)\n",
    "                emg_signals.append(windows_emg_electrode)\n",
    "            windows_quantity  = len(emg_signals[0])\n",
    "            if (windows_quantity != len(labels)):\n",
    "                print('Different longitude in labels with electrodes', movement, individual, database) #This is used to check the windows between labels and windows in electrodes\n",
    "                raise ValueError('No es compatible')\n",
    "            for index_label,label in enumerate(labels):\n",
    "                if not label:\n",
    "                    print(label)\n",
    "                data_label = {\n",
    "                    'label': label,\n",
    "                }\n",
    "                for index_electrode, element in enumerate(emg_signals):\n",
    "                    if len(element) != windows_quantity:\n",
    "                        print('Different longitude in electrodes', index_electrode, movement, individual, database) #This is used to find any signal with different number of window\n",
    "                        raise ValueError('No es compatible')\n",
    "                    for feature, function in features.items():\n",
    "                        feature_calculus = function(element[index_label])\n",
    "                        data_label[feature+str(index_electrode+1)] = np.array([feature_calculus])\n",
    "                new_data_label = pd.DataFrame(data_label)\n",
    "                signals_features = pd.concat([signals_features, new_data_label], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_features['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataframe \n",
    "signals_features.to_pickle('preprocessed_data/signals_50OL_03Windowing.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y limpieza de datos\n",
    "\n",
    "Una vez se logra obtener el dataset completo se procede a hacer un análisis y limpieza de datos para poder ingresarlos a los diferentes algoritmos de clasificación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_features_df = pd.read_pickle('preprocessed_data/signals_50OL_03Windowing.pkl') #upload data_frame\n",
    "print('Head of dataset:')\n",
    "print(signals_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numeric data information:')\n",
    "print(signals_features_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numeric data description:')\n",
    "print(signals_features_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_features_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_delete = signals_features_df[signals_features_df['label'] == 'None'].index\n",
    "\n",
    "signals_features_df.drop(index_to_delete, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante recalcar que las etiquetas clasificadas como Base superan en gran medida a las demás etiquetas a clasificar por lo que vamos a aplicar submuetreo sobre los datos para poder igualar los datos de la base y que sea de manera aleatoria. De esta manera buscamos balancear los datos y que así no exista un  sesgo en la clasificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = signals_features_df.drop('label', axis=1)\n",
    "y = signals_features_df['label']\n",
    "\n",
    "rus = RandomUnderSampler(random_state=40)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "print(f\"Distribución de clases antes de Under-sampling: {Counter(y)}\")\n",
    "print(f\"Distribución de clases después de Under-sampling: {Counter(y_res)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.concat([pd.DataFrame(X_res, columns=X.columns), pd.DataFrame(y_res, columns=['label'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a obtener un dataframe sin la clase Base para ver como se comporta la clasificación, aunque esto no es lo recomendado simplemente se hace para poder realizar comparaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = signals_features_df[signals_features_df['label'] != 'Base'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos los diferentes dataframes: signals_features_df, df_balanced, df_filtered\n",
    "Donde:\n",
    " * signals_features_df: dataFrame original \n",
    " * df_balanced: dataFrame balanceado usando submuestreo\n",
    " * df_filtered: dataFrame sin la clase Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_features_df.hist(figsize=(20, 16))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=signals_features_df)\n",
    "plt.xticks(rotation=90)  # Rotar etiquetas del eje x para mejor legibilidad\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calcular z-score para todas las variables numéricas\n",
    "z_scores = stats.zscore(signals_features_df.select_dtypes(include='number'))\n",
    "\n",
    "# Convertir a DataFrame para facilitar el análisis\n",
    "z_scores_df = pd.DataFrame(z_scores, columns=signals_features_df.select_dtypes(include='number').columns)\n",
    "\n",
    "# Encontrar filas con algún valor de z-score por encima de un umbral\n",
    "outliers = z_scores_df[(z_scores_df > 3).any(axis=1)]\n",
    "print(\"Filas con valores atípicos (Z-Score > 3):\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Como se observa anteriormente, se puede ver que existen muchos valores atípicos. Sin embargo, no vamos a omitirlos para ver como resulta el entrenamiento haciendo uso de los mismos. Procedemos a aplicar algoritmos de clasificación que no necesitan normalización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data divided in train and test\n",
    "features = signals_features_df.drop('label', axis=1) \n",
    "labels = signals_features_df['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "#Build the classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=300, random_state=32, criterion='gini')\n",
    "\n",
    "#Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Get prediction\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "src.print_evaluation_metrics(y_pred=y_pred, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data divided in train and test\n",
    "features = df_balanced.drop('label', axis=1) \n",
    "labels = df_balanced['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "#Build the classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=120, random_state=32, criterion='gini')\n",
    "\n",
    "#Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Get prediction\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "src.print_evaluation_metrics(y_pred=y_pred, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data divided in train and test\n",
    "features = df_filtered.drop('label', axis=1) \n",
    "labels = df_filtered['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "#Build the classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=32, criterion='gini')\n",
    "\n",
    "#Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Get prediction\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "src.print_evaluation_metrics(y_pred=y_pred, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data divided in train and test\n",
    "features = signals_features_df.drop('label', axis=1) \n",
    "labels = signals_features_df['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "#Build the classifier\n",
    "rf_classifier = GaussianNB()\n",
    "\n",
    "#Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Get prediction\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "src.print_evaluation_metrics(y_pred=y_pred, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data divided in train and test\n",
    "features = df_balanced.drop('label', axis=1) \n",
    "labels = df_balanced['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "#Build the classifier\n",
    "rf_classifier = GaussianNB()\n",
    "\n",
    "#Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Get prediction\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "src.print_evaluation_metrics(y_pred=y_pred, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data divided in train and test\n",
    "features = df_filtered.drop('label', axis=1) \n",
    "labels = df_filtered['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "#Build the classifier\n",
    "rf_classifier = GaussianNB()\n",
    "\n",
    "#Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Get prediction\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "src.print_evaluation_metrics(y_pred=y_pred, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicar normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = signals_features_df.drop('label', axis=1) \n",
    "labels = signals_features_df['label'] \n",
    "\n",
    "min_max_scaler = MinMaxScaler() # We can use StandardScaler, which provides values with a mean of 0 and a standard deviation of 1. Meanwhile, MinMaxScaler scales the data to a range of 0 to 1.\n",
    "X_normalized = min_max_scaler.fit_transform(X)\n",
    "\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "\n",
    "df_normalized = pd.concat([X_normalized_df, labels.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(df_normalized.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_balanced.drop('label', axis=1) \n",
    "labels = df_balanced['label'] \n",
    "\n",
    "min_max_scaler = MinMaxScaler() # We can use StandardScaler, which provides values with a mean of 0 and a standard deviation of 1. Meanwhile, MinMaxScaler scales the data to a range of 0 to 1.\n",
    "X_normalized = min_max_scaler.fit_transform(X)\n",
    "\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "\n",
    "df_normalized_balanced = pd.concat([X_normalized_df, labels.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(df_normalized_balanced.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data divided in train and test\n",
    "features = df_normalized.drop('label', axis=1) \n",
    "labels = df_normalized['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale') ##modify hyperparameters\n",
    "\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "src.print_evaluation_metrics(y_pred=y_pred, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales\n",
    "\n",
    "Vamos a aplicar una red neuronal para diferenciar entre base y los demás ejercicios y después una red neuronal para poder clasificar cada movimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_balanced['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplicar base vs los demás para hacer la primer clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_normalized.copy()\n",
    "condition = df_base['label'] != 'Base'\n",
    "df_base.loc[condition, 'label'] ='not-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primera red neuronal:\n",
    "Clasificar en base y no base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_base['label_encoded'] = label_encoder.fit_transform(df_base['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_base.drop(columns=['label_encoded', 'label'], axis=1) \n",
    "labels = df_base['label_encoded'] \n",
    "labels = to_categorical(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=20,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar la pérdida\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Graficar la precisión\n",
    "plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=20,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar la pérdida\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Graficar la precisión\n",
    "plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda Red Neuronal\n",
    "Vamos ahora a clasificar los modelos sin base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_delete = df_normalized[df_normalized['label'] == 'Base'].index\n",
    "\n",
    "df_normalized.drop(index_to_delete, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_normalized['label_encoded'] = label_encoder.fit_transform(df_normalized['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['label_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_normalized.drop(columns=['label_encoded', 'label'], axis=1) \n",
    "labels = df_normalized['label_encoded'] \n",
    "labels = to_categorical(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(80, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = LearningRateScheduler(src.warmup_scheduler) #Aumenta la taza de aprendizaje al inicio y luego la disminuye \n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=10,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar la pérdida\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Graficar la precisión\n",
    "plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=20,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar la pérdida\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Graficar la precisión\n",
    "plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
