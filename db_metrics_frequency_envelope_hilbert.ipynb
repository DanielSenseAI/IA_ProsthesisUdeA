{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libreries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat, whosmat\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "import src\n",
    "from src import config, loadmatNina\n",
    "import pywt\n",
    "\n",
    "from src.preprocessing_utils import get_envelope\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.spatial.distance as ssd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import inv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the database to analyze\n",
    "database = 'DB4'\n",
    "\n",
    "data_path = f'data/{database}'\n",
    "\n",
    "# Find the folder named with the convention s + \"number\"\n",
    "folder = None\n",
    "for item in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', item) or re.match(r'Subject\\d+', item):\n",
    "        folder = item\n",
    "        break\n",
    "\n",
    "if folder:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all .mat files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.mat'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            info = whosmat(file_path)\n",
    "            results.append((file_name, info))\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    data = {}\n",
    "    for file_name, info in results:\n",
    "        for item in info:\n",
    "            if item[0] not in data:\n",
    "                data[item[0]] = {}\n",
    "            data[item[0]][file_name] = item[1:]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    df.columns.name = 'File Name'\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No folder found with the convention s + 'number'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For complete signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics(signal, fs=1000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    # Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # Variance (VAR)\n",
    "    var = np.var(signal)\n",
    "    \n",
    "    # Coefficient of Variation (CoV)\n",
    "    mean_signal = np.mean(signal)\n",
    "    cov = (np.std(signal) / mean_signal) if mean_signal != 0 else 0\n",
    "    \n",
    "    # Mean Frequency (MNF)\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "    fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "    mnf = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    \n",
    "    # Marginal Discrete Wavelet Transform (mDWT)\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "    mdwt = np.sum([np.sum(np.abs(c)) for c in coeffs])\n",
    "    \n",
    "    # Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"RMS\": rms,\n",
    "        \"WL\": wl,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"VAR\": var,\n",
    "        \"CoV\": cov,\n",
    "        \"MNF\": mnf,\n",
    "        \"mDWT\": mdwt,\n",
    "        \"TD\": td,\n",
    "        \"MAVS\": mavs\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For signal with means and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_std(signal, fs=1000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal, including mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:\n",
    "        metrics_per_channel = [calculate_emg_metrics(signal[:, ch], fs) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    mav_std = np.std(np.abs(signal))\n",
    "    \n",
    "    # Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    iav_std = np.std(np.abs(signal))\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    rms_std = np.std(signal)\n",
    "    \n",
    "    # Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    wl_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    zc_std = np.std(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    ssc_std = np.std((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # Variance (VAR)\n",
    "    var = np.var(signal)\n",
    "    var_std = np.std(signal)\n",
    "    \n",
    "    # Coefficient of Variation (CoV)\n",
    "    mean_signal = np.mean(signal)\n",
    "    cov = (np.std(signal) / mean_signal) if mean_signal != 0 else 0\n",
    "    cov_std = np.std(cov)\n",
    "    \n",
    "    # Mean Frequency (MNF)\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "    fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "    mnf = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    mnf_std = np.std(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    \n",
    "    # Marginal Discrete Wavelet Transform (mDWT)\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "    mdwt = np.sum([np.sum(np.abs(c)) for c in coeffs])\n",
    "    mdwt_std = np.std([np.sum(np.abs(c)) for c in coeffs])\n",
    "    \n",
    "    # Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    td_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    mavs_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav, \"MAV_STD\": mav_std,\n",
    "        \"IAV\": iav, \"IAV_STD\": iav_std,\n",
    "        \"RMS\": rms, \"RMS_STD\": rms_std,\n",
    "        \"WL\": wl, \"WL_STD\": wl_std,\n",
    "        \"ZC\": zc, \"ZC_STD\": zc_std,\n",
    "        \"SSC\": ssc, \"SSC_STD\": ssc_std,\n",
    "        \"VAR\": var, \"VAR_STD\": var_std,\n",
    "        \"CoV\": cov, \"CoV_STD\": cov_std,\n",
    "        \"MNF\": mnf, \"MNF_STD\": mnf_std,\n",
    "        \"mDWT\": mdwt, \"mDWT_STD\": mdwt_std,\n",
    "        \"TD\": td, \"TD_STD\": td_std,\n",
    "        \"MAVS\": mavs, \"MAVS_STD\": mavs_std\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This functions calculate the metrics for channel and average the values for a complete result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_means(signal):\n",
    "    \"\"\"\n",
    "    Calculates the metrics of an EMG signal. If there are multiple channels, it computes \n",
    "    the metrics for each channel and then averages the results.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:  # If the signal has multiple channels\n",
    "        metrics_per_channel = [calculate_emg_metrics_means(signal[:, ch]) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # Variance (VAR)\n",
    "    var = np.var(signal)\n",
    "    \n",
    "    # Coefficient of Variation (CoV)\n",
    "    mean_signal = np.mean(signal)\n",
    "    cov = (np.std(signal) / mean_signal) if mean_signal != 0 else 0\n",
    "    \n",
    "    # Mean Frequency (MNF)\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "    fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "    mnf = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    \n",
    "    # Marginal Discrete Wavelet Transform (mDWT)\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "    mdwt = np.sum([np.sum(np.abs(c)) for c in coeffs])\n",
    "    \n",
    "    # Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"RMS\": rms,\n",
    "        \"WL\": wl,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"VAR\": var,\n",
    "        \"CoV\": cov,\n",
    "        \"MNF\": mnf,\n",
    "        \"mDWT\": mdwt,\n",
    "        \"TD\": td,\n",
    "        \"MAVS\": mavs\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots and metrics for complete grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name\n",
    "database = 'DB4'\n",
    "\n",
    "# Full path to the database folder\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "\n",
    "# List of subjects, generating names from 's1' to 's10'\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# Iterate over each subject\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3 for the current subject\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Verify the structure of the loaded dictionary\n",
    "        print(f\"Keys in mat_data: {mat_data.keys()}\")\n",
    "        \n",
    "        # Retrieve re-labeled data and the list of labeled grasps\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over each labeled grasp\n",
    "        for grasp  in grasps_etiquetados:\n",
    "            try:\n",
    "                # Check if 'emg' key exists in mat_data\n",
    "                if 'emg' not in mat_data:\n",
    "                    raise KeyError(f\"The key 'emg' is not in mat_data. Available keys: {mat_data.keys()}\")\n",
    "                \n",
    "                # Get the EMG signal for the labeled grasp\n",
    "                emg_signal = mat_data['emg'][grasp]  # Adjust based on the actual structure\n",
    "                \n",
    "                # Compute EMG signal metrics\n",
    "                metrics = calculate_emg_metrics(emg_signal)\n",
    "                \n",
    "                # Print computed metrics\n",
    "                print(f\"\\nMetrics for Grasp {grasp}:\")\n",
    "                for key, value in metrics.items():\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "                \n",
    "                # Plot the EMG signal for the grasp\n",
    "                src.plot_emg_data(\n",
    "                    database=database,\n",
    "                    mat_file=mat_data,\n",
    "                    grasp_number=grasp,\n",
    "                    interactive=False,\n",
    "                    include_rest=True,\n",
    "                    use_stimulus=False,\n",
    "                    addFourier=False,\n",
    "                    padding=100,\n",
    "                    title=f\"{filename} - Grasp {grasp}\"\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                print(f\"    Error: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing grasp {grasp}: {str(e)}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with metrics for a complete signal without discriminating by channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name\n",
    "database = 'DB4'\n",
    "\n",
    "# Full path to the database folder\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "\n",
    "# List of subjects, generating names from 's1' to 's10'\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# List to store all extracted metrics\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject in the database\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists before processing\n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Skip if file is not available\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Build DataFrame with re-labeled data\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over labeled grasps\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Retrieve the corresponding EMG signal\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                \n",
    "                # Compute EMG signal metrics\n",
    "                metrics = calculate_emg_metrics(emg_signal)\n",
    "                \n",
    "                # Append metrics with metadata to the list\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Unpack metrics into the dictionary\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Create a DataFrame with organized metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns (optional) for better visualization\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Print the final DataFrame with extracted metrics\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with average of metrics for channels in each grasp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all computed metrics\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject in the database\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists before processing\n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Skip if file is not available\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Build DataFrame with re-labeled data\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over labeled grasps\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Retrieve the corresponding EMG signal\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                \n",
    "                # Compute EMG signal metrics using standard deviation\n",
    "                metrics = calculate_emg_metrics_std(emg_signal)\n",
    "                \n",
    "                # Append metrics with metadata to the list\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Unpack metrics into the dictionary\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Create a DataFrame with organized metrics\n",
    "metrics_df_std = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns (optional) for better visualization\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics.keys())\n",
    "metrics_df_std = metrics_df_std[column_order]\n",
    "\n",
    "# Print the final DataFrame with extracted metrics\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "display(metrics_df_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe for every channels of data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all generated DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Look for folders matching the pattern \"s + number\" or \"Subject + number\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterate over all .mat files in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Attempt to load the .mat file\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Attempt to process the file with src.build_dataframe\n",
    "                try:\n",
    "                    test_df, grasps = src.build_dataframe(\n",
    "                        mat_file=mat_data,\n",
    "                        database=database,\n",
    "                        filename=file_name,\n",
    "                        rectify=False,\n",
    "                        normalize=True\n",
    "                    )\n",
    "                    \n",
    "                    # Add a column with the subject name (folder) to the DataFrame\n",
    "                    test_df['subject'] = folder  \n",
    "                    \n",
    "                    # Append the processed DataFrame to the list\n",
    "                    all_dataframes.append(test_df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Concatenate all DataFrames into a single one if data is available\n",
    "if all_dataframes:  \n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Display the combined DataFrame\n",
    "    print(\"\\n Combined DataFrame:\")\n",
    "    display(combined_df)  \n",
    "\n",
    "else:\n",
    "    print(\"Warning: No DataFrames were generated. Check the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all generated DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Look for folders matching the pattern \"s + number\" or \"Subject + number\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterate over all .mat files in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Attempt to load the .mat file\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Attempt to process the file with src.build_dataframe\n",
    "                try:\n",
    "                    test_df, grasps = src.build_dataframe(\n",
    "                        mat_file=mat_data,\n",
    "                        database=database,\n",
    "                        filename=file_name,\n",
    "                        rectify=False,\n",
    "                        normalize=True\n",
    "                    )\n",
    "                    \n",
    "                    # Apply envelope extraction\n",
    "                    emg_columns = [col for col in test_df.columns if \"Channel\" in col]\n",
    "                    envelope_df = get_envelope(test_df[emg_columns], envelope_type=1)  # Change type as needed\n",
    "                    \n",
    "                    # Preserve non-EMG columns\n",
    "                    meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                    result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                    \n",
    "                    # Add a column with the subject name (folder) to the DataFrame\n",
    "                    result_df['subject'] = folder  \n",
    "                    \n",
    "                    # Append the processed DataFrame to the list\n",
    "                    all_dataframes.append(result_df)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Concatenate all DataFrames into a single one if data is available\n",
    "if all_dataframes:  \n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Display the combined DataFrame\n",
    "    print(\"\\n Combined DataFrame:\")\n",
    "    display(combined_df)  \n",
    "\n",
    "else:\n",
    "    print(\"Warning: No DataFrames were generated. Check the input data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with metrics for channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics(signal, fs=2000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal, including mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if signal.ndim == 2:\n",
    "            metrics_per_channel = [calculate_emg_metrics(signal[:, ch], fs) for ch in range(signal.shape[1])]\n",
    "            averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "            return averaged_metrics\n",
    "        \n",
    "        abs_signal = np.abs(signal)\n",
    "        diff_signal = np.diff(signal)\n",
    "        diff_abs_signal = np.abs(diff_signal)\n",
    "        \n",
    "        # Compute Metrics\n",
    "        metrics = {\n",
    "            \"MAV\": np.mean(abs_signal), \"MAV_STD\": np.std(abs_signal),\n",
    "            \"IAV\": np.sum(abs_signal), \"IAV_STD\": np.std(abs_signal),\n",
    "            \"RMS\": np.sqrt(np.mean(signal**2)), \"RMS_STD\": np.std(signal),\n",
    "            \"WL\": np.sum(diff_abs_signal), \"WL_STD\": np.std(diff_abs_signal),\n",
    "            \"ZC\": np.sum(np.diff(np.sign(signal)) != 0), \"ZC_STD\": np.std(np.diff(np.sign(signal)) != 0),\n",
    "            \"SSC\": np.sum((diff_signal[1:] * diff_signal[:-1]) < 0), \"SSC_STD\": np.std((diff_signal[1:] * diff_signal[:-1]) < 0),\n",
    "            \"VAR\": np.var(signal), \"VAR_STD\": np.std(signal),\n",
    "            \"CoV\": (np.std(signal) / np.mean(signal)) if np.mean(signal) != 0 else 0,\n",
    "            \"TD\": np.sum(diff_abs_signal), \"TD_STD\": np.std(diff_abs_signal),\n",
    "            \"MAVS\": np.mean(diff_abs_signal), \"MAVS_STD\": np.std(diff_abs_signal),\n",
    "            \"MNP\": np.mean(signal**2), \"MNP_STD\": np.std(signal**2),\n",
    "        }\n",
    "        \n",
    "        # Spectral Metrics\n",
    "        freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "        fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "        metrics[\"MNF\"] = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude) if np.sum(fft_magnitude) != 0 else 0\n",
    "        metrics[\"MNF_STD\"] = np.std(freqs * fft_magnitude) / np.sum(fft_magnitude) if np.sum(fft_magnitude) != 0 else 0\n",
    "        \n",
    "        # Wavelet Transform\n",
    "        coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "        mdwt_values = np.array([np.sum(np.abs(c)) for c in coeffs])\n",
    "        metrics[\"mDWT\"] = np.sum(mdwt_values)\n",
    "        metrics[\"mDWT_STD\"] = np.std(mdwt_values)\n",
    "        \n",
    "        # Kurtosis\n",
    "        std_signal = np.std(signal)\n",
    "        metrics[\"Kurt\"] = np.mean((signal - np.mean(signal)) ** 4) / (std_signal ** 4) if std_signal != 0 else 0\n",
    "        metrics[\"Kurt_STD\"] = np.std(metrics[\"Kurt\"])\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_emg_metrics: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (subject, relabeled), group in combined_df.groupby(['subject', 'relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                \"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"subject\", \"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'channel' column to group data by subject and movement type\n",
    "grouped_df = metrics_df.drop(columns=['channel'])\n",
    "\n",
    "# Compute the mean value of each metric grouped by subject and movement\n",
    "df_mean = grouped_df.groupby(['subject', 'relabeled']).mean()\n",
    "\n",
    "# Compute the standard deviation of each metric grouped by subject and movement\n",
    "df_std = grouped_df.groupby(['subject', 'relabeled']).std()\n",
    "\n",
    "# Rename columns to indicate they contain mean values\n",
    "df_mean.columns = [f\"{col} mean\" for col in df_mean.columns]\n",
    "\n",
    "# Rename columns to indicate they contain standard deviation values\n",
    "df_std.columns = [f\"{col} std\" for col in df_std.columns]\n",
    "\n",
    "# Merge the mean and standard deviation DataFrames into a single DataFrame\n",
    "df_result = df_mean.merge(df_std, on=['subject', 'relabeled']).reset_index()\n",
    "\n",
    "# Display the final DataFrame with aggregated metrics\n",
    "display(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendogram for grasp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns containing numerical features\n",
    "features = df_result.iloc[:, 2:]  # Exclude 'subject' and 'relabeled'\n",
    "\n",
    "# Normalize the data to improve comparability and avoid bias due to different scales\n",
    "df_scaled = StandardScaler().fit_transform(features)\n",
    "\n",
    "# Apply hierarchical clustering using the Ward method (minimizes variance within clusters)\n",
    "linked = sch.linkage(df_scaled, method='ward')\n",
    "\n",
    "# Create and visualize the dendrogram\n",
    "plt.figure(figsize=(20, 10))\n",
    "sch.dendrogram(\n",
    "    linked, \n",
    "    labels=df_result['relabeled'].values,  # Labels on the x-axis based on the 'relabeled' variable\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=8  # Adjust font size\n",
    ")\n",
    "plt.title(\"Dendrogram based on the 'relabeled' variable\")\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Euclidean Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'relabeled' and calculate the mean and standard deviation of each numerical feature\n",
    "grouped = df_result.select_dtypes(include=['number']).groupby(df_result['relabeled']).agg(['mean', 'std'])\n",
    "display(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'relabeled' and calculate the mean and standard deviation of each numerical feature\n",
    "grouped = df_result.select_dtypes(include=['number']).groupby(df_result['relabeled']).agg(['mean', 'std'])\n",
    "display(grouped)\n",
    "\n",
    "# Flatten column names to make them easier to work with\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "# Normalize the data to prevent magnitude differences from affecting the clustering distance\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# Apply hierarchical clustering using the Ward method (minimizes variance within clusters)\n",
    "linked = sch.linkage(scaled_features, method='ward')\n",
    "\n",
    "# Create and visualize the dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sch.dendrogram(\n",
    "    linked, \n",
    "    labels=grouped.index.tolist(),  # Labels on the x-axis based on the 'relabeled' variable\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=8  # Adjust font size\n",
    ")\n",
    "plt.title(\"Dendrogram based on mean and standard deviation per grasp type\")\n",
    "plt.xlabel(\"Grasps\")\n",
    "plt.ylabel(\"Euclidean Distance\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the average of metrics per channel\n",
    "# Exclude 'subject', 'relabeled', and 'channel' to keep only the metric columns\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "\n",
    "# Group by 'channel' and compute the mean of each metric\n",
    "average_metrics_df = metrics_df.groupby('channel')[metrics_columns].mean().reset_index()\n",
    "display(average_metrics_df)\n",
    "\n",
    "# 2. Prepare data for clustering\n",
    "X = average_metrics_df[metrics_columns].values  # Extract metric values as an array for clustering\n",
    "\n",
    "# 3. Compute the distance matrix and perform hierarchical clustering\n",
    "Z = linkage(X, method='ward')  # 'ward' minimizes variance within clusters\n",
    "\n",
    "# 4. Plot the dendrogram with adjustments for better visualization\n",
    "plt.figure(figsize=(15, 8)) \n",
    "plt.title('Dendrogram of EMG Channels (Average Metrics)', fontsize=16, pad=20)\n",
    "plt.xlabel('Channels', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "\n",
    "# Adjust the dendrogram to prevent overlapping labels\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=average_metrics_df['channel'].values,  # Labels for each channel\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=12,  # Adjust font size\n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Threshold to color clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust layout for better fit\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the mean and standard deviation of metrics per channel\n",
    "# Exclude 'subject', 'relabeled', and 'channel' to keep only numerical metric columns\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "\n",
    "# Group by 'channel' and compute the mean and standard deviation for each metric\n",
    "agg_metrics_df = metrics_df.groupby('channel')[metrics_columns].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Flatten column names for easier access (concatenating \"metric_type\")\n",
    "agg_metrics_df.columns = ['_'.join(col).strip('_') for col in agg_metrics_df.columns]\n",
    "\n",
    "display(agg_metrics_df)  # Display the aggregated metrics table\n",
    "\n",
    "# 2. Prepare data for clustering using only the metric averages\n",
    "X = agg_metrics_df[[col for col in agg_metrics_df.columns if col.endswith('_mean')]].values  # Extract only \"_mean\" columns\n",
    "\n",
    "# 3. Compute the distance matrix and perform hierarchical clustering\n",
    "Z = linkage(X, method='ward')  # 'ward' minimizes variance within clusters\n",
    "\n",
    "# 4. Plot the dendrogram with adjustments for better visualization\n",
    "plt.figure(figsize=(15, 8)) \n",
    "plt.title('Dendrogram of EMG Channels (Average Metrics)', fontsize=16, pad=20)\n",
    "plt.xlabel('Channels', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "\n",
    "# Adjust the dendrogram to prevent overlapping labels\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=agg_metrics_df['channel'].values,  # Labels for EMG channels\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=12,  # Adjust font size\n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Threshold to color clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust layout for better fit\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the metrics of interest: RMS and MNF\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "print(dependent_vars)\n",
    "\n",
    "# Check and remove redundant columns due to high correlation\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# Verify variables in the DataFrame\n",
    "missing_vars = [var for var in dependent_vars if var not in df_result.columns]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"The following dependent variables are missing in the DataFrame: {missing_vars}\")\n",
    "if 'relabeled' not in df_result.columns:\n",
    "    raise ValueError(\"The 'relabeled' column is not present in the DataFrame.\")\n",
    "\n",
    "# MANOVA analysis\n",
    "formula = f\"{' + '.join(dependent_vars)} ~ relabeled\"\n",
    "try:\n",
    "    manova = MANOVA.from_formula(formula, data=df_result)\n",
    "    print(manova.mv_test())\n",
    "except Exception as e:\n",
    "    print(f\"Error in MANOVA: {e}\")\n",
    "\n",
    "# Group by 'relabeled' and compute the mean of each metric\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# Feature normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# Compute Mahalanobis distance\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# Square distance matrix\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# Agglomerative hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# Dendrogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.4 * max(linked[:, 2])\n",
    ")\n",
    "plt.title(\"Dendrogram Based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select only the metrics of interest\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "print(\"Selected variables:\", dependent_vars)\n",
    "\n",
    "# 2. Exclude data where relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Check and remove highly correlated columns\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]  # Threshold: 0.95\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Verify variables in the DataFrame\n",
    "missing_vars = [var for var in dependent_vars if var not in df_result.columns]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"The following dependent variables are missing in the DataFrame: {missing_vars}\")\n",
    "if 'relabeled' not in df_result.columns:\n",
    "    raise ValueError(\"The 'relabeled' column is not present in the DataFrame.\")\n",
    "\n",
    "# 6. MANOVA analysis\n",
    "formula = f\"{' + '.join(dependent_vars)} ~ relabeled\"\n",
    "try:\n",
    "    manova = MANOVA.from_formula(formula, data=df_result)\n",
    "    print(manova.mv_test())\n",
    "except Exception as e:\n",
    "    print(f\"Error in MANOVA: {e}\")\n",
    "\n",
    "# 7. Group by 'relabeled' and compute the mean of each metric\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# 8. Feature normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 9. Compute Mahalanobis distance\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Pseudo-inverse for numerical stability\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 10. Square distance matrix\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 11. Agglomerative hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "\n",
    "# 12. Normalize distances for the dendrogram\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 13. Dendrogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.5 * max(linked[:, 2])\n",
    ")\n",
    "plt.title(\"Dendrogram Based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select only the metrics of interest\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "print(\"Selected variables:\", dependent_vars)\n",
    "\n",
    "# 2. Exclude data where relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Check and remove highly correlated columns\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]  # Threshold: 0.95\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Verify variables in the DataFrame\n",
    "missing_vars = [var for var in dependent_vars if var not in df_result.columns]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"The following dependent variables are missing in the DataFrame: {missing_vars}\")\n",
    "if 'relabeled' not in df_result.columns:\n",
    "    raise ValueError(\"The 'relabeled' column is not present in the DataFrame.\")\n",
    "\n",
    "# 6. MANOVA analysis\n",
    "formula = f\"{' + '.join(dependent_vars)} ~ relabeled\"\n",
    "try:\n",
    "    manova = MANOVA.from_formula(formula, data=df_result)\n",
    "    print(manova.mv_test())\n",
    "except Exception as e:\n",
    "    print(f\"Error in MANOVA: {e}\")\n",
    "\n",
    "# 7. Group by 'relabeled' and compute the mean of each metric\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# 8. Feature normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 9. Compute Mahalanobis distance\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Pseudo-inverse for numerical stability\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 10. Square distance matrix\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 11. Agglomerative hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "\n",
    "# 12. Normalize distances for the dendrogram\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 13. Dendrogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "dendro = sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.4 * max(linked[:, 2])\n",
    ")\n",
    "\n",
    "cluster_colors = dendro['leaves_color_list']\n",
    "leaf_order = dendro['leaves']  # Orden de las hojas en el dendrograma\n",
    "data_color_map = {leaf_order[i]: cluster_colors[i] for i in range(len(leaf_order))}\n",
    "print(\"ndice de los datos originales y su color asignado:\")\n",
    "for index, color in sorted(data_color_map.items()):\n",
    "    print(f\"Dato {index}: Color {color}\")\n",
    "print(\"Colores usados en el threshold:\", cluster_colors)\n",
    "\n",
    "plt.title(\"Dendrogram Based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship between RMS and MNF for grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Associate colors with the relabeled values\n",
    "leaf_labels = [int(label) for label in dendro['ivl']]  # Convert labels to integers\n",
    "color_dict = {leaf_labels[i]: dendro['leaves_color_list'][i] for i in range(len(leaf_labels))}\n",
    "\n",
    "# 2. Prepare the PCA data\n",
    "df_plot = grouped[['RMS mean', 'MNF mean', 'RMS std', 'MNF std']].copy()\n",
    "df_plot['relabeled'] = grouped.index.astype(int)  # Ensure they are integers\n",
    "\n",
    "# 3. Create the PCA plot with dendrogram colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for label in df_plot['relabeled'].unique():\n",
    "    subset = df_plot[df_plot['relabeled'] == label]\n",
    "    mean_x, mean_y = subset['RMS mean'].values[0], subset['MNF mean'].values[0]\n",
    "    \n",
    "    # Get color from the dictionary\n",
    "    color = color_dict.get(label, 'black')  # If not found, use black as fallback\n",
    "\n",
    "    # Plot points\n",
    "    plt.scatter(mean_x, mean_y, label=f'Relabeled {label}', color=color, edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('RMS Mean')\n",
    "plt.ylabel('MNF Mean')\n",
    "plt.title('Variability Between Movements')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMS and MNF PCA analysis for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select the metrics of interest\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "print(dependent_vars)\n",
    "\n",
    "# 2. Exclude data with relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Check and remove highly correlated columns\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Feature normalization\n",
    "scaler = StandardScaler()\n",
    "df_pca_scaled = scaler.fit_transform(df_result[dependent_vars])\n",
    "\n",
    "# 6. Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_pca_scaled)\n",
    "\n",
    "# 7. Create DataFrame with results\n",
    "df_pca_result = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "df_pca_result['relabeled'] = df_result['relabeled'].astype(int)\n",
    "\n",
    "# 8. Compute Mahalanobis distances\n",
    "cov_matrix = np.cov(df_pca_scaled, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(df_pca_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 9. Hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 11. Map dendrogram colors\n",
    "dendro_labels = [int(label) for label in dendro['ivl']]\n",
    "unique_labels = sorted(df_pca_result['relabeled'].unique())\n",
    "color_dict = {dendro_labels[i]: dendro['leaves_color_list'][i] for i in range(len(dendro_labels)) if dendro_labels[i] in unique_labels}\n",
    "\n",
    "# 12. Plot PCA with dendrogram colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    subset = df_pca_result[df_pca_result['relabeled'] == label]\n",
    "    color = color_dict.get(label, plt.cm.tab10(label % 10))\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=f'Relabeled {label}', color=color, edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA of Movements with Dendrogram Colors')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select the metrics of interest\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "\n",
    "# 2. Exclude data with relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Check and remove highly correlated columns\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Feature normalization\n",
    "scaler = StandardScaler()\n",
    "df_pca_scaled = scaler.fit_transform(df_result[dependent_vars])\n",
    "\n",
    "# 6. Apply PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df_pca_scaled)\n",
    "\n",
    "# 7. Create DataFrame with results\n",
    "df_pca_result = pd.DataFrame(pca_result, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_result['relabeled'] = df_result['relabeled'].astype(int)\n",
    "\n",
    "# 8. Compute Mahalanobis distances\n",
    "cov_matrix = np.cov(df_pca_scaled, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(df_pca_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 9. Hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 11. Map dendrogram colors\n",
    "dendro_labels = [int(label) for label in dendro['ivl']]\n",
    "unique_labels = sorted(df_pca_result['relabeled'].unique())\n",
    "color_dict = {\n",
    "    int(dendro['ivl'][i]): matplotlib.colors.to_hex(dendro['leaves_color_list'][i])\n",
    "    for i in range(len(dendro['ivl'])) if int(dendro['ivl'][i]) in unique_labels\n",
    "}\n",
    "\n",
    "# 12. Plot 3D interactive PCA with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "for label in unique_labels:\n",
    "    subset = df_pca_result[df_pca_result['relabeled'] == label]\n",
    "    color = color_dict.get(label, 'gray')\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=subset['PC1'],\n",
    "        y=subset['PC2'],\n",
    "        z=subset['PC3'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color=color, opacity=0.8),\n",
    "        name=f'Relabeled {label}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Interactive PCA of Movements with Dendrogram Colors',\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each feature in each principal component\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # Transpose to have features as rows and components as columns\n",
    "    index=dependent_vars,  # Feature names\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)]  # Component names\n",
    ")\n",
    "\n",
    "# Display the weights\n",
    "print(\"Feature weights in the principal components:\")\n",
    "display(pca_loadings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each feature in each principal component\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # Transpose to have features as rows and components as columns\n",
    "    index=dependent_vars,  # Feature names\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)]  # Component names\n",
    ")\n",
    "\n",
    "# Calculate the total weight of each feature\n",
    "explained_variance = pca.explained_variance_ratio_  # Explained variance for each component\n",
    "feature_weights = (pca_loadings**2) @ explained_variance  # Weight loadings by explained variance and sum\n",
    "\n",
    "# Create a DataFrame with the total weights\n",
    "feature_weights_df = pd.DataFrame(feature_weights, columns=['Total Weight'])\n",
    "feature_weights_df = feature_weights_df.sort_values(by='Total Weight', ascending=False)  # Sort from highest to lowest\n",
    "\n",
    "# Display results\n",
    "print(\"Total weights of each feature in PCA:\")\n",
    "display(feature_weights_df)\n",
    "\n",
    "# Plot the total weights\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_weights_df.plot(kind='bar', figsize=(12, 6), cmap='viridis', edgecolor='black', legend=False)\n",
    "plt.title(\"Total Weight of Each Feature in PCA\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Total Weight\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship between RMS and MAV for grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Associate colors with relabeled values\n",
    "leaf_labels = [int(label) for label in dendro['ivl']]  # Convert labels to integers\n",
    "color_dict = {leaf_labels[i]: dendro['leaves_color_list'][i] for i in range(len(leaf_labels))}\n",
    "\n",
    "# 2. Prepare PCA data\n",
    "df_plot = df_result[['RMS mean', 'MAV mean', 'RMS std', 'MAV std']].copy()\n",
    "\n",
    "# Ensure the correct 'relabeled' column is used if it exists in df_result\n",
    "if 'relabeled' in df_result.columns:\n",
    "    df_plot['relabeled'] = df_result['relabeled'].astype(int)\n",
    "else:\n",
    "    raise KeyError(\"The 'relabeled' column is not found in df_result\")\n",
    "\n",
    "# 3. Create the PCA plot with dendrogram colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Iterate over each point and plot it with its corresponding color\n",
    "for i in range(len(df_plot)):\n",
    "    label = df_plot.iloc[i]['relabeled']\n",
    "    mean_x, mean_y = df_plot.iloc[i]['RMS mean'], df_plot.iloc[i]['MAV mean']\n",
    "    \n",
    "    # Get color from dictionary with fallback to 'black'\n",
    "    color = color_dict.get(label, 'black')\n",
    "\n",
    "    # Plot individual points\n",
    "    plt.scatter(mean_x, mean_y, color=color, edgecolor='black', s=100)\n",
    "\n",
    "legend_labels = {label: color_dict.get(label, 'black') for label in df_plot['relabeled'].unique()}\n",
    "\n",
    "for label, color in legend_labels.items():\n",
    "    plt.scatter([], [], color=color, label=f'Relabeled {label}', edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('RMS Mean')\n",
    "plt.ylabel('MAV Mean')\n",
    "plt.title('Variability Between Movements')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Exclude data with relabeled == 0\n",
    "dependent_vars = [col for col in df_result if col in df_result.columns and np.issubdtype(df_result[col].dtype, np.number)]\n",
    "\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Check and remove highly correlated columns\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Feature normalization\n",
    "scaler = StandardScaler()\n",
    "df_pca_scaled = scaler.fit_transform(df_result[dependent_vars])\n",
    "\n",
    "# 6. Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_pca_scaled)\n",
    "\n",
    "# 7. Create DataFrame with results\n",
    "df_pca_result = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "df_pca_result['relabeled'] = df_result['relabeled'].astype(int)\n",
    "\n",
    "# 8. Compute Mahalanobis distances\n",
    "cov_matrix = np.cov(df_pca_scaled, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(df_pca_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 9. Hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 11. Map dendrogram colors\n",
    "dendro_labels = [int(label) for label in dendro['ivl']]\n",
    "unique_labels = sorted(df_pca_result['relabeled'].unique())\n",
    "color_dict = {dendro_labels[i]: dendro['leaves_color_list'][i] for i in range(len(dendro_labels)) if dendro_labels[i] in unique_labels}\n",
    "\n",
    "# 12. Plot PCA with dendrogram colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    subset = df_pca_result[df_pca_result['relabeled'] == label]\n",
    "    color = color_dict.get(label, plt.cm.tab10(label % 10))\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=f'Relabeled {label}', color=color, edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA of Movements with Dendrogram Colors')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each feature in each principal component\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # Transpose to have features as rows and components as columns\n",
    "    index=dependent_vars,  # Feature names\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)]  # Component names\n",
    ")\n",
    "\n",
    "# Display the weights\n",
    "print(\"Feature weights in principal components:\")\n",
    "display(pca_loadings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loadings of the first three principal components\n",
    "plt.figure(figsize=(10, 6))\n",
    "pca_loadings.plot(kind='bar', figsize=(12, 6), cmap='viridis', edgecolor='black')\n",
    "plt.title(\"Feature Weights in Principal Components\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Loading\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title=\"Principal Components\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each feature in each principal component\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # Transpose to have features in rows and components in columns\n",
    "    index=dependent_vars,  # Feature names\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)]  # Component names\n",
    ")\n",
    "\n",
    "# Calculate the total weight of each feature\n",
    "explained_variance = pca.explained_variance_ratio_  # Variance explained by each component\n",
    "feature_weights = (pca_loadings**2) @ explained_variance  # Weight loadings by explained variance and sum\n",
    "\n",
    "# Create a DataFrame with the total weights\n",
    "feature_weights_df = pd.DataFrame(feature_weights, columns=['Total Weight'])\n",
    "feature_weights_df = feature_weights_df.sort_values(by='Total Weight', ascending=False)  # Sort from highest to lowest\n",
    "\n",
    "# Display results\n",
    "print(\"Total weights of each feature in PCA:\")\n",
    "display(feature_weights_df)\n",
    "\n",
    "# Plot the total weights\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_weights_df.plot(kind='bar', figsize=(12, 6), cmap='viridis', edgecolor='black', legend=False)\n",
    "plt.title(\"Total Weight of Each Feature in PCA\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Total Weight\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal complete without windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specific 'relabeled' values we want to filter\n",
    "filtered_labels = [55, 2, 4, 14, 10, 16, 17, 19, 32]\n",
    "\n",
    "# Filter the grouped DataFrame\n",
    "dataframe_windowing = grouped.loc[filtered_labels]\n",
    "\n",
    "dataframe_windowing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with enveloped with selection relabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the combined DataFrame\n",
    "dataframe_windowing = combined_df[combined_df['relabeled'].isin(filtered_labels)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(dataframe_windowing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowed of 100ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling rate in Hz \n",
    "sampling_rate = 2000  # For example, 200 Hz means 200 samples per second\n",
    "\n",
    "# Calculate the window size in number of samples\n",
    "window_size = int(0.1 * sampling_rate)  # 100 ms = 0.1 seconds\n",
    "\n",
    "# List to store the windows\n",
    "windowed_data = []\n",
    "\n",
    "# Apply windowing to each 'relabeled' group\n",
    "for label, group in dataframe_windowing.groupby('relabeled'):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = group.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Apply windowing with a sliding window\n",
    "    for i in range(0, len(numeric_cols) - window_size + 1, window_size):\n",
    "        window = numeric_cols.iloc[i:i + window_size]  # Extract the window\n",
    "        window_mean = window.mean()  # Compute the mean of the window\n",
    "        \n",
    "        # Add the 'relabeled' column and other categorical data if necessary\n",
    "        window_mean['relabeled'] = label  # Keep the label\n",
    "        windowed_data.append(window_mean)\n",
    "\n",
    "# Convert the list into a DataFrame\n",
    "dataframe_windowing_100 = pd.DataFrame(windowed_data)\n",
    "\n",
    "# Display the windowed DataFrame\n",
    "display(dataframe_windowing_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (relabeled), group in dataframe_windowing_100.groupby(['relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                #\"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df_windowing_100 = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df_windowing_100 = metrics_df_windowing_100[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df_windowing_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name\n",
    "database = 'DB4'\n",
    "\n",
    "# Full path to the database folder\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "\n",
    "# List of subjects, generating names from 's1' to 's10'\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# Iterate over each subject\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3 for the current subject\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Verify the structure of the loaded dictionary\n",
    "        print(f\"Keys in mat_data: {mat_data.keys()}\")\n",
    "                \n",
    "                # Attempt to process the file with src.build_dataframe\n",
    "        try:\n",
    "            test_df, grasps = src.build_dataframe(\n",
    "                mat_file=mat_data,\n",
    "                database=database,\n",
    "                filename=file_name,\n",
    "                rectify=False,\n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "            for grasp  in grasps:\n",
    "            # Apply envelope extraction\n",
    "                emg_columns = [col for col in test_df.columns if \"Channel\" in col]\n",
    "                envelope_df = get_envelope(test_df[emg_columns], envelope_type=1)  # Change type as needed\n",
    "                \n",
    "                # Preserve non-EMG columns\n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                \n",
    "                # Add a column with the subject name (folder) to the DataFrame\n",
    "                result_df['subject'] = folder \n",
    "\n",
    "                src.plot_emg_dataframe(database, result_df, grasp, length=0.6) #Using the relabeled grasp number and all default parameters \n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import src  # Asegrate de que src contiene todas las funciones necesarias\n",
    "#from src.utils import get_envelope\n",
    "\n",
    "database = 'DB4'\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# Extraer los valores reales de 'relabeled' en los datos\n",
    "actual_labels = set()\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "            print(f\"Keys in mat_data: {mat_data.keys()}\")\n",
    "            \n",
    "            test_df, grasps = src.build_dataframe(\n",
    "                mat_file=mat_data,\n",
    "                database=database,\n",
    "                filename=filename,\n",
    "                rectify=False,\n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "            if test_df is None or test_df.empty:\n",
    "                print(f\"Warning: DataFrame is empty after loading {filename}\")\n",
    "                continue\n",
    "            \n",
    "            actual_labels.update(test_df['relabeled'].unique())\n",
    "            print(f\"Available relabeled values: {sorted(actual_labels)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Definir filtered_labels con base en los valores reales en los datos\n",
    "filtered_labels = sorted(actual_labels.intersection([55, 2, 4, 14, 10, 16, 17, 19, 32]))\n",
    "print(f\"Filtered labels used for processing: {filtered_labels}\")\n",
    "\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "            test_df, grasps = src.build_dataframe(\n",
    "                mat_file=mat_data,\n",
    "                database=database,\n",
    "                filename=filename,\n",
    "                rectify=False,\n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "            if test_df is None or test_df.empty:\n",
    "                continue\n",
    "            \n",
    "            test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "            display(test_df)\n",
    "            if test_df.empty:\n",
    "                continue\n",
    "            \n",
    "            for grasp in grasps:\n",
    "                if grasp not in filtered_labels:\n",
    "                    continue\n",
    "                \n",
    "                emg_columns = [col for col in test_df.columns if \"Channel\" in col]\n",
    "                \n",
    "                envelope_df = get_envelope(test_df[emg_columns], envelope_type=1)\n",
    "                \n",
    "                meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                \n",
    "                result_df['subject'] = subject  # Usar 'subject' en lugar de 'folder'\n",
    "                \n",
    "                if result_df.empty:\n",
    "                    continue\n",
    "                \n",
    "                filtered_emg_data = src.db_utils.filter_data_pandas(result_df, grasp, include_rest=True, padding=0)\n",
    "                \n",
    "                if filtered_emg_data is None or filtered_emg_data.empty:\n",
    "                    print(f\"Warning: No data found for the chosen number: {grasp} in {filename}\")\n",
    "                    continue\n",
    "                print(f\"Plotting data for grasp {grasp}, size: {filtered_emg_data.shape}\")\n",
    "\n",
    "                src.plot_emg_dataframe(database, filtered_emg_data, grasp, length=0.6)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "\n",
    "# Parmetros\n",
    "database = 'DB4'\n",
    "window_size = 100  # Ventana en ms\n",
    "\n",
    "# Ruta de la base de datos\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# Funcin para calcular la envolvente con ventana rectangular\n",
    "def compute_envelope(signal, fs, window_ms=100):\n",
    "    window_samples = int((window_ms / 1000) * fs)  # Convertir ms a muestras\n",
    "    return np.convolve(np.abs(signal), np.ones(window_samples)/window_samples, mode='same')\n",
    "\n",
    "# Iterar sobre cada sujeto y ejercicio\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Cargar datos\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        try:\n",
    "            test_df, grasps = src.build_dataframe(\n",
    "                mat_file=mat_data,\n",
    "                database=database,\n",
    "                filename=filename,\n",
    "                rectify=False,   # No rectificar aqu, lo haremos en compute_envelope\n",
    "                normalize=True\n",
    "            )\n",
    "\n",
    "            fs = 2000  # Frecuencia de muestreo en Hz (ajustar segn corresponda)\n",
    "            emg_columns = [col for col in test_df.columns if \"Channel\" in col]\n",
    "\n",
    "            # Aplicar la envolvente con ventana rectangular\n",
    "            envelope_df = test_df[emg_columns].apply(lambda x: compute_envelope(x, fs, window_ms=window_size))\n",
    "\n",
    "            # Conservar columnas de metadatos\n",
    "            meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "            result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "\n",
    "            # Graficar solo la envolvente\n",
    "            src.plot_emg_dataframe(database, result_df, grasp, length=0.6)\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowed 200ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling rate in Hz \n",
    "sampling_rate = 2000  # For example, 200 Hz means 200 samples per second\n",
    "\n",
    "# Calculate the window size in number of samples\n",
    "window_size = int(0.2 * sampling_rate)  # 200 ms = 0.2 seconds\n",
    "\n",
    "# List to store the windows\n",
    "windowed_data = []\n",
    "\n",
    "# Apply windowing to each 'relabeled' group\n",
    "for label, group in dataframe_windowing.groupby('relabeled'):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = group.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Apply windowing using a sliding window\n",
    "    for i in range(0, len(numeric_cols) - window_size + 1, window_size):\n",
    "        window = numeric_cols.iloc[i:i + window_size]  # Extract the window\n",
    "        window_mean = window.mean()  # Compute the mean of the window\n",
    "        \n",
    "        # Add the 'relabeled' column and other categorical data if necessary\n",
    "        window_mean['relabeled'] = label  # Keep the label\n",
    "        windowed_data.append(window_mean)\n",
    "\n",
    "# Convert the list into a DataFrame\n",
    "dataframe_windowing_200 = pd.DataFrame(windowed_data)\n",
    "\n",
    "# Display the windowed DataFrame\n",
    "display(dataframe_windowing_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling rate in Hz\n",
    "sampling_rate = 2000  # Adjust according to the data\n",
    "window_size = int(0.2 * sampling_rate)  # 200 ms = 400 samples\n",
    "step_size = window_size // 1  # without overlap\n",
    "\n",
    "# Database name\n",
    "database = 'DB4'\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# Filter specific movements\n",
    "filtered_labels = [55, 2, 4, 14, 10, 16, 17, 19, 32]\n",
    "\n",
    "# Verify the existence of required functions in src\n",
    "if not all(hasattr(src, func) for func in [\"loadmatNina\", \"build_dataframe\", \"plot_emg_data\"]):\n",
    "    raise ImportError(\"Error: The required functions are not defined in the 'src' module.\")\n",
    "\n",
    "# Iterate over each subject\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "\n",
    "    # Iterate over exercise files\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "        # Load .mat file data with error handling\n",
    "        try:\n",
    "            mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        if 'emg' not in mat_data:\n",
    "            print(f\"    Error: Key 'emg' not found in {filename}. Available keys: {mat_data.keys()}\")\n",
    "            continue\n",
    "\n",
    "        # Build DataFrame\n",
    "        try:\n",
    "            test_df, grasps_etiquetados = src.build_dataframe(\n",
    "                mat_file=mat_data,\n",
    "                database=database,\n",
    "                filename=filename,\n",
    "                rectify=False,\n",
    "                normalize=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error building DataFrame in {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Filter only the movements of interest\n",
    "        test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "\n",
    "        # Apply corrected windowing (200 ms with 50% overlap)\n",
    "        windowed_data = []\n",
    "        for label, group in test_df.groupby('relabeled'):\n",
    "            numeric_cols = group.select_dtypes(include=['number'])\n",
    "            num_rows = len(numeric_cols)\n",
    "\n",
    "            for i in range(0, num_rows - window_size + 1, step_size):\n",
    "                window = numeric_cols.iloc[i:i + window_size]\n",
    "                \n",
    "                if len(window) < window_size:\n",
    "                    continue  # Avoid incomplete windows in the middle of the process\n",
    "\n",
    "                window_mean = window.mean()\n",
    "                window_mean['relabeled'] = label\n",
    "                windowed_data.append(window_mean)\n",
    "\n",
    "        dataframe_windowing_200 = pd.DataFrame(windowed_data).dropna()\n",
    "\n",
    "        # Plot EMG signals (only first 200 ms)\n",
    "        for grasp in grasps_etiquetados:\n",
    "            if grasp not in filtered_labels:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Select only the first 200 ms (400 samples)\n",
    "                emg_signal = mat_data['emg'][:window_size, :]  # Take the first 400 samples\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing grasp {grasp}: {str(e)}\")\n",
    "\n",
    "            src.plot_emg_windowed(database=database, mat_file=mat_data, grasp_number=grasp, windowing=0.2, interactive=False, time=True, include_rest=False, padding=10, use_stimulus=False, addFourier=False, title=f\"{filename} - Grasp {grasp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (relabeled), group in dataframe_windowing_200.groupby(['relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                #\"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df_windowing_200 = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df_windowing_200 = metrics_df_windowing_100[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df_windowing_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowed 300ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling rate in Hz\n",
    "sampling_rate = 2000  # For example, 200 Hz means 200 samples per second\n",
    "\n",
    "# Calculate the window size in number of samples\n",
    "window_size = int(0.3 * sampling_rate)  # 300 ms = 0.3 seconds\n",
    "\n",
    "# List to store the windowed data\n",
    "windowed_data = []\n",
    "\n",
    "# Apply windowing to each 'relabeled' group\n",
    "for label, group in dataframe_windowing.groupby('relabeled'):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = group.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Apply sliding window\n",
    "    for i in range(0, len(numeric_cols) - window_size + 1, window_size):\n",
    "        window = numeric_cols.iloc[i:i + window_size]  # Extract the window\n",
    "        window_mean = window.mean()  # Compute the window mean\n",
    "        \n",
    "        # Add the 'relabeled' column and other categorical data if necessary\n",
    "        window_mean['relabeled'] = label  # Keep the label\n",
    "        windowed_data.append(window_mean)\n",
    "\n",
    "# Convert the list into a DataFrame\n",
    "dataframe_windowing_300 = pd.DataFrame(windowed_data)\n",
    "\n",
    "# Display the windowed DataFrame\n",
    "display(dataframe_windowing_300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (relabeled), group in dataframe_windowing_300.groupby(['relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                #\"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df_windowing_300 = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df_windowing_300 = metrics_df_windowing_100[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df_windowing_300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
