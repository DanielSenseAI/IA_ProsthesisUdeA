{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libreries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat, whosmat\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "import src\n",
    "from src import config, loadmatNina\n",
    "import pywt\n",
    "\n",
    "from src.preprocessing_utils import get_envelope\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.spatial.distance as ssd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the database to analyze\n",
    "database = 'DB4'\n",
    "\n",
    "data_path = f'data/{database}'\n",
    "\n",
    "# Find the folder named with the convention s + \"number\"\n",
    "folder = None\n",
    "for item in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', item) or re.match(r'Subject\\d+', item):\n",
    "        folder = item\n",
    "        break\n",
    "\n",
    "if folder:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all .mat files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.mat'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            info = whosmat(file_path)\n",
    "            results.append((file_name, info))\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    data = {}\n",
    "    for file_name, info in results:\n",
    "        for item in info:\n",
    "            if item[0] not in data:\n",
    "                data[item[0]] = {}\n",
    "            data[item[0]][file_name] = item[1:]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    df.columns.name = 'File Name'\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"No folder found with the convention s + 'number'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For complete signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics(signal, fs=1000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    # Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # Variance (VAR)\n",
    "    var = np.var(signal)\n",
    "    \n",
    "    # Coefficient of Variation (CoV)\n",
    "    mean_signal = np.mean(signal)\n",
    "    cov = (np.std(signal) / mean_signal) if mean_signal != 0 else 0\n",
    "    \n",
    "    # Mean Frequency (MNF)\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "    fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "    mnf = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    \n",
    "    # Marginal Discrete Wavelet Transform (mDWT)\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "    mdwt = np.sum([np.sum(np.abs(c)) for c in coeffs])\n",
    "    \n",
    "    # Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"RMS\": rms,\n",
    "        \"WL\": wl,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"VAR\": var,\n",
    "        \"CoV\": cov,\n",
    "        \"MNF\": mnf,\n",
    "        \"mDWT\": mdwt,\n",
    "        \"TD\": td,\n",
    "        \"MAVS\": mavs\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For signal with means and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_std(signal, fs=1000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal, including mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:\n",
    "        metrics_per_channel = [calculate_emg_metrics(signal[:, ch], fs) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    mav_std = np.std(np.abs(signal))\n",
    "    \n",
    "    # Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    iav_std = np.std(np.abs(signal))\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    rms_std = np.std(signal)\n",
    "    \n",
    "    # Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    wl_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    zc_std = np.std(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    ssc_std = np.std((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # Variance (VAR)\n",
    "    var = np.var(signal)\n",
    "    var_std = np.std(signal)\n",
    "    \n",
    "    # Coefficient of Variation (CoV)\n",
    "    mean_signal = np.mean(signal)\n",
    "    cov = (np.std(signal) / mean_signal) if mean_signal != 0 else 0\n",
    "    cov_std = np.std(cov)\n",
    "    \n",
    "    # Mean Frequency (MNF)\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "    fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "    mnf = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    mnf_std = np.std(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    \n",
    "    # Marginal Discrete Wavelet Transform (mDWT)\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "    mdwt = np.sum([np.sum(np.abs(c)) for c in coeffs])\n",
    "    mdwt_std = np.std([np.sum(np.abs(c)) for c in coeffs])\n",
    "    \n",
    "    # Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    td_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    mavs_std = np.std(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav, \"MAV_STD\": mav_std,\n",
    "        \"IAV\": iav, \"IAV_STD\": iav_std,\n",
    "        \"RMS\": rms, \"RMS_STD\": rms_std,\n",
    "        \"WL\": wl, \"WL_STD\": wl_std,\n",
    "        \"ZC\": zc, \"ZC_STD\": zc_std,\n",
    "        \"SSC\": ssc, \"SSC_STD\": ssc_std,\n",
    "        \"VAR\": var, \"VAR_STD\": var_std,\n",
    "        \"CoV\": cov, \"CoV_STD\": cov_std,\n",
    "        \"MNF\": mnf, \"MNF_STD\": mnf_std,\n",
    "        \"mDWT\": mdwt, \"mDWT_STD\": mdwt_std,\n",
    "        \"TD\": td, \"TD_STD\": td_std,\n",
    "        \"MAVS\": mavs, \"MAVS_STD\": mavs_std\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This functions calculate the metrics for channel and average the values for a complete result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics_means(signal):\n",
    "    \"\"\"\n",
    "    Calculates the metrics of an EMG signal. If there are multiple channels, it computes \n",
    "    the metrics for each channel and then averages the results.\n",
    "    \"\"\"\n",
    "    if signal.ndim == 2:  # If the signal has multiple channels\n",
    "        metrics_per_channel = [calculate_emg_metrics_means(signal[:, ch]) for ch in range(signal.shape[1])]\n",
    "        averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "        return averaged_metrics\n",
    "    \n",
    "    # Mean Absolute Value (MAV)\n",
    "    mav = np.mean(np.abs(signal))\n",
    "    \n",
    "    # Integrated Absolute Value (IAV)\n",
    "    iav = np.sum(np.abs(signal))\n",
    "    \n",
    "    # Root Mean Square (RMS)\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    \n",
    "    # Waveform Length (WL)\n",
    "    wl = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Zero Crossings (ZC)\n",
    "    zc = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "    \n",
    "    # Slope Sign Changes (SSC)\n",
    "    diff_signal = np.diff(signal)\n",
    "    ssc = np.sum((diff_signal[1:] * diff_signal[:-1]) < 0)\n",
    "    \n",
    "    # Variance (VAR)\n",
    "    var = np.var(signal)\n",
    "    \n",
    "    # Coefficient of Variation (CoV)\n",
    "    mean_signal = np.mean(signal)\n",
    "    cov = (np.std(signal) / mean_signal) if mean_signal != 0 else 0\n",
    "    \n",
    "    # Mean Frequency (MNF)\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "    fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "    mnf = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude)\n",
    "    \n",
    "    # Marginal Discrete Wavelet Transform (mDWT)\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "    mdwt = np.sum([np.sum(np.abs(c)) for c in coeffs])\n",
    "    \n",
    "    # Temporal Difference (TD)\n",
    "    td = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Mean Absolute Value Slope (MAVS)\n",
    "    mavs = np.mean(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    metrics = {\n",
    "        \"MAV\": mav,\n",
    "        \"IAV\": iav,\n",
    "        \"RMS\": rms,\n",
    "        \"WL\": wl,\n",
    "        \"ZC\": zc,\n",
    "        \"SSC\": ssc,\n",
    "        \"VAR\": var,\n",
    "        \"CoV\": cov,\n",
    "        \"MNF\": mnf,\n",
    "        \"mDWT\": mdwt,\n",
    "        \"TD\": td,\n",
    "        \"MAVS\": mavs\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots and metrics for complete grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name\n",
    "database = 'DB4'\n",
    "\n",
    "# Full path to the database folder\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "\n",
    "# List of subjects, generating names from 's1' to 's10'\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# Iterate over each subject\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3 for the current subject\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Verify the structure of the loaded dictionary\n",
    "        print(f\"Keys in mat_data: {mat_data.keys()}\")\n",
    "        \n",
    "        # Retrieve re-labeled data and the list of labeled grasps\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over each labeled grasp\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Check if 'emg' key exists in mat_data\n",
    "                if 'emg' not in mat_data:\n",
    "                    raise KeyError(f\"The key 'emg' is not in mat_data. Available keys: {mat_data.keys()}\")\n",
    "                \n",
    "                # Get the EMG signal for the labeled grasp\n",
    "                emg_signal = mat_data['emg'][grasp]  # Adjust based on the actual structure\n",
    "                \n",
    "                # Compute EMG signal metrics\n",
    "                metrics = calculate_emg_metrics(emg_signal)\n",
    "                \n",
    "                # Print computed metrics\n",
    "                print(f\"\\nMetrics for Grasp {grasp}:\")\n",
    "                for key, value in metrics.items():\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "                \n",
    "                # Plot the EMG signal for the grasp\n",
    "                src.plot_emg_data(\n",
    "                    database=database,\n",
    "                    mat_file=mat_data,\n",
    "                    grasp_number=grasp,\n",
    "                    interactive=False,\n",
    "                    include_rest=True,\n",
    "                    use_stimulus=False,\n",
    "                    addFourier=False,\n",
    "                    padding=100,\n",
    "                    title=f\"{filename} - Grasp {grasp}\"\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                print(f\"    Error: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing grasp {grasp}: {str(e)}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with metrics for a complete signal without discriminating by channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database name\n",
    "database = 'DB4'\n",
    "\n",
    "# Full path to the database folder\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "\n",
    "# List of subjects, generating names from 's1' to 's10'\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# List to store all extracted metrics\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject in the database\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists before processing\n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Skip if file is not available\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Build DataFrame with re-labeled data\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over labeled grasps\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Retrieve the corresponding EMG signal\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                \n",
    "                # Compute EMG signal metrics\n",
    "                metrics = calculate_emg_metrics(emg_signal)\n",
    "                \n",
    "                # Append metrics with metadata to the list\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Unpack metrics into the dictionary\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Create a DataFrame with organized metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns (optional) for better visualization\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Print the final DataFrame with extracted metrics\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with average of metrics for channels in each grasp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all computed metrics\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject in the database\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterate over exercise files E1, E2, and E3\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        # Check if the file exists before processing\n",
    "        if not os.path.exists(file_path):\n",
    "            continue  # Skip if file is not available\n",
    "        \n",
    "        # Load data from the .mat file\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        \n",
    "        # Build DataFrame with re-labeled data\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Iterate over labeled grasps\n",
    "        for grasp in grasps_etiquetados:\n",
    "            try:\n",
    "                # Retrieve the corresponding EMG signal\n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                \n",
    "                # Compute EMG signal metrics using standard deviation\n",
    "                metrics = calculate_emg_metrics_std(emg_signal)\n",
    "                \n",
    "                # Append metrics with metadata to the list\n",
    "                metrics_data.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"exercise\": exercise,\n",
    "                    \"filename\": filename,\n",
    "                    \"grasp\": grasp,\n",
    "                    **metrics  # Unpack metrics into the dictionary\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename} - Grasp {grasp}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "# Create a DataFrame with organized metrics\n",
    "metrics_df_std = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns (optional) for better visualization\n",
    "column_order = [\"subject\", \"exercise\", \"filename\", \"grasp\"] + list(metrics.keys())\n",
    "metrics_df_std = metrics_df_std[column_order]\n",
    "\n",
    "# Print the final DataFrame with extracted metrics\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "display(metrics_df_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe for every channels of data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all generated DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Look for folders matching the pattern \"s + number\" or \"Subject + number\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterate over all .mat files in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Attempt to load the .mat file\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Attempt to process the file with src.build_dataframe\n",
    "                try:\n",
    "                    test_df, grasps = src.build_dataframe(\n",
    "                        mat_file=mat_data,\n",
    "                        database=database,\n",
    "                        filename=file_name,\n",
    "                        rectify=False,\n",
    "                        normalize=True\n",
    "                    )\n",
    "                    \n",
    "                    # Add a column with the subject name (folder) to the DataFrame\n",
    "                    test_df['subject'] = folder  \n",
    "                    \n",
    "                    # Append the processed DataFrame to the list\n",
    "                    all_dataframes.append(test_df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Concatenate all DataFrames into a single one if data is available\n",
    "if all_dataframes:  \n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Display the combined DataFrame\n",
    "    print(\"\\n Combined DataFrame:\")\n",
    "    display(combined_df)  \n",
    "\n",
    "else:\n",
    "    print(\"Warning: No DataFrames were generated. Check the input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all generated DataFrames\n",
    "all_dataframes = []\n",
    "\n",
    "# Look for folders matching the pattern \"s + number\" or \"Subject + number\"\n",
    "for folder in os.listdir(data_path):\n",
    "    if re.match(r'[sS]\\d+', folder) or re.match(r'Subject\\d+', folder):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        \n",
    "        # Iterate over all .mat files in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.mat'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                # Attempt to load the .mat file\n",
    "                try:\n",
    "                    mat_data = src.loadmatNina(database, file_name, subject=folder)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Attempt to process the file with src.build_dataframe\n",
    "                try:\n",
    "                    test_df, grasps = src.build_dataframe(\n",
    "                        mat_file=mat_data,\n",
    "                        database=database,\n",
    "                        filename=file_name,\n",
    "                        rectify=False,\n",
    "                        normalize=True\n",
    "                    )\n",
    "                    \n",
    "                    # Apply envelope extraction\n",
    "                    emg_columns = [col for col in test_df.columns if \"Channel\" in col]\n",
    "                    envelope_df = get_envelope(test_df[emg_columns], envelope_type=1)  # Change type as needed\n",
    "                    \n",
    "                    # Preserve non-EMG columns\n",
    "                    meta_columns = [\"Time (s)\", \"subject\", \"re_repetition\", \"stimulus\", \"relabeled\"]\n",
    "                    result_df = pd.concat([envelope_df, test_df[meta_columns]], axis=1)\n",
    "                    \n",
    "                    # Add a column with the subject name (folder) to the DataFrame\n",
    "                    result_df['subject'] = folder  \n",
    "                    \n",
    "                    # Append the processed DataFrame to the list\n",
    "                    all_dataframes.append(result_df)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Concatenate all DataFrames into a single one if data is available\n",
    "if all_dataframes:  \n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Display the combined DataFrame\n",
    "    print(\"\\n Combined DataFrame:\")\n",
    "    display(combined_df)  \n",
    "\n",
    "else:\n",
    "    print(\"Warning: No DataFrames were generated. Check the input data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with metrics for channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emg_metrics(signal, fs=2000):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for an EMG signal, including mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - signal: NumPy array containing the EMG signal.\n",
    "    - fs: Sampling frequency in Hz (default: 1000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if signal.ndim == 2:\n",
    "            metrics_per_channel = [calculate_emg_metrics(signal[:, ch], fs) for ch in range(signal.shape[1])]\n",
    "            averaged_metrics = {key: np.mean([m[key] for m in metrics_per_channel]) for key in metrics_per_channel[0]}\n",
    "            return averaged_metrics\n",
    "        \n",
    "        abs_signal = np.abs(signal)\n",
    "        diff_signal = np.diff(signal)\n",
    "        diff_abs_signal = np.abs(diff_signal)\n",
    "        \n",
    "        # Compute Metrics\n",
    "        metrics = {\n",
    "            \"MAV\": np.mean(abs_signal), \"MAV_STD\": np.std(abs_signal),\n",
    "            \"IAV\": np.sum(abs_signal), \"IAV_STD\": np.std(abs_signal),\n",
    "            \"RMS\": np.sqrt(np.mean(signal**2)), \"RMS_STD\": np.std(signal),\n",
    "            \"WL\": np.sum(diff_abs_signal), \"WL_STD\": np.std(diff_abs_signal),\n",
    "            \"ZC\": np.sum(np.diff(np.sign(signal)) != 0), \"ZC_STD\": np.std(np.diff(np.sign(signal)) != 0),\n",
    "            \"SSC\": np.sum((diff_signal[1:] * diff_signal[:-1]) < 0), \"SSC_STD\": np.std((diff_signal[1:] * diff_signal[:-1]) < 0),\n",
    "            \"VAR\": np.var(signal), \"VAR_STD\": np.std(signal),\n",
    "            \"CoV\": (np.std(signal) / np.mean(signal)) if np.mean(signal) != 0 else 0,\n",
    "            \"TD\": np.sum(diff_abs_signal), \"TD_STD\": np.std(diff_abs_signal),\n",
    "            \"MAVS\": np.mean(diff_abs_signal), \"MAVS_STD\": np.std(diff_abs_signal),\n",
    "            \"MNP\": np.mean(signal**2), \"MNP_STD\": np.std(signal**2),\n",
    "        }\n",
    "        \n",
    "        # Spectral Metrics\n",
    "        freqs = np.fft.rfftfreq(len(signal), d=1/fs)\n",
    "        fft_magnitude = np.abs(np.fft.rfft(signal))\n",
    "        metrics[\"MNF\"] = np.sum(freqs * fft_magnitude) / np.sum(fft_magnitude) if np.sum(fft_magnitude) != 0 else 0\n",
    "        metrics[\"MNF_STD\"] = np.std(freqs * fft_magnitude) / np.sum(fft_magnitude) if np.sum(fft_magnitude) != 0 else 0\n",
    "        \n",
    "        # Wavelet Transform\n",
    "        coeffs = pywt.wavedec(signal, 'db4', level=4)\n",
    "        mdwt_values = np.array([np.sum(np.abs(c)) for c in coeffs])\n",
    "        metrics[\"mDWT\"] = np.sum(mdwt_values)\n",
    "        metrics[\"mDWT_STD\"] = np.std(mdwt_values)\n",
    "        \n",
    "        # Kurtosis\n",
    "        std_signal = np.std(signal)\n",
    "        metrics[\"Kurt\"] = np.mean((signal - np.mean(signal)) ** 4) / (std_signal ** 4) if std_signal != 0 else 0\n",
    "        metrics[\"Kurt_STD\"] = np.std(metrics[\"Kurt\"])\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_emg_metrics: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (subject, relabeled), group in combined_df.groupby(['subject', 'relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                \"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"subject\", \"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df = metrics_df[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'channel' column to group data by subject and movement type\n",
    "grouped_df = metrics_df.drop(columns=['channel'])\n",
    "\n",
    "# Compute the mean value of each metric grouped by subject and movement\n",
    "df_mean = grouped_df.groupby(['subject', 'relabeled']).mean()\n",
    "\n",
    "# Compute the standard deviation of each metric grouped by subject and movement\n",
    "df_std = grouped_df.groupby(['subject', 'relabeled']).std()\n",
    "\n",
    "# Rename columns to indicate they contain mean values\n",
    "df_mean.columns = [f\"{col} mean\" for col in df_mean.columns]\n",
    "\n",
    "# Rename columns to indicate they contain standard deviation values\n",
    "df_std.columns = [f\"{col} std\" for col in df_std.columns]\n",
    "\n",
    "# Merge the mean and standard deviation DataFrames into a single DataFrame\n",
    "df_result = df_mean.merge(df_std, on=['subject', 'relabeled']).reset_index()\n",
    "\n",
    "# Display the final DataFrame with aggregated metrics\n",
    "display(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendogram for grasp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns containing numerical features\n",
    "features = df_result.iloc[:, 2:]  # Exclude 'subject' and 'relabeled'\n",
    "\n",
    "# Normalize the data to improve comparability and avoid bias due to different scales\n",
    "df_scaled = StandardScaler().fit_transform(features)\n",
    "\n",
    "# Apply hierarchical clustering using the Ward method (minimizes variance within clusters)\n",
    "linked = sch.linkage(df_scaled, method='ward')\n",
    "\n",
    "# Create and visualize the dendrogram\n",
    "plt.figure(figsize=(20, 10))\n",
    "sch.dendrogram(\n",
    "    linked, \n",
    "    labels=df_result['relabeled'].values,  # Labels on the x-axis based on the 'relabeled' variable\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=8  # Adjust font size\n",
    ")\n",
    "plt.title(\"Dendrogram based on the 'relabeled' variable\")\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Euclidean Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'relabeled' and calculate the mean and standard deviation of each numerical feature\n",
    "grouped = df_result.select_dtypes(include=['number']).groupby(df_result['relabeled']).agg(['mean', 'std'])\n",
    "display(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'relabeled' and calculate the mean and standard deviation of each numerical feature\n",
    "grouped = df_result.select_dtypes(include=['number']).groupby(df_result['relabeled']).agg(['mean', 'std'])\n",
    "display(grouped)\n",
    "\n",
    "# Flatten column names to make them easier to work with\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "# Normalize the data to prevent magnitude differences from affecting the clustering distance\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# Apply hierarchical clustering using the Ward method (minimizes variance within clusters)\n",
    "linked = sch.linkage(scaled_features, method='ward')\n",
    "\n",
    "# Create and visualize the dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sch.dendrogram(\n",
    "    linked, \n",
    "    labels=grouped.index.tolist(),  # Labels on the x-axis based on the 'relabeled' variable\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=8  # Adjust font size\n",
    ")\n",
    "plt.title(\"Dendrogram based on mean and standard deviation per grasp type\")\n",
    "plt.xlabel(\"Grasps\")\n",
    "plt.ylabel(\"Euclidean Distance\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the average of metrics per channel\n",
    "# Exclude 'subject', 'relabeled', and 'channel' to keep only the metric columns\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "\n",
    "# Group by 'channel' and compute the mean of each metric\n",
    "average_metrics_df = metrics_df.groupby('channel')[metrics_columns].mean().reset_index()\n",
    "display(average_metrics_df)\n",
    "\n",
    "# 2. Prepare data for clustering\n",
    "X = average_metrics_df[metrics_columns].values  # Extract metric values as an array for clustering\n",
    "\n",
    "# 3. Compute the distance matrix and perform hierarchical clustering\n",
    "Z = linkage(X, method='ward')  # 'ward' minimizes variance within clusters\n",
    "\n",
    "# 4. Plot the dendrogram with adjustments for better visualization\n",
    "plt.figure(figsize=(15, 8)) \n",
    "plt.title('Dendrogram of EMG Channels (Average Metrics)', fontsize=16, pad=20)\n",
    "plt.xlabel('Channels', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "\n",
    "# Adjust the dendrogram to prevent overlapping labels\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=average_metrics_df['channel'].values,  # Labels for each channel\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=12,  # Adjust font size\n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Threshold to color clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust layout for better fit\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the mean and standard deviation of metrics per channel\n",
    "# Exclude 'subject', 'relabeled', and 'channel' to keep only numerical metric columns\n",
    "metrics_columns = [col for col in metrics_df.columns if col not in [\"subject\", \"relabeled\", \"channel\"]]\n",
    "\n",
    "# Group by 'channel' and compute the mean and standard deviation for each metric\n",
    "agg_metrics_df = metrics_df.groupby('channel')[metrics_columns].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Flatten column names for easier access (concatenating \"metric_type\")\n",
    "agg_metrics_df.columns = ['_'.join(col).strip('_') for col in agg_metrics_df.columns]\n",
    "\n",
    "display(agg_metrics_df)  # Display the aggregated metrics table\n",
    "\n",
    "# 2. Prepare data for clustering using only the metric averages\n",
    "X = agg_metrics_df[[col for col in agg_metrics_df.columns if col.endswith('_mean')]].values  # Extract only \"_mean\" columns\n",
    "\n",
    "# 3. Compute the distance matrix and perform hierarchical clustering\n",
    "Z = linkage(X, method='ward')  # 'ward' minimizes variance within clusters\n",
    "\n",
    "# 4. Plot the dendrogram with adjustments for better visualization\n",
    "plt.figure(figsize=(15, 8)) \n",
    "plt.title('Dendrogram of EMG Channels (Average Metrics)', fontsize=16, pad=20)\n",
    "plt.xlabel('Channels', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "\n",
    "# Adjust the dendrogram to prevent overlapping labels\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=agg_metrics_df['channel'].values,  # Labels for EMG channels\n",
    "    leaf_rotation=90,  # Rotate labels for better readability\n",
    "    leaf_font_size=12,  # Adjust font size\n",
    "    color_threshold=0.7 * max(Z[:, 2]),  # Threshold to color clusters\n",
    ")\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust layout for better fit\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the metrics of interest: RMS and MNF\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "print(dependent_vars)\n",
    "\n",
    "# Check and remove redundant columns due to high correlation\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# Verify variables in the DataFrame\n",
    "missing_vars = [var for var in dependent_vars if var not in df_result.columns]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"The following dependent variables are missing in the DataFrame: {missing_vars}\")\n",
    "if 'relabeled' not in df_result.columns:\n",
    "    raise ValueError(\"The 'relabeled' column is not present in the DataFrame.\")\n",
    "\n",
    "# MANOVA analysis\n",
    "formula = f\"{' + '.join(dependent_vars)} ~ relabeled\"\n",
    "try:\n",
    "    manova = MANOVA.from_formula(formula, data=df_result)\n",
    "    print(manova.mv_test())\n",
    "except Exception as e:\n",
    "    print(f\"Error in MANOVA: {e}\")\n",
    "\n",
    "# Group by 'relabeled' and compute the mean of each metric\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# Feature normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# Compute Mahalanobis distance\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# Square distance matrix\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# Agglomerative hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# Dendrogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.4 * max(linked[:, 2])\n",
    ")\n",
    "plt.title(\"Dendrogram Based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select only the metrics of interest\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "print(\"Selected variables:\", dependent_vars)\n",
    "\n",
    "# 2. Exclude data where relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Check and remove highly correlated columns\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]  # Threshold: 0.95\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Verify variables in the DataFrame\n",
    "missing_vars = [var for var in dependent_vars if var not in df_result.columns]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"The following dependent variables are missing in the DataFrame: {missing_vars}\")\n",
    "if 'relabeled' not in df_result.columns:\n",
    "    raise ValueError(\"The 'relabeled' column is not present in the DataFrame.\")\n",
    "\n",
    "# 6. MANOVA analysis\n",
    "formula = f\"{' + '.join(dependent_vars)} ~ relabeled\"\n",
    "try:\n",
    "    manova = MANOVA.from_formula(formula, data=df_result)\n",
    "    print(manova.mv_test())\n",
    "except Exception as e:\n",
    "    print(f\"Error in MANOVA: {e}\")\n",
    "\n",
    "# 7. Group by 'relabeled' and compute the mean of each metric\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# 8. Feature normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 9. Compute Mahalanobis distance\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Pseudo-inverse for numerical stability\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 10. Square distance matrix\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 11. Agglomerative hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "\n",
    "# 12. Normalize distances for the dendrogram\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 13. Dendrogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.4 * max(linked[:, 2])\n",
    ")\n",
    "plt.title(\"Dendrogram Based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select only the metrics of interest\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "print(\"Selected variables:\", dependent_vars)\n",
    "\n",
    "# 2. Exclude data where relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Check and remove highly correlated columns\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]  # Threshold: 0.95\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Handle missing values\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Verify variables in the DataFrame\n",
    "missing_vars = [var for var in dependent_vars if var not in df_result.columns]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"The following dependent variables are missing in the DataFrame: {missing_vars}\")\n",
    "if 'relabeled' not in df_result.columns:\n",
    "    raise ValueError(\"The 'relabeled' column is not present in the DataFrame.\")\n",
    "\n",
    "# 6. MANOVA analysis\n",
    "formula = f\"{' + '.join(dependent_vars)} ~ relabeled\"\n",
    "try:\n",
    "    manova = MANOVA.from_formula(formula, data=df_result)\n",
    "    print(manova.mv_test())\n",
    "except Exception as e:\n",
    "    print(f\"Error in MANOVA: {e}\")\n",
    "\n",
    "# 7. Group by 'relabeled' and compute the mean of each metric\n",
    "grouped = df_result[dependent_vars].groupby(df_result['relabeled']).mean()\n",
    "\n",
    "# 8. Feature normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(grouped)\n",
    "\n",
    "# 9. Compute Mahalanobis distance\n",
    "cov_matrix = np.cov(scaled_features, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Pseudo-inverse for numerical stability\n",
    "mahalanobis_distances = pdist(scaled_features, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "\n",
    "# 10. Square distance matrix\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 11. Agglomerative hierarchical clustering\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "\n",
    "# 12. Normalize distances for the dendrogram\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 13. Dendrogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "dendro = sch.dendrogram(\n",
    "    linked,\n",
    "    labels=grouped.index.tolist(),\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.4 * max(linked[:, 2])\n",
    ")\n",
    "\n",
    "cluster_colors = dendro['leaves_color_list']\n",
    "leaf_order = dendro['leaves']  # Orden de las hojas en el dendrograma\n",
    "data_color_map = {leaf_order[i]: cluster_colors[i] for i in range(len(leaf_order))}\n",
    "print(\"ndice de los datos originales y su color asignado:\")\n",
    "for index, color in sorted(data_color_map.items()):\n",
    "    print(f\"Dato {index}: Color {color}\")\n",
    "print(\"Colores usados en el threshold:\", cluster_colors)\n",
    "\n",
    "plt.title(\"Dendrogram Based on Normalized Mahalanobis Distance\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Grasps\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Mahalanobis Distance\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Asociar los colores a los valores de relabeled\n",
    "leaf_labels = [int(label) for label in dendro['ivl']]  # Convertimos etiquetas a enteros\n",
    "color_dict = {leaf_labels[i]: dendro['leaves_color_list'][i] for i in range(len(leaf_labels))}\n",
    "\n",
    "# 3. Preparar los datos del PCA\n",
    "df_plot = grouped[['RMS mean', 'MNF mean', 'RMS std', 'MNF std']].copy()\n",
    "df_plot['relabeled'] = grouped.index.astype(int)  # Asegurar que sean enteros\n",
    "\n",
    "# 4. Crear el grfico PCA con los colores del dendrograma\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for label in df_plot['relabeled'].unique():\n",
    "    subset = df_plot[df_plot['relabeled'] == label]\n",
    "    mean_x, mean_y = subset['RMS mean'].values[0], subset['MNF mean'].values[0]\n",
    "    \n",
    "    # Obtener color desde el diccionario\n",
    "    color = color_dict.get(label, 'black')  # Si no se encuentra, usa negro como fallback\n",
    "\n",
    "    # Graficar puntos\n",
    "    plt.scatter(mean_x, mean_y, label=f'Relabeled {label}', color=color, edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('RMS Mean')\n",
    "plt.ylabel('MNF Mean')\n",
    "plt.title('Variabilidad entre Movimientos')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# 1. Seleccionar las mtricas de inters\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "\n",
    "# 2. Excluir datos con relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Verificar y eliminar columnas altamente correlacionadas\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Manejo de valores faltantes\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Normalizacin de caractersticas\n",
    "scaler = StandardScaler()\n",
    "df_pca_scaled = scaler.fit_transform(df_result[dependent_vars])\n",
    "\n",
    "# 6. Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_pca_scaled)\n",
    "\n",
    "# 7. Crear DataFrame con resultados\n",
    "df_pca_result = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "df_pca_result['relabeled'] = df_result['relabeled'].astype(int)\n",
    "\n",
    "# 8. Clculo de distancias de Mahalanobis\n",
    "cov_matrix = np.cov(df_pca_scaled, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(df_pca_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 9. Clustering jerrquico\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 11. Mapear colores del dendrograma\n",
    "dendro_labels = [int(label) for label in dendro['ivl']]\n",
    "unique_labels = sorted(df_pca_result['relabeled'].unique())\n",
    "color_dict = {dendro_labels[i]: dendro['leaves_color_list'][i] for i in range(len(dendro_labels)) if dendro_labels[i] in unique_labels}\n",
    "\n",
    "# 12. Graficar PCA con colores del dendrograma\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    subset = df_pca_result[df_pca_result['relabeled'] == label]\n",
    "    color = color_dict.get(label, plt.cm.tab10(label % 10))\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=f'Relabeled {label}', color=color, edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA de Movimientos con Colores del Dendrograma')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# 1. Seleccionar las mtricas de inters\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "\n",
    "# 2. Excluir datos con relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Verificar y eliminar columnas altamente correlacionadas\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Manejo de valores faltantes\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Normalizacin de caractersticas\n",
    "scaler = StandardScaler()\n",
    "df_pca_scaled = scaler.fit_transform(df_result[dependent_vars])\n",
    "\n",
    "# 6. Aplicar PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df_pca_scaled)\n",
    "\n",
    "# 7. Crear DataFrame con resultados\n",
    "df_pca_result = pd.DataFrame(pca_result, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_result['relabeled'] = df_result['relabeled'].astype(int)\n",
    "\n",
    "# 8. Clculo de distancias de Mahalanobis\n",
    "cov_matrix = np.cov(df_pca_scaled, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(df_pca_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 9. Clustering jerrquico\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 11. Mapear colores del dendrograma\n",
    "dendro_labels = [int(label) for label in dendro['ivl']]\n",
    "unique_labels = sorted(df_pca_result['relabeled'].unique())\n",
    "color_dict = {dendro_labels[i]: dendro['leaves_color_list'][i] for i in range(len(dendro_labels)) if dendro_labels[i] in unique_labels}\n",
    "\n",
    "# 12. Graficar PCA en 3D con colores del dendrograma\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for label in unique_labels:\n",
    "    subset = df_pca_result[df_pca_result['relabeled'] == label]\n",
    "    color = color_dict.get(label, plt.cm.tab10(label % 10))\n",
    "    ax.scatter(subset['PC1'], subset['PC2'], subset['PC3'], label=f'Relabeled {label}', color=color, edgecolor='black', s=100)\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('PCA 3D de Movimientos con Colores del Dendrograma')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# 1. Seleccionar las mtricas de inters\n",
    "metrics = ['RMS mean', 'RMS_STD mean', 'RMS std', 'RMS_STD std', 'MNF mean', 'MNF_STD mean', 'MNF std', 'MNF_STD std']\n",
    "dependent_vars = [col for col in df_result.columns if col in metrics]\n",
    "\n",
    "# 2. Excluir datos con relabeled == 0\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Verificar y eliminar columnas altamente correlacionadas\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Manejo de valores faltantes\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Normalizacin de caractersticas\n",
    "scaler = StandardScaler()\n",
    "df_pca_scaled = scaler.fit_transform(df_result[dependent_vars])\n",
    "\n",
    "# 6. Aplicar PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df_pca_scaled)\n",
    "\n",
    "# 7. Crear DataFrame con resultados\n",
    "df_pca_result = pd.DataFrame(pca_result, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_result['relabeled'] = df_result['relabeled'].astype(int)\n",
    "\n",
    "# 8. Clculo de distancias de Mahalanobis\n",
    "cov_matrix = np.cov(df_pca_scaled, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(df_pca_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 9. Clustering jerrquico\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 11. Mapear colores del dendrograma\n",
    "dendro_labels = [int(label) for label in dendro['ivl']]\n",
    "unique_labels = sorted(df_pca_result['relabeled'].unique())\n",
    "color_dict = {\n",
    "    int(dendro['ivl'][i]): matplotlib.colors.to_hex(dendro['leaves_color_list'][i])\n",
    "    for i in range(len(dendro['ivl'])) if int(dendro['ivl'][i]) in unique_labels\n",
    "}\n",
    "\n",
    "# 12. Graficar PCA en 3D interactivo con Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "for label in unique_labels:\n",
    "    subset = df_pca_result[df_pca_result['relabeled'] == label]\n",
    "    color = color_dict.get(label, 'gray')\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=subset['PC1'],\n",
    "        y=subset['PC2'],\n",
    "        z=subset['PC3'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color=color, opacity=0.8),\n",
    "        name=f'Relabeled {label}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='PCA 3D Interactivo de Movimientos con Colores del Dendrograma',\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para todos los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_result.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# 2. Excluir datos con relabeled == 0\n",
    "dependent_vars = [col for col in dependent_vars if col in df_result.columns and np.issubdtype(df_result[col].dtype, np.number)]\n",
    "\n",
    "df_result = df_result[df_result['relabeled'] != 0]\n",
    "\n",
    "# 3. Verificar y eliminar columnas altamente correlacionadas\n",
    "corr_matrix = df_result[dependent_vars].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "dependent_vars = [col for col in dependent_vars if col not in to_drop]\n",
    "\n",
    "# 4. Manejo de valores faltantes\n",
    "df_result = df_result.dropna(subset=dependent_vars + ['relabeled'])\n",
    "\n",
    "# 5. Normalizacin de caractersticas\n",
    "scaler = StandardScaler()\n",
    "df_pca_scaled = scaler.fit_transform(df_result[dependent_vars])\n",
    "\n",
    "# 6. Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_pca_scaled)\n",
    "\n",
    "# 7. Crear DataFrame con resultados\n",
    "df_pca_result = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "df_pca_result['relabeled'] = df_result['relabeled'].astype(int)\n",
    "\n",
    "# 8. Clculo de distancias de Mahalanobis\n",
    "cov_matrix = np.cov(df_pca_scaled, rowvar=False)\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "mahalanobis_distances = pdist(df_pca_scaled, metric='mahalanobis', VI=inv_cov_matrix)\n",
    "distance_matrix = squareform(mahalanobis_distances)\n",
    "\n",
    "# 9. Clustering jerrquico\n",
    "linked = sch.linkage(distance_matrix, method='complete')\n",
    "linked[:, 2] /= np.max(linked[:, 2])\n",
    "\n",
    "# 11. Mapear colores del dendrograma\n",
    "dendro_labels = [int(label) for label in dendro['ivl']]\n",
    "unique_labels = sorted(df_pca_result['relabeled'].unique())\n",
    "color_dict = {dendro_labels[i]: dendro['leaves_color_list'][i] for i in range(len(dendro_labels)) if dendro_labels[i] in unique_labels}\n",
    "\n",
    "# 12. Graficar PCA con colores del dendrograma\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    subset = df_pca_result[df_pca_result['relabeled'] == label]\n",
    "    color = color_dict.get(label, plt.cm.tab10(label % 10))\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=f'Relabeled {label}', color=color, edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA de Movimientos con Colores del Dendrograma')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener pesos de cada feature en cada componente principal\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # Transponer para tener features en filas y componentes en columnas\n",
    "    index=dependent_vars,  # Nombres de las features\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)]  # Nombres de los componentes\n",
    ")\n",
    "\n",
    "# Mostrar los pesos\n",
    "print(\"Pesos de las caractersticas en los componentes principales:\")\n",
    "display(pca_loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: Graficar las cargas de los primeros tres componentes\n",
    "plt.figure(figsize=(10, 6))\n",
    "pca_loadings.plot(kind='bar', figsize=(12, 6), cmap='viridis', edgecolor='black')\n",
    "plt.title(\"Pesos de cada Feature en los Componentes Principales\")\n",
    "plt.xlabel(\"Caractersticas\")\n",
    "plt.ylabel(\"Carga\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title=\"Componentes Principales\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener pesos de cada feature en cada componente principal\n",
    "pca_loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # Transponer para tener features en filas y componentes en columnas\n",
    "    index=dependent_vars,  # Nombres de las features\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)]  # Nombres de los componentes\n",
    ")\n",
    "\n",
    "# Calcular el peso total de cada feature\n",
    "explained_variance = pca.explained_variance_ratio_  # Varianza explicada por cada componente\n",
    "feature_weights = (pca_loadings**2) @ explained_variance  # Ponderar cargas por la varianza explicada y sumar\n",
    "\n",
    "# Crear DataFrame con los pesos totales\n",
    "feature_weights_df = pd.DataFrame(feature_weights, columns=['Total Weight'])\n",
    "feature_weights_df = feature_weights_df.sort_values(by='Total Weight', ascending=False)  # Ordenar de mayor a menor\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Pesos totales de cada caracterstica en el PCA:\")\n",
    "display(feature_weights_df)\n",
    "\n",
    "# Graficar los pesos totales\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_weights_df.plot(kind='bar', figsize=(12, 6), cmap='viridis', edgecolor='black', legend=False)\n",
    "plt.title(\"Peso Total de cada Feature en el PCA\")\n",
    "plt.xlabel(\"Caractersticas\")\n",
    "plt.ylabel(\"Peso Total\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Asociar los colores a los valores de relabeled\n",
    "leaf_labels = [int(label) for label in dendro['ivl']]  # Convertimos etiquetas a enteros\n",
    "color_dict = {leaf_labels[i]: dendro['leaves_color_list'][i] for i in range(len(leaf_labels))}\n",
    "\n",
    "# 3. Preparar los datos del PCA\n",
    "df_plot = df_result[['RMS mean', 'MNF mean', 'RMS std', 'MNF std']].copy()\n",
    "\n",
    "# Asegurar que se use la columna de 'relabeled' correcta si existe en df_result\n",
    "if 'relabeled' in df_result.columns:\n",
    "    df_plot['relabeled'] = df_result['relabeled'].astype(int)\n",
    "else:\n",
    "    raise KeyError(\"La columna 'relabeled' no se encuentra en df_result\")\n",
    "\n",
    "# 4. Crear el grfico PCA con los colores del dendrograma\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Iterar sobre cada punto y graficarlo con su respectivo color\n",
    "for i in range(len(df_plot)):\n",
    "    label = df_plot.iloc[i]['relabeled']\n",
    "    mean_x, mean_y = df_plot.iloc[i]['RMS mean'], df_plot.iloc[i]['MNF mean']\n",
    "    \n",
    "    # Obtener color desde el diccionario con fallback a 'black'\n",
    "    color = color_dict.get(label, 'black')\n",
    "\n",
    "    # Graficar puntos individuales\n",
    "    plt.scatter(mean_x, mean_y, color=color, edgecolor='black', s=100)\n",
    "\n",
    "legend_labels = {label: color_dict.get(label, 'black') for label in df_plot['relabeled'].unique()}\n",
    "\n",
    "for label, color in legend_labels.items():\n",
    "    plt.scatter([], [], color=color, label=f'Relabeled {label}', edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.title('Variabilidad entre Movimientos')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Asociar los colores a los valores de relabeled\n",
    "leaf_labels = [int(label) for label in dendro['ivl']]  # Convertimos etiquetas a enteros\n",
    "color_dict = {leaf_labels[i]: dendro['leaves_color_list'][i] for i in range(len(leaf_labels))}\n",
    "\n",
    "# 3. Preparar los datos del PCA\n",
    "df_plot = df_result[['RMS mean', 'MAV mean', 'RMS std', 'MAV std']].copy()\n",
    "\n",
    "# Asegurar que se use la columna de 'relabeled' correcta si existe en df_result\n",
    "if 'relabeled' in df_result.columns:\n",
    "    df_plot['relabeled'] = df_result['relabeled'].astype(int)\n",
    "else:\n",
    "    raise KeyError(\"La columna 'relabeled' no se encuentra en df_result\")\n",
    "\n",
    "# 4. Crear el grfico PCA con los colores del dendrograma\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Iterar sobre cada punto y graficarlo con su respectivo color\n",
    "for i in range(len(df_plot)):\n",
    "    label = df_plot.iloc[i]['relabeled']\n",
    "    mean_x, mean_y = df_plot.iloc[i]['RMS mean'], df_plot.iloc[i]['MAV mean']\n",
    "    \n",
    "    # Obtener color desde el diccionario con fallback a 'black'\n",
    "    color = color_dict.get(label, 'black')\n",
    "\n",
    "    # Graficar puntos individuales\n",
    "    plt.scatter(mean_x, mean_y, color=color, edgecolor='black', s=100)\n",
    "\n",
    "legend_labels = {label: color_dict.get(label, 'black') for label in df_plot['relabeled'].unique()}\n",
    "\n",
    "for label, color in legend_labels.items():\n",
    "    plt.scatter([], [], color=color, label=f'Relabeled {label}', edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('RMS Mean')\n",
    "plt.ylabel('MAV Mean')\n",
    "plt.title('Variabilidad entre Movimientos')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Asociar los colores a los valores de relabeled\n",
    "leaf_labels = [int(label) for label in dendro['ivl']]  # Convertimos etiquetas a enteros\n",
    "color_dict = {leaf_labels[i]: dendro['leaves_color_list'][i] for i in range(len(leaf_labels))}\n",
    "\n",
    "# 3. Preparar los datos del PCA\n",
    "df_plot = df_result[['RMS mean', 'MAV mean', 'RMS std', 'MAV std']].copy()\n",
    "\n",
    "# Asegurar que se use la columna de 'relabeled' correcta si existe en df_result\n",
    "if 'relabeled' in df_result.columns:\n",
    "    df_plot['relabeled'] = df_result['relabeled'].astype(int)\n",
    "else:\n",
    "    raise KeyError(\"La columna 'relabeled' no se encuentra en df_result\")\n",
    "\n",
    "# 4. Crear el grfico PCA con los colores del dendrograma\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Iterar sobre cada punto y graficarlo con su respectivo color\n",
    "for i in range(len(df_plot)):\n",
    "    label = df_plot.iloc[i]['relabeled']\n",
    "    mean_x, mean_y = df_plot.iloc[i]['RMS mean'], df_plot.iloc[i]['MAV mean']\n",
    "    \n",
    "    # Obtener color desde el diccionario con fallback a 'black'\n",
    "    color = color_dict.get(label, 'black')\n",
    "\n",
    "    # Graficar puntos individuales\n",
    "    plt.scatter(mean_x, mean_y, color=color, edgecolor='black', s=100)\n",
    "\n",
    "legend_labels = {label: color_dict.get(label, 'black') for label in df_plot['relabeled'].unique()}\n",
    "\n",
    "for label, color in legend_labels.items():\n",
    "    plt.scatter([], [], color=color, label=f'Relabeled {label}', edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('RMS Mean')\n",
    "plt.ylabel('MAV Mean')\n",
    "plt.title('Variabilidad entre Movimientos')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# 1. Seleccionar las caractersticas para PCA\n",
    "df_pca = df_result[['RMS mean', 'MNF mean', 'RMS std', 'MNF std']].copy()\n",
    "\n",
    "# 2. Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "df_pca_scaled = scaler.fit_transform(df_pca)\n",
    "\n",
    "# 3. Aplicar PCA\n",
    "pca = PCA(n_components=2)  # Reducimos a 2D para visualizar\n",
    "pca_result = pca.fit_transform(df_pca_scaled)\n",
    "\n",
    "# 4. Crear DataFrame con resultados\n",
    "df_pca_result = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "df_pca_result['relabeled'] = combined_df['relabeled'].astype(int)  # Asegurar etiquetas enteras\n",
    "\n",
    "# 5. Verificar que hay mltiples etiquetas en relabeled\n",
    "unique_labels = df_pca_result['relabeled'].unique()\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "# 6. Generar colores automticamente\n",
    "cmap = cm.get_cmap('tab10', num_labels)\n",
    "color_dict = {label: cmap(i) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# 7. Graficar PCA con colores adecuados\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in unique_labels:\n",
    "    subset = df_pca_result[df_pca_result['relabeled'] == label]\n",
    "    color = color_dict[label]  # Obtener color correcto\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=f'Relabeled {label}', c=[color], edgecolor='black', s=100)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA de Movimientos')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEAL COMPLETA SIN ENVENTANADO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los valores especficos de 'relabeled' que queremos filtrar\n",
    "filtered_labels = [55, 2, 4, 14, 10, 16, 17, 19, 32]\n",
    "\n",
    "# Filtrar el DataFrame agrupado\n",
    "dataframe_windowing = grouped.loc[filtered_labels]\n",
    "\n",
    "dataframe_windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATAFRAME RELABELED DE INTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar el DataFrame combinado\n",
    "dataframe_windowing = combined_df[combined_df['relabeled'].isin(filtered_labels)]\n",
    "\n",
    "# Mostrar el DataFrame filtrado\n",
    "display(dataframe_windowing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENVENTANADO DE 100 ms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la frecuencia de muestreo en Hz (ajustar segn los datos)\n",
    "sampling_rate = 2000  # Por ejemplo, 200 Hz significa 200 muestras por segundo\n",
    "\n",
    "# Calcular el tamao de la ventana en nmero de muestras\n",
    "window_size = int(0.1 * sampling_rate)  # 100 ms = 0.1 segundos\n",
    "\n",
    "# Lista para almacenar las ventanas\n",
    "windowed_data = []\n",
    "\n",
    "# Aplicar enventanado a cada grupo de 'relabeled'\n",
    "for label, group in dataframe_windowing.groupby('relabeled'):\n",
    "    # Seleccionar solo columnas numricas\n",
    "    numeric_cols = group.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Aplicar el enventanado con una ventana deslizante\n",
    "    for i in range(0, len(numeric_cols) - window_size + 1, window_size):\n",
    "        window = numeric_cols.iloc[i:i + window_size]  # Extraer la ventana\n",
    "        window_mean = window.mean()  # Obtener la media de la ventana\n",
    "        \n",
    "        # Agregar la columna 'relabeled' y otros datos categricos si es necesario\n",
    "        window_mean['relabeled'] = label  # Mantener la etiqueta\n",
    "        windowed_data.append(window_mean)\n",
    "\n",
    "# Convertir la lista en un DataFrame\n",
    "dataframe_windowing_100 = pd.DataFrame(windowed_data)\n",
    "\n",
    "# Mostrar el DataFrame enventanado\n",
    "display(dataframe_windowing_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (relabeled), group in dataframe_windowing_100.groupby(['relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                #\"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df_windowing_100 = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df_windowing_100 = metrics_df_windowing_100[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df_windowing_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir la frecuencia de muestreo en Hz\n",
    "sampling_rate = 2000  # Ajustar segn los datos\n",
    "window_size = int(0.1 * sampling_rate)  # 100 ms\n",
    "\n",
    "# Nombre de la base de datos\n",
    "database = 'DB4'\n",
    "data_path = os.path.abspath(os.path.join('data', database))\n",
    "subjects = [f's{i}' for i in range(1, 11)]\n",
    "\n",
    "# Filtrar movimientos especficos\n",
    "filtered_labels = [55, 2, 4, 14, 10, 16, 17, 19, 32]\n",
    "\n",
    "# Iterar sobre cada sujeto\n",
    "for subject in subjects:\n",
    "    subject_dir = os.path.join(data_path, subject)\n",
    "    \n",
    "    # Iterar sobre archivos de ejercicios\n",
    "    for exercise in [\"E1\", \"E2\", \"E3\"]:\n",
    "        filename = f\"{subject.upper()}_{exercise}_A1.mat\"\n",
    "        file_path = os.path.join(subject_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        \n",
    "        # Cargar datos del archivo .mat\n",
    "        mat_data = src.loadmatNina(database, filename, subject=subject)\n",
    "        print(f\"Keys in mat_data: {mat_data.keys()}\")\n",
    "        \n",
    "        # Construir DataFrame\n",
    "        test_df, grasps_etiquetados = src.build_dataframe(\n",
    "            mat_file=mat_data,\n",
    "            database=database,\n",
    "            filename=filename,\n",
    "            rectify=False,\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Filtrar solo los movimientos de inters\n",
    "        test_df = test_df[test_df['relabeled'].isin(filtered_labels)]\n",
    "        \n",
    "        # Aplicar enventanado\n",
    "        windowed_data = []\n",
    "        for label, group in test_df.groupby('relabeled'):\n",
    "            numeric_cols = group.select_dtypes(include=['number'])\n",
    "            for i in range(0, len(numeric_cols) - window_size + 1, window_size):\n",
    "                window = numeric_cols.iloc[i:i + window_size]\n",
    "                window_mean = window.mean()\n",
    "                window_mean['relabeled'] = label\n",
    "                windowed_data.append(window_mean)\n",
    "        \n",
    "        dataframe_windowing_100 = pd.DataFrame(windowed_data)\n",
    "        display(dataframe_windowing_100)\n",
    "        \n",
    "        # Graficar las seales de EMG\n",
    "        for grasp in grasps_etiquetados:\n",
    "            if grasp not in filtered_labels:\n",
    "                continue\n",
    "            try:\n",
    "                if 'emg' not in mat_data:\n",
    "                    raise KeyError(f\"The key 'emg' is not in mat_data. Available keys: {mat_data.keys()}\")\n",
    "                \n",
    "                emg_signal = mat_data['emg'][grasp]\n",
    "                \n",
    "                # Graficar usando la funcin de visualizacin de EMG\n",
    "                src.plot_emg_data(\n",
    "                    database=database,\n",
    "                    mat_file=mat_data,\n",
    "                    grasp_number=grasp,\n",
    "                    interactive=False,\n",
    "                    include_rest=True,\n",
    "                    use_stimulus=False,\n",
    "                    addFourier=False,\n",
    "                    padding=100,\n",
    "                    title=f\"{filename} - Grasp {grasp}\"\n",
    "                )\n",
    "            except KeyError as e:\n",
    "                print(f\"    Error: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing grasp {grasp}: {str(e)}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENVENTANADO 200ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la frecuencia de muestreo en Hz (ajustar segn los datos)\n",
    "sampling_rate = 2000  # Por ejemplo, 200 Hz significa 200 muestras por segundo\n",
    "\n",
    "# Calcular el tamao de la ventana en nmero de muestras\n",
    "window_size = int(0.2 * sampling_rate)  # 200 ms = 0.2 segundos\n",
    "\n",
    "# Lista para almacenar las ventanas\n",
    "windowed_data = []\n",
    "\n",
    "# Aplicar enventanado a cada grupo de 'relabeled'\n",
    "for label, group in dataframe_windowing.groupby('relabeled'):\n",
    "    # Seleccionar solo columnas numricas\n",
    "    numeric_cols = group.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Aplicar el enventanado con una ventana deslizante\n",
    "    for i in range(0, len(numeric_cols) - window_size + 1, window_size):\n",
    "        window = numeric_cols.iloc[i:i + window_size]  # Extraer la ventana\n",
    "        window_mean = window.mean()  # Obtener la media de la ventana\n",
    "        \n",
    "        # Agregar la columna 'relabeled' y otros datos categricos si es necesario\n",
    "        window_mean['relabeled'] = label  # Mantener la etiqueta\n",
    "        windowed_data.append(window_mean)\n",
    "\n",
    "# Convertir la lista en un DataFrame\n",
    "dataframe_windowing_200 = pd.DataFrame(windowed_data)\n",
    "\n",
    "# Mostrar el DataFrame enventanado\n",
    "display(dataframe_windowing_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (relabeled), group in dataframe_windowing_200.groupby(['relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                #\"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df_windowing_200 = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df_windowing_200 = metrics_df_windowing_100[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df_windowing_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENVENTANADO 300ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la frecuencia de muestreo en Hz (ajustar segn los datos)\n",
    "sampling_rate = 2000  # Por ejemplo, 200 Hz significa 200 muestras por segundo\n",
    "\n",
    "# Calcular el tamao de la ventana en nmero de muestras\n",
    "window_size = int(0.3 * sampling_rate)  # 200 ms = 0.2 segundos\n",
    "\n",
    "# Lista para almacenar las ventanas\n",
    "windowed_data = []\n",
    "\n",
    "# Aplicar enventanado a cada grupo de 'relabeled'\n",
    "for label, group in dataframe_windowing.groupby('relabeled'):\n",
    "    # Seleccionar solo columnas numricas\n",
    "    numeric_cols = group.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Aplicar el enventanado con una ventana deslizante\n",
    "    for i in range(0, len(numeric_cols) - window_size + 1, window_size):\n",
    "        window = numeric_cols.iloc[i:i + window_size]  # Extraer la ventana\n",
    "        window_mean = window.mean()  # Obtener la media de la ventana\n",
    "        \n",
    "        # Agregar la columna 'relabeled' y otros datos categricos si es necesario\n",
    "        window_mean['relabeled'] = label  # Mantener la etiqueta\n",
    "        windowed_data.append(window_mean)\n",
    "\n",
    "# Convertir la lista en un DataFrame\n",
    "dataframe_windowing_300 = pd.DataFrame(windowed_data)\n",
    "\n",
    "# Mostrar el DataFrame enventanado\n",
    "display(dataframe_windowing_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the calculated metrics for each channel\n",
    "metrics_data = []\n",
    "\n",
    "# Iterate over each subject and each identified movement (relabeled or stimulus)\n",
    "for (relabeled), group in dataframe_windowing_300.groupby(['relabeled']):  # Change 'relabeled' to 'stimulus' if needed\n",
    "    # Iterate over each EMG channel\n",
    "    for channel in group.columns:  # Loop through all DataFrame columns\n",
    "        if channel.startswith('Channel'):  # Filter only EMG signal columns\n",
    "            # Get the signal values for the current channel\n",
    "            channel_signal = group[channel].values\n",
    "            \n",
    "            # Compute EMG signal metrics for the current channel\n",
    "            metrics = calculate_emg_metrics(channel_signal)\n",
    "            \n",
    "            # Append metadata and computed metrics to the list\n",
    "            metrics_data.append({\n",
    "                #\"subject\": subject,  # Subject identification\n",
    "                \"relabeled\": relabeled,  # Movement identification (relabeled or stimulus)\n",
    "                \"channel\": channel,  # EMG channel\n",
    "                **metrics  # Unpack all computed metrics\n",
    "            })\n",
    "\n",
    "# Create a DataFrame containing all the obtained metrics\n",
    "metrics_df_windowing_300 = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Reorder columns for better visualization (optional)\n",
    "column_order = [\"relabeled\", \"channel\"] + list(metrics.keys())\n",
    "metrics_df_windowing_300 = metrics_df_windowing_100[column_order]\n",
    "\n",
    "# Display the DataFrame with the computed metrics\n",
    "print(\"\\nMetrics DataFrame by Channel, Subject, and Relabeled:\")\n",
    "display(metrics_df_windowing_300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
