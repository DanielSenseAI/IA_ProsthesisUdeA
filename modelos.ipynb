{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2807be",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat, whosmat\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.linalg import inv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "import pywt\n",
    "\n",
    "import pycaret.classification as pyc\n",
    "from pycaret.classification import *\n",
    "\n",
    "import src\n",
    "from src import config, loadmatNina\n",
    "from src.preprocessing_utils import get_envelope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_by_relabeled_200 = pd.read_csv(\"metrics_avg_by_repetition_tesis_3.csv\")\n",
    "#summary_by_relabeled_200.drop\n",
    "display(summary_by_relabeled_200)\n",
    "print(summary_by_relabeled_200.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Separar columnas que no se deben normalizar\n",
    "# non_normalized_cols = ['subject', 'relabeled', 're_repetition']\n",
    "# columns_to_normalize = [col for col in summary_by_relabeled_200.columns if col not in non_normalized_cols]\n",
    "\n",
    "# # Inicializar el escalador\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Aplicar MinMaxScaler solo a las columnas deseadas\n",
    "# normalized_data = scaler.fit_transform(summary_by_relabeled_200[columns_to_normalize])\n",
    "\n",
    "# # Combinar columnas no normalizadas con las normalizadas\n",
    "# pivoted_normalized = pd.concat(\n",
    "#     [summary_by_relabeled_200[non_normalized_cols].reset_index(drop=True),\n",
    "#      pd.DataFrame(normalized_data, columns=columns_to_normalize)],\n",
    "#     axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Channel 1_window_number', 'Channel 2_window_number', 'Channel 3_window_number', 'Channel 4_window_number','Channel 5_window_number', 'Channel 6_window_number', 'Channel 7_window_number', 'Channel 8_window_number','Channel 9_window_number', 'Channel 10_window_number', 'Channel 11_window_number', 'Channel 12_window_number','Channel 1_SSC',\n",
    "       'Channel 10_SSC', 'Channel 2_SSC', 'Channel 3_SSC', 'Channel 4_SSC',\n",
    "       'Channel 5_SSC', 'Channel 6_SSC', 'Channel 7_SSC', 'Channel 8_SSC',\n",
    "       'Channel 9_SSC']\n",
    "summary_by_relabeled_200 = summary_by_relabeled_200.drop(columns=cols_to_drop, errors='ignore')\n",
    "print(summary_by_relabeled_200.columns.tolist())\n",
    "display(summary_by_relabeled_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90887752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_by_relabeled_200['relabeled'].value_counts(normalize=True))  # proporciones\n",
    "print(summary_by_relabeled_200['relabeled'].value_counts())  # conteo absoluto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar el dataframe\n",
    "df = summary_by_relabeled_200.copy()\n",
    "\n",
    "# Identificar columnas de m√©tricas (todas excepto 'subject', 'relabeled' y 're_repetition')\n",
    "metric_columns = [col for col in df.columns if col not in ['subject', 'relabeled', 're_repetition']]\n",
    "\n",
    "# Escalar las m√©tricas\n",
    "scaler = MinMaxScaler()\n",
    "df[metric_columns] = scaler.fit_transform(df[metric_columns])\n",
    "\n",
    "# N√∫mero de m√©tricas\n",
    "n_metrics = len(metric_columns)\n",
    "\n",
    "# Crear figura para subplots\n",
    "fig, axes = plt.subplots(nrows=(n_metrics + 1) // 2, ncols=2, figsize=(14, 3 * ((n_metrics + 1) // 2)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Crear boxplots por m√©trica\n",
    "for i, metric in enumerate(metric_columns):\n",
    "    if i < len(axes):\n",
    "        sns.boxplot(x='relabeled', y=metric, data=df, ax=axes[i], palette='viridis')\n",
    "        axes[i].set_title(f'{metric} por grasp', fontsize=14)\n",
    "        axes[i].set_xlabel('Grasp')\n",
    "        axes[i].set_ylabel('Valor normalizado')\n",
    "        if df['relabeled'].nunique() > 5:\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Eliminar subplots vac√≠os\n",
    "for i in range(n_metrics, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# T√≠tulo general y ajuste del espacio\n",
    "plt.suptitle('Comparaci√≥n de m√©tricas EMG por grasp', fontsize=16)\n",
    "plt.subplots_adjust(top=0.96, hspace=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_by_relabeled_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e84243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar el dataframe\n",
    "df = summary_by_relabeled_200.copy()\n",
    "\n",
    "# Identificar columnas de m√©tricas (todas excepto 'subject', 'relabeled' y 're_repetition')\n",
    "metric_columns = [col for col in df.columns if col not in ['subject', 'relabeled', 're_repetition']]\n",
    "\n",
    "# Escalar las m√©tricas\n",
    "scaler = MinMaxScaler()\n",
    "df[metric_columns] = scaler.fit_transform(df[metric_columns])\n",
    "\n",
    "# N√∫mero de m√©tricas\n",
    "n_metrics = len(metric_columns)\n",
    "\n",
    "# Crear figura para subplots\n",
    "fig, axes = plt.subplots(nrows=(n_metrics + 1) // 2, ncols=2, figsize=(14, 3 * ((n_metrics + 1) // 2)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Crear violin plots por m√©trica\n",
    "for i, metric in enumerate(metric_columns):\n",
    "    if i < len(axes):\n",
    "        sns.violinplot(x='relabeled', y=metric, data=df, ax=axes[i], palette='viridis', inner='box')\n",
    "        axes[i].set_title(f'{metric} por grasp', fontsize=14)\n",
    "        axes[i].set_xlabel('Grasp')\n",
    "        axes[i].set_ylabel('Valor normalizado')\n",
    "        if df['relabeled'].nunique() > 5:\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Eliminar subplots vac√≠os\n",
    "for i in range(n_metrics, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# T√≠tulo general y ajuste del espacio\n",
    "plt.suptitle('Distribuci√≥n de m√©tricas EMG por grasp (Violin Plots)', fontsize=16)\n",
    "plt.subplots_adjust(top=0.96, hspace=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8f551",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4cf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar el DataFrame\n",
    "df = summary_by_relabeled_200.copy()\n",
    "\n",
    "# Variables predictoras (excluyendo 'subject', 'relabeled', y 're_repetition')\n",
    "X = df.drop(columns=['subject', 'relabeled', 're_repetition'])\n",
    "y = df['relabeled']\n",
    "\n",
    "# Calcular la informaci√≥n mutua\n",
    "mi = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "\n",
    "# Crear DataFrame de importancia\n",
    "mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi})\n",
    "mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 50))\n",
    "sns.barplot(x='Mutual Information', y='Feature', data=mi_df, palette='magma')\n",
    "plt.title('Importancia de caracter√≠sticas EMG seg√∫n Informaci√≥n Mutua con grasp', fontsize=14)\n",
    "plt.xlabel('Informaci√≥n Mutua')\n",
    "plt.ylabel('Caracter√≠stica')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f53aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar DataFrame original\n",
    "df = summary_by_relabeled_200.copy()\n",
    "\n",
    "# Variables predictoras y target\n",
    "X = df.drop(columns=['subject', 'relabeled', 're_repetition'])\n",
    "y = df['relabeled']\n",
    "\n",
    "# Calcular informaci√≥n mutua\n",
    "mi = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "\n",
    "# Crear DataFrame con la MI\n",
    "mi_df = pd.DataFrame({'FullName': X.columns, 'Mutual Information': mi})\n",
    "\n",
    "# Separar en \"Channel\" y \"Feature\" \n",
    "mi_df[['Channel', 'Feature']] = mi_df['FullName'].str.extract(r'(Channel \\d+)_(.*)')\n",
    "\n",
    "# Crear tabla tipo matriz (canales como filas, features como columnas)\n",
    "heatmap_df = mi_df.pivot(index='Channel', columns='Feature', values='Mutual Information')\n",
    "\n",
    "# Ordenar los canales num√©ricamente\n",
    "heatmap_df.index = heatmap_df.index.str.extract(r'Channel (\\d+)')[0].astype(int)\n",
    "heatmap_df = heatmap_df.sort_index()\n",
    "heatmap_df.index = [f'Channel {i}' for i in heatmap_df.index]\n",
    "\n",
    "# Visualizar heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(heatmap_df, cmap='YlOrRd', annot=False, linewidths=0.5, linecolor='black', cbar_kws={'label': 'Mutual Information'})\n",
    "plt.title('Heatmap de Importancia de Caracter√≠sticas EMG\\n(mutual_info_classif)', fontsize=16)\n",
    "plt.xlabel('Caracter√≠stica')\n",
    "plt.ylabel('Canal EMG')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185912cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar el DataFrame\n",
    "df = summary_by_relabeled_200.copy()\n",
    "\n",
    "# Variables predictoras (excluyendo 'subject', 'relabeled' y 're_repetition')\n",
    "X = df.drop(columns=['subject', 'relabeled', 're_repetition'])\n",
    "y = df['relabeled']\n",
    "\n",
    "# Entrenar √°rbol de decisi√≥n\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "# Obtener importancias\n",
    "importances = tree_clf.feature_importances_\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 50))\n",
    "sns.barplot(x='Importance', y='Feature', data=features_df, palette='crest')\n",
    "plt.title('Importancia de caracter√≠sticas EMG seg√∫n √Årbol de Decisi√≥n', fontsize=14)\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Caracter√≠stica')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar DataFrame\n",
    "df = summary_by_relabeled_200.copy()\n",
    "\n",
    "# Variables predictoras y etiquetas\n",
    "X = df.drop(columns=['subject', 'relabeled', 're_repetition'])\n",
    "y = df['relabeled']\n",
    "\n",
    "# Entrenar √°rbol de decisi√≥n\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "# Obtener importancias\n",
    "importances = tree_clf.feature_importances_\n",
    "\n",
    "# Crear DataFrame con nombres completos y valores de importancia\n",
    "importance_df = pd.DataFrame({'FullName': X.columns, 'Importance': importances})\n",
    "\n",
    "# Extraer nombre del canal y caracter√≠stica (ej: 'Channel 1', 'RMS')\n",
    "importance_df[['Channel', 'Feature']] = importance_df['FullName'].str.extract(r'(Channel \\d+)_(.*)')\n",
    "\n",
    "# Pivotear para crear matriz (filas=canales, columnas=features)\n",
    "heatmap_df = importance_df.pivot(index='Channel', columns='Feature', values='Importance')\n",
    "\n",
    "# Ordenar canales num√©ricamente\n",
    "heatmap_df.index = heatmap_df.index.str.extract(r'Channel (\\d+)')[0].astype(int)\n",
    "heatmap_df = heatmap_df.sort_index()\n",
    "heatmap_df.index = [f'Channel {i}' for i in heatmap_df.index]\n",
    "\n",
    "# Visualizaci√≥n tipo heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(heatmap_df, cmap='crest', annot=False, linewidths=0.5, linecolor='black', cbar_kws={'label': 'Feature Importance'})\n",
    "plt.title('Heatmap de Importancia de Caracter√≠sticas EMG\\n(√Årbol de Decisi√≥n)', fontsize=16)\n",
    "plt.xlabel('Caracter√≠stica')\n",
    "plt.ylabel('Canal EMG')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268801da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold config\n",
    "kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 1. Seleccionar m√©tricas (excluir ZC, ZC_STD, Kurt, Kurt_STD)\n",
    "excluded = ['ZC', 'ZC_STD', 'Kurt', 'Kurt_STD']\n",
    "features = [c for c in summary_by_relabeled_200.columns \n",
    "            if c not in ['subject', 'relabeled', 'stimulus','re_repetition'] \n",
    "            and not any(exc.upper() in c.upper() for exc in excluded)]\n",
    "\n",
    "X = summary_by_relabeled_200[features].values\n",
    "y = summary_by_relabeled_200['relabeled'].values\n",
    "\n",
    "# 2. Escalado\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Determinar n√∫mero √≥ptimo de componentes usando varianza explicada acumulada\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "explained_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Graficar varianza explicada\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.xlabel('N√∫mero de componentes principales')\n",
    "plt.ylabel('Varianza explicada acumulada')\n",
    "plt.title('Selecci√≥n del n√∫mero √≥ptimo de componentes en PCA')\n",
    "plt.grid(True)\n",
    "plt.axhline(0.90, color='r', linestyle='--', label='90% de varianza explicada')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Aplicar PCA con un n√∫mero fijo de componentes (puedes ajustar este n√∫mero seg√∫n la curva)\n",
    "n_components = np.argmax(explained_variance >= 0.90) + 1  # Componentes que explican >=90% varianza\n",
    "print(f\"‚úÖ N√∫mero de componentes seleccionados: {n_components}\")\n",
    "\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 5. Carga de cada feature en cada componente\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=features,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components)]\n",
    ")\n",
    "\n",
    "# Mostrar los features m√°s influyentes en PC1 y PC2\n",
    "print(\"Top 5 features por carga absoluta en PC1:\")\n",
    "display(loadings['PC1'].abs().sort_values(ascending=False).head(5))\n",
    "\n",
    "if n_components >= 2:\n",
    "    print(\"Top 5 features por carga absoluta en PC2:\")\n",
    "    display(loadings['PC2'].abs().sort_values(ascending=False).head(5))\n",
    "\n",
    "# 6. Visualizaci√≥n PCA 2D si hay al menos 2 componentes\n",
    "if n_components >= 2:\n",
    "    pca_df = pd.DataFrame(X_pca[:, :2], columns=['PC1', 'PC2'])\n",
    "    pca_df['relabeled'] = y\n",
    "    pca_df['subject'] = summary_by_relabeled_200['subject'].values\n",
    "\n",
    "    fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2',\n",
    "        color='relabeled', symbol='subject',\n",
    "        title='Proyecci√≥n PCA (PC1 vs PC2)',\n",
    "        hover_data=['subject', 'relabeled']\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Leyenda mejor distribuida y desplazable\n",
    "    fig.update_layout(\n",
    "    width=1000,\n",
    "    height=950,\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=-0.1,  # Debajo del gr√°fico\n",
    "        xanchor='center',\n",
    "        x=0.5,\n",
    "        font=dict(size=15)\n",
    "    )\n",
    ")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf49593",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings.abs().sum(axis=1).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico para PC1\n",
    "pc1_abs_loadings = loadings['PC1'].abs().sort_values(ascending=False)\n",
    "fig_pc1 = px.bar(\n",
    "    pc1_abs_loadings,  # Top 10 features\n",
    "    x=pc1_abs_loadings.index,\n",
    "    y=pc1_abs_loadings.values,\n",
    "    labels={'x': 'Feature', 'y': 'Carga absoluta'},\n",
    "    title='Cargas absolutas en PC1'\n",
    ")\n",
    "fig_pc1.update_layout(xaxis_tickangle=-45)\n",
    "fig_pc1.show()\n",
    "\n",
    "# Gr√°fico para PC2\n",
    "pc2_abs_loadings = loadings['PC2'].abs().sort_values(ascending=False)\n",
    "fig_pc2 = px.bar(\n",
    "    pc2_abs_loadings,\n",
    "    x=pc2_abs_loadings.index,\n",
    "    y=pc2_abs_loadings.values,\n",
    "    labels={'x': 'Feature', 'y': 'Carga absoluta'},\n",
    "    title='Cargas absolutas en PC2'\n",
    ")\n",
    "fig_pc2.update_layout(xaxis_tickangle=-45)\n",
    "fig_pc2.show()\n",
    "\n",
    "pc3_abs_loadings = loadings['PC3'].abs().sort_values(ascending=False)\n",
    "fig_pc3 = px.bar(\n",
    "    pc3_abs_loadings,\n",
    "    x=pc3_abs_loadings.index,\n",
    "    y=pc3_abs_loadings.values,\n",
    "    labels={'x': 'Feature', 'y': 'Carga absoluta'},\n",
    "    title='Cargas absolutas en PC3'\n",
    ")\n",
    "\n",
    "fig_pc3.update_layout(xaxis_tickangle=-45)\n",
    "fig_pc3.show()\n",
    "\n",
    "pc4_abs_loadings = loadings['PC4'].abs().sort_values(ascending=False)\n",
    "fig_pc4 = px.bar(\n",
    "    pc4_abs_loadings,\n",
    "    x=pc4_abs_loadings.index,\n",
    "    y=pc4_abs_loadings.values,\n",
    "    labels={'x': 'Feature', 'y': 'Carga absoluta'},\n",
    "    title='Cargas absolutas en PC4'\n",
    ")\n",
    "\n",
    "fig_pc4.update_layout(xaxis_tickangle=-45)\n",
    "fig_pc4.show()\n",
    "\n",
    "pc5_abs_loadings = loadings['PC5'].abs().sort_values(ascending=False)\n",
    "fig_pc5 = px.bar(\n",
    "    pc5_abs_loadings,\n",
    "    x=pc5_abs_loadings.index,\n",
    "    y=pc5_abs_loadings.values,\n",
    "    labels={'x': 'Feature', 'y': 'Carga absoluta'},\n",
    "    title='Cargas absolutas en PC5'\n",
    ")\n",
    "\n",
    "fig_pc5.update_layout(xaxis_tickangle=-45)\n",
    "fig_pc5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°lculo de importancia global por feature: suma ponderada de cargas absolutas por varianza explicada\n",
    "explained_var = pca.explained_variance_ratio_[:n_components]  # vector (PC1, PC2, ..., PCn)\n",
    "\n",
    "# Multiplicar cada carga absoluta por la varianza explicada\n",
    "weighted_loadings = loadings.abs().values * explained_var  # matriz (features x PCs)\n",
    "\n",
    "# Sumar por filas para obtener importancia total por feature\n",
    "global_importance = weighted_loadings.sum(axis=1)\n",
    "\n",
    "# Crear DataFrame ordenado\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': loadings.index,\n",
    "    'Importance': global_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar top 10\n",
    "print(\"Top 10 features por importancia global (PCA):\")\n",
    "display(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899731ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separar canal y m√©trica con expresi√≥n regular\n",
    "importance_df[['Channel', 'Feature']] = (\n",
    "    importance_df['Feature']\n",
    "    .str.extract(r'^(Channel \\d+)[ _-]*(.+)$')   # admite separador _, espacio o -\n",
    ")\n",
    "\n",
    "# 2. Eliminar filas donde no se haya podido extraer canal o feature\n",
    "bad_rows = importance_df[importance_df['Channel'].isna() | importance_df['Feature'].isna()]\n",
    "\n",
    "\n",
    "importance_df = importance_df.dropna(subset=['Channel', 'Feature'])\n",
    "\n",
    "# 3. Pivotar a matriz canales √ó features\n",
    "heatmap_df = importance_df.pivot(index='Channel',\n",
    "                                 columns='Feature',\n",
    "                                 values='Importance')\n",
    "\n",
    "# 4. Ordenar los canales por n√∫mero \n",
    "heatmap_df = (\n",
    "    heatmap_df\n",
    "    .assign(_n = heatmap_df.index.str.extract(r'(\\d+)').astype(int))\n",
    "    .sort_values('_n')\n",
    "    .drop(columns='_n')\n",
    ")\n",
    "\n",
    "# 5. Dibujar heatmap\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_df,\n",
    "    cmap='viridis',\n",
    "    linewidths=.5,\n",
    "    linecolor='black',\n",
    "    cbar_kws={'label': 'Importancia global (PCA)'}\n",
    ")\n",
    "plt.title('Importancia de caracter√≠sticas EMG ‚Äì PCA ponderada', fontsize=16)\n",
    "plt.xlabel('M√©trica')\n",
    "plt.ylabel('Canal')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = pca.explained_variance_ratio_\n",
    "for i, var in enumerate(explained_var, start=1):\n",
    "    print(f'PC{i}: {var:.4f} varianza explicada ({var*100:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1bd72",
   "metadata": {},
   "source": [
    "## Clasificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8157f",
   "metadata": {},
   "source": [
    "- PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c14b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "scores = cross_val_score(clf, X_pca, y, cv=kf, scoring='accuracy')\n",
    "print(f\"\\nAccuracy 5-fold CV en espacio PCA ({n_components} componentes): {scores.mean():.3f} ¬± {scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf742ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_scores = cross_val_score(rf_model, X_pca, y, cv=kf)\n",
    "\n",
    "print(f\"üéØ Accuracy 5-fold CV con Random Forest: {rf_scores.mean():.3f} ¬± {rf_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908abb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Evaluaci√≥n utilizando validaci√≥n cruzada\n",
    "knn_scores = cross_val_score(knn_model, X_pca, y, cv=kf)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"üéØ Accuracy 5-fold CV con KNN: {knn_scores.mean():.3f} ¬± {knn_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a433426",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_scores = cross_val_score(svm_model, X_pca, y, cv=kf)\n",
    "\n",
    "print(f\"üéØ Accuracy 5-fold CV con SVM (RBF kernel): {svm_scores.mean():.3f} ¬± {svm_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar etiquetas\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Clasificador XGBoost\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Validaci√≥n cruzada 5-fold\n",
    "xgb_scores = cross_val_score(xgb_clf, X_pca, y_encoded, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"üéØ Accuracy 5-fold CV con XGBoost: {xgb_scores.mean():.3f} ¬± {xgb_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Red neuronal\n",
    "mlp_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),  # una capa oculta con 100 neuronas\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Validaci√≥n cruzada 5-fold\n",
    "mlp_scores = cross_val_score(mlp_clf, X_pca, y_encoded, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"üß† Accuracy 5-fold CV con MLPClassifier: {mlp_scores.mean():.3f} ¬± {mlp_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el clasificador con los par√°metros del modelo de PyCaret\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    class_weight=None,\n",
    "    colsample_bytree=1.0,\n",
    "    importance_type='split',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=0.001,\n",
    "    min_split_gain=0.0,\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    subsample=1.0,\n",
    "    subsample_for_bin=200000,\n",
    "    subsample_freq=0\n",
    ")\n",
    "\n",
    "# Usar el mismo esquema de validaci√≥n cruzada que PyCaret\n",
    "kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "scores = cross_val_score(model, X_pca, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(f\"LGBMClassifier Accuracy 7-fold CV: {scores.mean():.3f} ¬± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el entorno de pycaret\n",
    "data = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "data['relabeled'] = y\n",
    "\n",
    "# Iniciar la configuraci√≥n de PyCaret\n",
    "clf = setup(\n",
    "    data=data, \n",
    "    target='relabeled', \n",
    "    session_id=42, \n",
    "    fold=7,  \n",
    "    normalize=False, \n",
    "    feature_selection=True, \n",
    "    pca=False,\n",
    "    fold_shuffle=True, \n",
    "    verbose=True   \n",
    ")\n",
    "\n",
    "best_model = compare_models(sort='Accuracy')\n",
    "\n",
    "\n",
    "if best_model is not None:\n",
    "    tuned_model = tune_model(best_model, optimize='Accuracy', n_iter=50)\n",
    "\n",
    "    # Finalizar \n",
    "    final_model = finalize_model(tuned_model)\n",
    "\n",
    "    # Evaluar el modelo final\n",
    "    evaluate_model(final_model)\n",
    "else:\n",
    "    print(\"No se encontr√≥ un modelo v√°lido para tunear.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3c5c3",
   "metadata": {},
   "source": [
    "- Without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4374a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "# Asegurar que la columna target sea tipo string\n",
    "summary_by_relabeled_200['relabeled'] = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# Configurar PyCaret\n",
    "clf = setup(\n",
    "    data=summary_by_relabeled_200,\n",
    "    target='relabeled',\n",
    "    session_id=42,\n",
    "    fold=7,\n",
    "    fold_shuffle=True,\n",
    "    normalize=True,\n",
    "    feature_selection=False,\n",
    "    pca=False,\n",
    "    data_split_stratify=True,\n",
    "    #silent=True,\n",
    "    use_gpu=False  # cambia a True si tienes GPU y quieres usarla\n",
    ")\n",
    "\n",
    "# Comparar modelos incluyendo el MLP\n",
    "models_to_compare = compare_models(\n",
    "    sort='Accuracy',\n",
    "    include=['mlp', 'lr', 'rf', 'xgboost', 'lightgbm', 'dt', 'knn', 'et', 'svm']  # puedes agregar o quitar modelos aqu√≠\n",
    ")\n",
    "\n",
    "# Seleccionar el mejor modelo\n",
    "if models_to_compare is not None:\n",
    "    best_model = models_to_compare\n",
    "    tuned_model = tune_model(best_model, optimize='Accuracy', n_iter=80)\n",
    "    final_model = finalize_model(tuned_model)\n",
    "    evaluate_model(final_model)\n",
    "else:\n",
    "    print(\"No se encontr√≥ un modelo v√°lido para tunear.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0aa8bd",
   "metadata": {},
   "source": [
    "## Independent Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842154a",
   "metadata": {},
   "source": [
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adffd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, accuracy_score, f1_score, recall_score, precision_score,\n",
    "    cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# 2. Pipeline\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. M√©tricas personalizadas\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "}\n",
    "\n",
    "# 5. Evaluaci√≥n de m√©tricas\n",
    "results = cross_validate(\n",
    "    model, X, y, cv=cv, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ M√©tricas promedio en validaci√≥n cruzada (7 folds):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric.replace('test_', '').upper():<15}: {np.mean(values):.4f}\")\n",
    "\n",
    "# 6. Matrices de confusi√≥n por fold y global\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    # Matriz por fold\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Matriz total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Curva de aprendizaje (accuracy vs. tama√±o del set de entrenamiento)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calcular media y desviaci√≥n est√°ndar\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Graficar curva de aprendizaje\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Accuracy Entrenamiento')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='green', label='Accuracy Validaci√≥n')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "\n",
    "plt.title('Curva de Aprendizaje - Logistic Regression')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092bb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    cohen_kappa_score, matthews_corrcoef, confusion_matrix, log_loss\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Preparar datos\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "classes = np.unique(y)\n",
    "\n",
    "# 2. Configuraci√≥n\n",
    "max_epochs = 500\n",
    "patience = 3\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# Almacenar resultados globales\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "# Curvas por fold\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# 3. Cross-validation manual con early stopping\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    print(f\"\\nüîÅ Fold {fold}\")\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Escalar\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Modelo SGD\n",
    "    model = SGDClassifier(\n",
    "        loss='log_loss',  # equivalente a regresi√≥n log√≠stica\n",
    "        max_iter=1,       # solo una iteraci√≥n por .fit\n",
    "        tol=None,\n",
    "        learning_rate='optimal',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.partial_fit(X_train_scaled, y_train, classes=classes)\n",
    "\n",
    "        # Accuracy\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_val_pred = model.predict(X_val_scaled)\n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Early stopping basado en validation loss\n",
    "        val_loss = log_loss(y_val, model.predict_proba(X_val_scaled), labels=classes)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping en epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Guardar resultados globales\n",
    "    y_pred_fold = best_model.predict(X_val_scaled)\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred_fold)\n",
    "\n",
    "    # Matriz por fold\n",
    "    cm = confusion_matrix(y_val, y_pred_fold, labels=classes)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Guardar curvas\n",
    "    all_train_acc.append(train_acc_history)\n",
    "    all_val_acc.append(val_acc_history)\n",
    "\n",
    "# 4. Matriz total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=classes)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Curva de accuracy vs. epoch (promediada)\n",
    "max_len = max(len(acc) for acc in all_val_acc)\n",
    "train_acc_array = np.array([np.pad(acc, (0, max_len - len(acc)), constant_values=np.nan) for acc in all_train_acc])\n",
    "val_acc_array = np.array([np.pad(acc, (0, max_len - len(acc)), constant_values=np.nan) for acc in all_val_acc])\n",
    "\n",
    "mean_train = np.nanmean(train_acc_array, axis=0)\n",
    "mean_val = np.nanmean(val_acc_array, axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, max_len + 1), mean_train, label='Accuracy Entrenamiento', color='blue')\n",
    "plt.plot(range(1, max_len + 1), mean_val, label='Accuracy Validaci√≥n', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Curva de Accuracy vs. Epoch (con early stopping)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. M√©tricas globales (macro)\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='macro')\n",
    "recall = recall_score(all_y_true, all_y_pred, average='macro')\n",
    "precision = precision_score(all_y_true, all_y_pred, average='macro')\n",
    "kappa = cohen_kappa_score(all_y_true, all_y_pred)\n",
    "mcc = matthews_corrcoef(all_y_true, all_y_pred)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nüìä M√©tricas globales (macro):\")\n",
    "print(f\"Accuracy          : {accuracy:.4f}\")\n",
    "print(f\"F1 Score (macro)  : {f1:.4f}\")\n",
    "print(f\"Recall (macro)    : {recall:.4f}\")\n",
    "print(f\"Precision (macro) : {precision:.4f}\")\n",
    "print(f\"Cohen's Kappa     : {kappa:.4f}\")\n",
    "print(f\"Matthews CorrCoef : {mcc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128c504",
   "metadata": {},
   "source": [
    "- Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01adc499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# 2. Crear pipeline\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# 3. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. Definir m√©tricas\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "}\n",
    "\n",
    "# 5. Evaluaci√≥n por validaci√≥n cruzada\n",
    "results = cross_validate(\n",
    "    model, X, y, cv=cv, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ExtraTreesClassifier - M√©tricas promedio en validaci√≥n cruzada (7 folds):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric.replace('test_', '').upper():<15}: {np.mean(values):.4f}\")\n",
    "\n",
    "# 6. Matrices de confusi√≥n por fold\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 7. Matriz de confusi√≥n total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Curva de aprendizaje: accuracy vs n√∫mero de muestras (tama√±o del set de entrenamiento)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y, cv=cv, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Calcular promedio y desviaci√≥n\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Graficar la curva de aprendizaje\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label=\"Accuracy Entrenamiento\", color='blue')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', label=\"Accuracy Validaci√≥n\", color='green')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "\n",
    "plt.title('Curva de Aprendizaje - ExtraTreesClassifier')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# 2. Crear pipeline\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    ExtraTreesClassifier(min_samples_split=5, min_samples_leaf=3)\n",
    "\n",
    ")\n",
    "\n",
    "# 3. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. Definir m√©tricas\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "}\n",
    "\n",
    "# 5. Evaluaci√≥n por validaci√≥n cruzada\n",
    "results = cross_validate(\n",
    "    model, X, y, cv=cv, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ExtraTreesClassifier - M√©tricas promedio en validaci√≥n cruzada (7 folds):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric.replace('test_', '').upper():<15}: {np.mean(values):.4f}\")\n",
    "\n",
    "# 6. Matrices de confusi√≥n por fold\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 7. Matriz de confusi√≥n total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Curva de aprendizaje: accuracy vs n√∫mero de muestras (tama√±o del set de entrenamiento)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y, cv=cv, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Calcular promedio y desviaci√≥n\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Graficar la curva de aprendizaje\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label=\"Accuracy Entrenamiento\", color='blue')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', label=\"Accuracy Validaci√≥n\", color='green')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "\n",
    "plt.title('Curva de Aprendizaje - ExtraTreesClassifier')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef89a1",
   "metadata": {},
   "source": [
    "- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, accuracy_score, f1_score, recall_score, precision_score,\n",
    "    cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# 2. Pipeline\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='rbf', probability=True, random_state=42)\n",
    ")\n",
    "\n",
    "# 3. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. M√©tricas personalizadas\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "}\n",
    "\n",
    "# 5. Evaluaci√≥n de m√©tricas\n",
    "results = cross_validate(\n",
    "    model, X, y, cv=cv, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SVM - M√©tricas promedio en validaci√≥n cruzada (7 folds):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric.replace('test_', '').upper():<15}: {np.mean(values):.4f}\")\n",
    "\n",
    "# 6. Matrices de confusi√≥n por fold y global\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    # Matriz por fold\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Matriz total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Curva de aprendizaje\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', label='Entrenamiento', color='blue')\n",
    "plt.plot(train_sizes, val_scores_mean, 's--', label='Validaci√≥n', color='green')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Curva de aprendizaje - SVM (kernel RBF)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, accuracy_score, f1_score, recall_score, precision_score,\n",
    "    cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='rbf', C=0.5, gamma=0.01, probability=True, random_state=42)\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. M√©tricas personalizadas\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "}\n",
    "\n",
    "# 5. Evaluaci√≥n de m√©tricas\n",
    "results = cross_validate(\n",
    "    model, X, y, cv=cv, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SVM - M√©tricas promedio en validaci√≥n cruzada (7 folds):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric.replace('test_', '').upper():<15}: {np.mean(values):.4f}\")\n",
    "\n",
    "# 6. Matrices de confusi√≥n por fold y global\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    # Matriz por fold\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Matriz total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Curva de aprendizaje\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', label='Entrenamiento', color='blue')\n",
    "plt.plot(train_sizes, val_scores_mean, 's--', label='Validaci√≥n', color='green')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Curva de aprendizaje - SVM (kernel RBF)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8222795",
   "metadata": {},
   "source": [
    "- KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, accuracy_score, f1_score, recall_score, precision_score,\n",
    "    cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# 2. Pipeline\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(n_neighbors=5)\n",
    ")\n",
    "\n",
    "# 3. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. M√©tricas personalizadas\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "}\n",
    "\n",
    "# 5. Evaluaci√≥n de m√©tricas\n",
    "results = cross_validate(\n",
    "    model, X, y, cv=cv, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ KNN - M√©tricas promedio en validaci√≥n cruzada (7 folds):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric.replace('test_', '').upper():<15}: {np.mean(values):.4f}\")\n",
    "\n",
    "# 6. Matrices de confusi√≥n por fold y global\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    # Matriz por fold\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Matriz total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Curva de aprendizaje (accuracy vs training size)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y, cv=cv, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Precisi√≥n en entrenamiento')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='green', label='Precisi√≥n en validaci√≥n')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "\n",
    "plt.title('Curva de aprendizaje - KNN')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Precisi√≥n')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61285016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, accuracy_score, f1_score, recall_score, precision_score,\n",
    "    cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# 2. Pipeline\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(n_neighbors=3)\n",
    ")\n",
    "\n",
    "# 3. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. M√©tricas personalizadas\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "}\n",
    "\n",
    "# 5. Evaluaci√≥n de m√©tricas\n",
    "results = cross_validate(\n",
    "    model, X, y, cv=cv, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ KNN - M√©tricas promedio en validaci√≥n cruzada (7 folds):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric.replace('test_', '').upper():<15}: {np.mean(values):.4f}\")\n",
    "\n",
    "# 6. Matrices de confusi√≥n por fold y global\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    # Matriz por fold\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Matriz total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Curva de aprendizaje (accuracy vs training size)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y, cv=cv, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Precisi√≥n en entrenamiento')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='green', label='Precisi√≥n en validaci√≥n')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "\n",
    "plt.title('Curva de aprendizaje - KNN')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Precisi√≥n')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e3a84",
   "metadata": {},
   "source": [
    "- RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, matthews_corrcoef,\n",
    "    confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# 2. Pipeline con MLPClassifier\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    ")\n",
    "\n",
    "# 3. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. M√©tricas personalizadas\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'recall_macro': 'recall_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "}\n",
    "\n",
    "# 5. Evaluaci√≥n de m√©tricas\n",
    "results = cross_validate(\n",
    "    model, X, y, cv=cv, scoring=scoring, return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RNA (MLPClassifier) - M√©tricas promedio en validaci√≥n cruzada (7 folds):\")\n",
    "for metric, values in results.items():\n",
    "    if \"test\" in metric:\n",
    "        print(f\"{metric.replace('test_', '').upper():<15}: {np.mean(values):.4f}\")\n",
    "\n",
    "# 6. Matrices de confusi√≥n por fold y global\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    # Matriz de confusi√≥n por fold\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Matriz de confusi√≥n total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Curva de aprendizaje (accuracy vs training size)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y, cv=cv, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Precisi√≥n en entrenamiento')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='green', label='Precisi√≥n en validaci√≥n')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "\n",
    "plt.title('Curva de aprendizaje - MLPClassifier')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Precisi√≥n')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ed18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, matthews_corrcoef,\n",
    "    confusion_matrix, accuracy_score\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Definir X e y\n",
    "X = summary_by_relabeled_200.drop(columns=['relabeled', 'subject', 're_repetition'])\n",
    "y = summary_by_relabeled_200['relabeled'].astype(str)\n",
    "\n",
    "# 2. Validaci√≥n cruzada\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "# 3. Matrices de confusi√≥n y curvas por fold\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "    print(f\"\\nüîÅ Fold {fold}\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # 4. Modelo con early stopping\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(100,),\n",
    "        max_iter=500,\n",
    "        early_stopping=False,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    model = make_pipeline(StandardScaler(), mlp)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 5. Predicci√≥n\n",
    "    y_pred = model.predict(X_val)\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "    # 6. Matriz de confusi√≥n por fold\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=np.unique(y))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "    plt.title(f'Matriz de confusi√≥n - Fold {fold}')\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Verdadero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "# Matriz de confusi√≥n total\n",
    "cm_total = confusion_matrix(all_y_true, all_y_pred, labels=np.unique(y))\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_total, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Matriz de confusi√≥n global (todos los folds)')\n",
    "plt.xlabel(\"Predicho\")\n",
    "plt.ylabel(\"Verdadero\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Curva de aprendizaje (accuracy vs training size)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X, y, cv=cv, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Precisi√≥n en entrenamiento')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='green', label='Precisi√≥n en validaci√≥n')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color='green')\n",
    "\n",
    "plt.title('Curva de aprendizaje - MLPClassifier')\n",
    "plt.xlabel('Tama√±o del conjunto de entrenamiento')\n",
    "plt.ylabel('Precisi√≥n')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "# 6. M√©tricas globales (macro)\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='macro')\n",
    "recall = recall_score(all_y_true, all_y_pred, average='macro')\n",
    "precision = precision_score(all_y_true, all_y_pred, average='macro')\n",
    "kappa = cohen_kappa_score(all_y_true, all_y_pred)\n",
    "mcc = matthews_corrcoef(all_y_true, all_y_pred)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nüìä M√©tricas globales (macro):\")\n",
    "print(f\"Accuracy          : {accuracy:.4f}\")\n",
    "print(f\"F1 Score (macro)  : {f1:.4f}\")\n",
    "print(f\"Recall (macro)    : {recall:.4f}\")\n",
    "print(f\"Precision (macro) : {precision:.4f}\")\n",
    "print(f\"Cohen's Kappa     : {kappa:.4f}\")\n",
    "print(f\"Matthews CorrCoef : {mcc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
